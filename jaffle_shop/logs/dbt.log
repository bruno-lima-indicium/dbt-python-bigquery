

============================== 2023-01-24 02:09:50.738726 | 85238755-0784-4d14-aab4-6586bc071251 ==============================
[0m02:09:50.738794 [info ] [MainThread]: Running with dbt=1.3.2
[0m02:09:50.739504 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'indirect_selection': 'eager', 'resource_types': [], 'which': 'build', 'rpc_method': 'build'}
[0m02:09:50.739656 [debug] [MainThread]: Tracking: tracking
[0m02:09:50.740262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e712b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e712be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e712c40>]}
[0m02:09:50.751516 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m02:09:50.751783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '85238755-0784-4d14-aab4-6586bc071251', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e72c0d0>]}
[0m02:09:50.774007 [debug] [MainThread]: Parsing macros/catalog.sql
[0m02:09:50.776202 [debug] [MainThread]: Parsing macros/adapters.sql
[0m02:09:50.814830 [debug] [MainThread]: Parsing macros/apply_grants.sql
[0m02:09:50.815731 [debug] [MainThread]: Parsing macros/materializations/test.sql
[0m02:09:50.816429 [debug] [MainThread]: Parsing macros/materializations/merge.sql
[0m02:09:50.819422 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m02:09:50.823470 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m02:09:50.824441 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m02:09:50.829060 [debug] [MainThread]: Parsing macros/materializations/incremental.sql
[0m02:09:50.837069 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m02:09:50.837775 [debug] [MainThread]: Parsing macros/utils/timestamps.sql
[0m02:09:50.839190 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m02:09:50.839623 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m02:09:50.840089 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m02:09:50.840446 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m02:09:50.840752 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m02:09:50.841130 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m02:09:50.843999 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m02:09:50.845789 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m02:09:50.847034 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m02:09:50.859757 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m02:09:50.871199 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m02:09:50.881371 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m02:09:50.884827 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m02:09:50.886133 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m02:09:50.887442 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m02:09:50.893707 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m02:09:50.907145 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m02:09:50.908264 [debug] [MainThread]: Parsing macros/materializations/models/incremental/strategies.sql
[0m02:09:50.913350 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m02:09:50.921640 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m02:09:50.936556 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m02:09:50.940857 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m02:09:50.943368 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m02:09:50.947763 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m02:09:50.948687 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m02:09:50.951287 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m02:09:50.952969 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m02:09:50.958774 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m02:09:50.974910 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m02:09:50.976069 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m02:09:50.977892 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m02:09:50.979024 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m02:09:50.979669 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m02:09:50.980235 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m02:09:50.980717 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m02:09:50.981724 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m02:09:50.985880 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m02:09:50.992566 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m02:09:50.993161 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m02:09:50.994036 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m02:09:50.994715 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m02:09:50.995374 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m02:09:50.996272 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m02:09:50.996843 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m02:09:50.997570 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m02:09:50.998443 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m02:09:51.000194 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m02:09:51.001071 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m02:09:51.001825 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m02:09:51.002564 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m02:09:51.003283 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m02:09:51.003931 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m02:09:51.004685 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m02:09:51.005314 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m02:09:51.010753 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m02:09:51.011486 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m02:09:51.012126 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m02:09:51.013396 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m02:09:51.015094 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m02:09:51.015818 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m02:09:51.016878 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m02:09:51.017612 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m02:09:51.019124 [debug] [MainThread]: Parsing macros/adapters/timestamps.sql
[0m02:09:51.021491 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m02:09:51.023495 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m02:09:51.035296 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m02:09:51.036716 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m02:09:51.046828 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m02:09:51.050105 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m02:09:51.055655 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m02:09:51.063168 [debug] [MainThread]: Parsing macros/python_model/python.sql
[0m02:09:51.067870 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m02:09:51.267420 [debug] [MainThread]: 1699: static parser successfully parsed customers.sql
[0m02:09:51.276669 [debug] [MainThread]: 1603: static parser failed on order.sql
[0m02:09:51.280707 [debug] [MainThread]: 1602: parser fallback to jinja rendering on order.sql
[0m02:09:51.281578 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_customers.sql
[0m02:09:51.283685 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_payments.sql
[0m02:09:51.285567 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_orders.sql
[0m02:09:51.320105 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'orders' in the 'models' section of file 'models/schema.yml'
[0m02:09:51.385732 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.jaffle_shop.unique_orders_order_id.fed79b3a6e' (models/schema.yml) depends on a node named 'orders' which was not found
[0m02:09:51.386010 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.jaffle_shop.not_null_orders_order_id.cf6c17daed' (models/schema.yml) depends on a node named 'orders' which was not found
[0m02:09:51.386169 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.jaffle_shop.not_null_orders_customer_id.c5f02694af' (models/schema.yml) depends on a node named 'orders' which was not found
[0m02:09:51.386325 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2' (models/schema.yml) depends on a node named 'orders' which was not found
[0m02:09:51.386483 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3' (models/schema.yml) depends on a node named 'orders' which was not found
[0m02:09:51.386640 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.jaffle_shop.not_null_orders_amount.106140f9fd' (models/schema.yml) depends on a node named 'orders' which was not found
[0m02:09:51.386790 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59' (models/schema.yml) depends on a node named 'orders' which was not found
[0m02:09:51.387013 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625' (models/schema.yml) depends on a node named 'orders' which was not found
[0m02:09:51.387183 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49' (models/schema.yml) depends on a node named 'orders' which was not found
[0m02:09:51.387339 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a' (models/schema.yml) depends on a node named 'orders' which was not found
[0m02:09:51.450704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '85238755-0784-4d14-aab4-6586bc071251', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cb845e0>]}
[0m02:09:51.457979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '85238755-0784-4d14-aab4-6586bc071251', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e72c9a0>]}
[0m02:09:51.458171 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m02:09:51.458346 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '85238755-0784-4d14-aab4-6586bc071251', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ec86e20>]}
[0m02:09:51.459737 [info ] [MainThread]: 
[0m02:09:51.460201 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:09:51.461074 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m02:09:51.472657 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m02:09:51.472822 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m02:09:51.472909 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:09:52.790270 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.32 seconds
[0m02:09:52.795831 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m02:09:53.140629 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m02:09:53.154841 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m02:09:53.155195 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m02:09:53.155407 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m02:09:53.759966 [debug] [ThreadPool]: SQL status: SUCCESS 0 in 0.6 seconds
[0m02:09:53.763874 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m02:09:54.144056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '85238755-0784-4d14-aab4-6586bc071251', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e838b50>]}
[0m02:09:54.145600 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:09:54.146136 [info ] [MainThread]: 
[0m02:09:54.152682 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_customers
[0m02:09:54.153291 [info ] [Thread-1  ]: 1 of 18 START seed file bruno.raw_customers .................................... [RUN]
[0m02:09:54.155316 [debug] [Thread-1  ]: Acquiring new snowflake connection "seed.jaffle_shop.raw_customers"
[0m02:09:54.155624 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_customers
[0m02:09:54.155918 [debug] [Thread-1  ]: finished collecting timing info
[0m02:09:54.156166 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_customers
[0m02:09:54.209990 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m02:09:54.210199 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
create table jaffle_shop.bruno.raw_customers (id integer,first_name text,last_name text)
[0m02:09:54.210314 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:09:55.045995 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.84 seconds
[0m02:09:55.083842 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m02:09:55.084244 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
BEGIN
[0m02:09:55.283269 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.2 seconds
[0m02:09:55.289444 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m02:09:55.289979 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
insert into jaffle_shop.bruno.raw_customers (id, first_name, last_name) values
            (%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s...
[0m02:09:56.326975 [debug] [Thread-1  ]: SQL status: SUCCESS 100 in 1.04 seconds
[0m02:09:56.327629 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m02:09:56.327912 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
COMMIT
[0m02:09:56.600563 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.27 seconds
[0m02:09:56.620103 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_customers"
[0m02:09:56.659225 [debug] [Thread-1  ]: finished collecting timing info
[0m02:09:56.659509 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: Close
[0m02:09:56.989757 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '85238755-0784-4d14-aab4-6586bc071251', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11054cd60>]}
[0m02:09:56.991126 [info ] [Thread-1  ]: 1 of 18 OK loaded seed file bruno.raw_customers ................................ [[32mINSERT 100[0m in 2.84s]
[0m02:09:56.992380 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_customers
[0m02:09:56.992786 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_orders
[0m02:09:56.993805 [info ] [Thread-1  ]: 2 of 18 START seed file bruno.raw_orders ....................................... [RUN]
[0m02:09:56.995816 [debug] [Thread-1  ]: Acquiring new snowflake connection "seed.jaffle_shop.raw_orders"
[0m02:09:56.996161 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_orders
[0m02:09:56.996454 [debug] [Thread-1  ]: finished collecting timing info
[0m02:09:56.996723 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_orders
[0m02:09:57.014996 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m02:09:57.015379 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
create table jaffle_shop.bruno.raw_orders (id integer,user_id integer,order_date date,status text)
[0m02:09:57.015595 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:09:57.645498 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.63 seconds
[0m02:09:57.653871 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m02:09:57.654119 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
BEGIN
[0m02:09:57.960567 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.31 seconds
[0m02:09:57.969924 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m02:09:57.970468 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
insert into jaffle_shop.bruno.raw_orders (id, user_id, order_date, status) values
            (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s...
[0m02:09:58.916766 [debug] [Thread-1  ]: SQL status: SUCCESS 99 in 0.95 seconds
[0m02:09:58.917659 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m02:09:58.917999 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
COMMIT
[0m02:09:59.291222 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.37 seconds
[0m02:09:59.294006 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_orders"
[0m02:09:59.301289 [debug] [Thread-1  ]: finished collecting timing info
[0m02:09:59.301823 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: Close
[0m02:09:59.635997 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '85238755-0784-4d14-aab4-6586bc071251', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11054c280>]}
[0m02:09:59.636908 [info ] [Thread-1  ]: 2 of 18 OK loaded seed file bruno.raw_orders ................................... [[32mINSERT 99[0m in 2.64s]
[0m02:09:59.637672 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_orders
[0m02:09:59.637990 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_payments
[0m02:09:59.638598 [info ] [Thread-1  ]: 3 of 18 START seed file bruno.raw_payments ..................................... [RUN]
[0m02:09:59.640166 [debug] [Thread-1  ]: Acquiring new snowflake connection "seed.jaffle_shop.raw_payments"
[0m02:09:59.640544 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_payments
[0m02:09:59.640847 [debug] [Thread-1  ]: finished collecting timing info
[0m02:09:59.641124 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_payments
[0m02:09:59.657992 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m02:09:59.658342 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
create table jaffle_shop.bruno.raw_payments (id integer,order_id integer,payment_method text,amount integer)
[0m02:09:59.658557 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:10:00.351219 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.69 seconds
[0m02:10:00.362510 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m02:10:00.362751 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
BEGIN
[0m02:10:00.560413 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.2 seconds
[0m02:10:00.570209 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m02:10:00.570741 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
insert into jaffle_shop.bruno.raw_payments (id, order_id, payment_method, amount) values
            (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s...
[0m02:10:01.146483 [debug] [Thread-1  ]: SQL status: SUCCESS 113 in 0.58 seconds
[0m02:10:01.147434 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m02:10:01.147820 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
COMMIT
[0m02:10:01.543671 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.4 seconds
[0m02:10:01.546018 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_payments"
[0m02:10:01.553738 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:01.556905 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: Close
[0m02:10:01.957129 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '85238755-0784-4d14-aab4-6586bc071251', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110495160>]}
[0m02:10:01.958102 [info ] [Thread-1  ]: 3 of 18 OK loaded seed file bruno.raw_payments ................................. [[32mINSERT 113[0m in 2.32s]
[0m02:10:01.959014 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_payments
[0m02:10:01.959392 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m02:10:01.960083 [info ] [Thread-1  ]: 4 of 18 START sql view model bruno.stg_customers ............................... [RUN]
[0m02:10:01.961701 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.stg_customers"
[0m02:10:01.962065 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m02:10:01.962363 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m02:10:01.969383 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m02:10:01.970726 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:01.971012 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m02:10:02.000578 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_customers"
[0m02:10:02.001911 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.stg_customers"
[0m02:10:02.002075 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace  view jaffle_shop.bruno.stg_customers
  
   as (
    with source as (
    select * from jaffle_shop.bruno.raw_customers

),

renamed as (

    select
        id as customer_id,
        first_name,
        last_name

    from source

)

select * from renamed
  );
[0m02:10:02.002213 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:10:02.748078 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.75 seconds
[0m02:10:02.750890 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:02.751158 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m02:10:02.898754 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:10:02.899261 [debug] [MainThread]: Snowflake adapter: Cancelling query 'model.jaffle_shop.stg_customers' (1183654165)
[0m02:10:02.899817 [debug] [MainThread]: Using snowflake connection "master"
[0m02:10:02.900153 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "master"} */
select system$abort_session(1183654165)
[0m02:10:02.901721 [debug] [MainThread]: Opening a new connection, currently in state init
[0m02:10:03.140990 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '85238755-0784-4d14-aab4-6586bc071251', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1105acf40>]}
[0m02:10:03.141559 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:10:03.141753 [debug] [MainThread]: Connection 'model.jaffle_shop.stg_customers' was properly closed.
[0m02:10:03.142058 [info ] [Thread-1  ]: 4 of 18 OK created sql view model bruno.stg_customers .......................... [[32mSUCCESS 1[0m in 1.18s]
[0m02:10:03.142200 [debug] [MainThread]: Flushing usage events
[0m02:10:03.142562 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m02:10:03.282078 [info ] [MainThread]: ctrl-c


============================== 2023-01-24 02:10:16.479378 | 61b7a1e4-52d3-4c84-b034-dba31f823dff ==============================
[0m02:10:16.479459 [info ] [MainThread]: Running with dbt=1.3.2
[0m02:10:16.480451 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'indirect_selection': 'eager', 'resource_types': [], 'which': 'build', 'rpc_method': 'build'}
[0m02:10:16.480623 [debug] [MainThread]: Tracking: tracking
[0m02:10:16.481067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fb94b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fb94be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fb94c40>]}
[0m02:10:16.541022 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m02:10:16.541384 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models/orders.sql
[0m02:10:16.541491 [debug] [MainThread]: Partial parsing: deleted file: jaffle_shop://models/order.sql
[0m02:10:16.554979 [debug] [MainThread]: 1603: static parser failed on orders.sql
[0m02:10:16.567529 [debug] [MainThread]: 1602: parser fallback to jinja rendering on orders.sql
[0m02:10:16.585401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '61b7a1e4-52d3-4c84-b034-dba31f823dff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110116190>]}
[0m02:10:16.592568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '61b7a1e4-52d3-4c84-b034-dba31f823dff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fcba970>]}
[0m02:10:16.592797 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m02:10:16.592985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '61b7a1e4-52d3-4c84-b034-dba31f823dff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1100deaf0>]}
[0m02:10:16.594515 [info ] [MainThread]: 
[0m02:10:16.595003 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:10:16.596189 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m02:10:16.609261 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m02:10:16.609472 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m02:10:16.609574 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:10:17.493589 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 0.88 seconds
[0m02:10:17.498431 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m02:10:17.829519 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m02:10:17.844179 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m02:10:17.844532 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m02:10:17.844750 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m02:10:18.406194 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.56 seconds
[0m02:10:18.410379 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m02:10:18.797321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '61b7a1e4-52d3-4c84-b034-dba31f823dff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110116250>]}
[0m02:10:18.798885 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:10:18.799443 [info ] [MainThread]: 
[0m02:10:18.805530 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_customers
[0m02:10:18.806159 [info ] [Thread-1  ]: 1 of 18 START seed file bruno.raw_customers .................................... [RUN]
[0m02:10:18.808218 [debug] [Thread-1  ]: Acquiring new snowflake connection "seed.jaffle_shop.raw_customers"
[0m02:10:18.808576 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_customers
[0m02:10:18.808860 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:18.809113 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_customers
[0m02:10:18.866283 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m02:10:18.866491 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
BEGIN
[0m02:10:18.866600 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:10:19.454913 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.59 seconds
[0m02:10:19.455759 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m02:10:19.456140 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
truncate table "JAFFLE_SHOP"."BRUNO"."RAW_CUSTOMERS"
  ;
[0m02:10:19.970662 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.51 seconds
[0m02:10:19.971924 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m02:10:19.972219 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
COMMIT
[0m02:10:20.281506 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.31 seconds
[0m02:10:20.314150 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m02:10:20.314511 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
BEGIN
[0m02:10:20.515959 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.2 seconds
[0m02:10:20.522237 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m02:10:20.522935 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
insert into jaffle_shop.bruno.raw_customers (id, first_name, last_name) values
            (%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s...
[0m02:10:21.202156 [debug] [Thread-1  ]: SQL status: SUCCESS 100 in 0.68 seconds
[0m02:10:21.203033 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m02:10:21.203309 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
COMMIT
[0m02:10:21.604262 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.4 seconds
[0m02:10:21.621502 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_customers"
[0m02:10:21.654952 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:21.655172 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: Close
[0m02:10:22.028227 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '61b7a1e4-52d3-4c84-b034-dba31f823dff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11067adf0>]}
[0m02:10:22.029294 [info ] [Thread-1  ]: 1 of 18 OK loaded seed file bruno.raw_customers ................................ [[32mINSERT 100[0m in 3.22s]
[0m02:10:22.030417 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_customers
[0m02:10:22.030792 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_orders
[0m02:10:22.031778 [info ] [Thread-1  ]: 2 of 18 START seed file bruno.raw_orders ....................................... [RUN]
[0m02:10:22.033327 [debug] [Thread-1  ]: Acquiring new snowflake connection "seed.jaffle_shop.raw_orders"
[0m02:10:22.034082 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_orders
[0m02:10:22.034399 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:22.034675 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_orders
[0m02:10:22.054186 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m02:10:22.054508 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
BEGIN
[0m02:10:22.054703 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:10:22.815723 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.76 seconds
[0m02:10:22.816818 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m02:10:22.817335 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
truncate table "JAFFLE_SHOP"."BRUNO"."RAW_ORDERS"
  ;
[0m02:10:23.342938 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.53 seconds
[0m02:10:23.344155 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m02:10:23.344588 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
COMMIT
[0m02:10:23.649236 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.3 seconds
[0m02:10:23.661037 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m02:10:23.661572 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
BEGIN
[0m02:10:23.877710 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.22 seconds
[0m02:10:23.885193 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m02:10:23.885739 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
insert into jaffle_shop.bruno.raw_orders (id, user_id, order_date, status) values
            (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s...
[0m02:10:24.506310 [debug] [Thread-1  ]: SQL status: SUCCESS 99 in 0.62 seconds
[0m02:10:24.508328 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m02:10:24.508881 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
COMMIT
[0m02:10:24.791669 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.28 seconds
[0m02:10:24.794614 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_orders"
[0m02:10:24.801761 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:24.802199 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: Close
[0m02:10:25.134275 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '61b7a1e4-52d3-4c84-b034-dba31f823dff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1113b85e0>]}
[0m02:10:25.134853 [info ] [Thread-1  ]: 2 of 18 OK loaded seed file bruno.raw_orders ................................... [[32mINSERT 99[0m in 3.10s]
[0m02:10:25.135503 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_orders
[0m02:10:25.135814 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_payments
[0m02:10:25.136325 [info ] [Thread-1  ]: 3 of 18 START seed file bruno.raw_payments ..................................... [RUN]
[0m02:10:25.137533 [debug] [Thread-1  ]: Acquiring new snowflake connection "seed.jaffle_shop.raw_payments"
[0m02:10:25.137814 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_payments
[0m02:10:25.138060 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:25.138289 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_payments
[0m02:10:25.153897 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m02:10:25.154170 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
BEGIN
[0m02:10:25.154334 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:10:25.915129 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.76 seconds
[0m02:10:25.915857 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m02:10:25.916149 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
truncate table "JAFFLE_SHOP"."BRUNO"."RAW_PAYMENTS"
  ;
[0m02:10:26.391421 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.47 seconds
[0m02:10:26.392568 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m02:10:26.392902 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
COMMIT
[0m02:10:26.667710 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.27 seconds
[0m02:10:26.679158 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m02:10:26.679671 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
BEGIN
[0m02:10:26.868040 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.19 seconds
[0m02:10:26.876393 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m02:10:26.876969 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
insert into jaffle_shop.bruno.raw_payments (id, order_id, payment_method, amount) values
            (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s...
[0m02:10:27.576547 [debug] [Thread-1  ]: SQL status: SUCCESS 113 in 0.7 seconds
[0m02:10:27.577819 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m02:10:27.578359 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
COMMIT
[0m02:10:27.861186 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.28 seconds
[0m02:10:27.863494 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_payments"
[0m02:10:27.869587 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:27.869921 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: Close
[0m02:10:28.204349 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '61b7a1e4-52d3-4c84-b034-dba31f823dff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1113b8850>]}
[0m02:10:28.205294 [info ] [Thread-1  ]: 3 of 18 OK loaded seed file bruno.raw_payments ................................. [[32mINSERT 113[0m in 3.07s]
[0m02:10:28.206251 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_payments
[0m02:10:28.206641 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m02:10:28.207119 [info ] [Thread-1  ]: 4 of 18 START sql view model bruno.stg_customers ............................... [RUN]
[0m02:10:28.208656 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.stg_customers"
[0m02:10:28.209156 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m02:10:28.209531 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m02:10:28.218151 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m02:10:28.218912 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:28.219182 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m02:10:28.250378 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_customers"
[0m02:10:28.251503 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.stg_customers"
[0m02:10:28.251670 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace  view jaffle_shop.bruno.stg_customers
  
   as (
    with source as (
    select * from jaffle_shop.bruno.raw_customers

),

renamed as (

    select
        id as customer_id,
        first_name,
        last_name

    from source

)

select * from renamed
  );
[0m02:10:28.251809 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:10:29.191912 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.94 seconds
[0m02:10:29.207549 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:29.208299 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m02:10:29.607371 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '61b7a1e4-52d3-4c84-b034-dba31f823dff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1113e6fa0>]}
[0m02:10:29.608743 [info ] [Thread-1  ]: 4 of 18 OK created sql view model bruno.stg_customers .......................... [[32mSUCCESS 1[0m in 1.40s]
[0m02:10:29.610017 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m02:10:29.610590 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m02:10:29.611579 [info ] [Thread-1  ]: 5 of 18 START sql view model bruno.stg_orders .................................. [RUN]
[0m02:10:29.614042 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.stg_orders"
[0m02:10:29.615158 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m02:10:29.615567 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m02:10:29.623347 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m02:10:29.624296 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:29.624556 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m02:10:29.629243 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_orders"
[0m02:10:29.630768 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.stg_orders"
[0m02:10:29.631160 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace  view jaffle_shop.bruno.stg_orders
  
   as (
    with source as (
    select * from jaffle_shop.bruno.raw_orders

),

renamed as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from source

)

select * from renamed
  );
[0m02:10:29.631367 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:10:30.420064 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.79 seconds
[0m02:10:30.427684 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:30.428498 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m02:10:30.780993 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '61b7a1e4-52d3-4c84-b034-dba31f823dff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1113b8a00>]}
[0m02:10:30.782731 [info ] [Thread-1  ]: 5 of 18 OK created sql view model bruno.stg_orders ............................. [[32mSUCCESS 1[0m in 1.17s]
[0m02:10:30.784306 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m02:10:30.785211 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m02:10:30.786713 [info ] [Thread-1  ]: 6 of 18 START sql view model bruno.stg_payments ................................ [RUN]
[0m02:10:30.788321 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.stg_payments"
[0m02:10:30.788708 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m02:10:30.789086 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m02:10:30.804137 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m02:10:30.805367 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:30.805524 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m02:10:30.810052 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_payments"
[0m02:10:30.811735 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.stg_payments"
[0m02:10:30.811895 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace  view jaffle_shop.bruno.stg_payments
  
   as (
    with source as (
    select * from jaffle_shop.bruno.raw_payments

),

renamed as (

    select
        id as payment_id,
        order_id,
        payment_method,

        -- `amount` is currently stored in cents, so we convert it to dollars
        amount / 100 as amount

    from source

)

select * from renamed
  );
[0m02:10:30.812049 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:10:31.497778 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.69 seconds
[0m02:10:31.506030 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:31.506819 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m02:10:31.847800 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '61b7a1e4-52d3-4c84-b034-dba31f823dff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1118d0430>]}
[0m02:10:31.849044 [info ] [Thread-1  ]: 6 of 18 OK created sql view model bruno.stg_payments ........................... [[32mSUCCESS 1[0m in 1.06s]
[0m02:10:31.850231 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m02:10:31.850772 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m02:10:31.851440 [info ] [Thread-1  ]: 7 of 18 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m02:10:31.853129 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m02:10:31.853695 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m02:10:31.854146 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m02:10:31.882385 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m02:10:31.883251 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:31.883491 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m02:10:31.958709 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m02:10:31.959539 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m02:10:31.959638 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from jaffle_shop.bruno.stg_customers
where customer_id is null



      
    ) dbt_internal_test
[0m02:10:31.959727 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:10:32.752049 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.79 seconds
[0m02:10:32.770321 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:32.771040 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m02:10:33.126658 [info ] [Thread-1  ]: 7 of 18 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 1.27s]
[0m02:10:33.128373 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m02:10:33.129398 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m02:10:33.130027 [info ] [Thread-1  ]: 8 of 18 START test unique_stg_customers_customer_id ............................ [RUN]
[0m02:10:33.131898 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m02:10:33.132331 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m02:10:33.132664 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m02:10:33.147879 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m02:10:33.148722 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:33.148955 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m02:10:33.153009 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m02:10:33.154643 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m02:10:33.154877 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from jaffle_shop.bruno.stg_customers
where customer_id is not null
group by customer_id
having count(*) > 1



      
    ) dbt_internal_test
[0m02:10:33.155083 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:10:33.785028 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.63 seconds
[0m02:10:33.791164 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:33.791796 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: Close
[0m02:10:34.220361 [info ] [Thread-1  ]: 8 of 18 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 1.09s]
[0m02:10:34.222329 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m02:10:34.223050 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m02:10:34.223816 [info ] [Thread-1  ]: 9 of 18 START test accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned  [RUN]
[0m02:10:34.226388 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m02:10:34.226994 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m02:10:34.227385 [debug] [Thread-1  ]: Compiling test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m02:10:34.255783 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m02:10:34.256707 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:34.256997 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m02:10:34.261156 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m02:10:34.262780 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m02:10:34.263027 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from jaffle_shop.bruno.stg_orders
    group by status

)

select *
from all_values
where value_field not in (
    'placed','shipped','completed','return_pending','returned'
)



      
    ) dbt_internal_test
[0m02:10:34.263213 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:10:35.180085 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.92 seconds
[0m02:10:35.185078 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:35.185535 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad: Close
[0m02:10:35.640007 [info ] [Thread-1  ]: 9 of 18 PASS accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned  [[32mPASS[0m in 1.42s]
[0m02:10:35.641278 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m02:10:35.641890 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m02:10:35.642775 [info ] [Thread-1  ]: 10 of 18 START test not_null_stg_orders_order_id ............................... [RUN]
[0m02:10:35.645332 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m02:10:35.645893 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m02:10:35.646263 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m02:10:35.658430 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m02:10:35.659384 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:35.659680 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m02:10:35.662941 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m02:10:35.663688 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m02:10:35.663795 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from jaffle_shop.bruno.stg_orders
where order_id is null



      
    ) dbt_internal_test
[0m02:10:35.663884 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:10:36.387892 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.72 seconds
[0m02:10:36.393360 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:36.393920 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: Close
[0m02:10:36.743259 [info ] [Thread-1  ]: 10 of 18 PASS not_null_stg_orders_order_id ..................................... [[32mPASS[0m in 1.10s]
[0m02:10:36.745544 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m02:10:36.746159 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m02:10:36.746921 [info ] [Thread-1  ]: 11 of 18 START test unique_stg_orders_order_id ................................. [RUN]
[0m02:10:36.748825 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m02:10:36.749473 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m02:10:36.749931 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m02:10:36.764708 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m02:10:36.765963 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:36.766320 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m02:10:36.772131 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m02:10:36.773964 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m02:10:36.774204 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    order_id as unique_field,
    count(*) as n_records

from jaffle_shop.bruno.stg_orders
where order_id is not null
group by order_id
having count(*) > 1



      
    ) dbt_internal_test
[0m02:10:36.774421 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:10:37.419799 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.65 seconds
[0m02:10:37.430520 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:37.431534 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: Close
[0m02:10:37.778218 [info ] [Thread-1  ]: 11 of 18 PASS unique_stg_orders_order_id ....................................... [[32mPASS[0m in 1.03s]
[0m02:10:37.780090 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m02:10:37.780600 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m02:10:37.781201 [info ] [Thread-1  ]: 12 of 18 START test accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card  [RUN]
[0m02:10:37.782823 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m02:10:37.783435 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m02:10:37.783803 [debug] [Thread-1  ]: Compiling test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m02:10:37.802211 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m02:10:37.803276 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:37.803551 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m02:10:37.808284 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m02:10:37.810097 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m02:10:37.810331 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        payment_method as value_field,
        count(*) as n_records

    from jaffle_shop.bruno.stg_payments
    group by payment_method

)

select *
from all_values
where value_field not in (
    'credit_card','coupon','bank_transfer','gift_card'
)



      
    ) dbt_internal_test
[0m02:10:37.810552 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:10:38.690373 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.88 seconds
[0m02:10:38.700107 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:38.700833 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278: Close
[0m02:10:39.044928 [info ] [Thread-1  ]: 12 of 18 PASS accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card  [[32mPASS[0m in 1.26s]
[0m02:10:39.047102 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m02:10:39.047949 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m02:10:39.048890 [info ] [Thread-1  ]: 13 of 18 START test not_null_stg_payments_payment_id ........................... [RUN]
[0m02:10:39.050918 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m02:10:39.051513 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m02:10:39.051981 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m02:10:39.066305 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m02:10:39.067321 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:39.067639 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m02:10:39.073108 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m02:10:39.074449 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m02:10:39.074692 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select payment_id
from jaffle_shop.bruno.stg_payments
where payment_id is null



      
    ) dbt_internal_test
[0m02:10:39.074914 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:10:39.938367 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.86 seconds
[0m02:10:39.944097 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:39.944582 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075: Close
[0m02:10:40.280452 [info ] [Thread-1  ]: 13 of 18 PASS not_null_stg_payments_payment_id ................................. [[32mPASS[0m in 1.23s]
[0m02:10:40.282155 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m02:10:40.282810 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m02:10:40.283261 [info ] [Thread-1  ]: 14 of 18 START test unique_stg_payments_payment_id ............................. [RUN]
[0m02:10:40.284881 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m02:10:40.285339 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m02:10:40.285637 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m02:10:40.299048 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m02:10:40.300274 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:40.300601 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m02:10:40.306264 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m02:10:40.307924 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m02:10:40.308158 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_payments_payment_id.3744510712: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    payment_id as unique_field,
    count(*) as n_records

from jaffle_shop.bruno.stg_payments
where payment_id is not null
group by payment_id
having count(*) > 1



      
    ) dbt_internal_test
[0m02:10:40.308366 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:10:41.071356 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.76 seconds
[0m02:10:41.078992 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:41.079777 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_payments_payment_id.3744510712: Close
[0m02:10:41.490133 [info ] [Thread-1  ]: 14 of 18 PASS unique_stg_payments_payment_id ................................... [[32mPASS[0m in 1.21s]
[0m02:10:41.491969 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m02:10:41.493840 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m02:10:41.494820 [info ] [Thread-1  ]: 15 of 18 START sql table model bruno.customers ................................. [RUN]
[0m02:10:41.497187 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m02:10:41.497770 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m02:10:41.498157 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m02:10:41.508639 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m02:10:41.509777 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:41.510117 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m02:10:41.545837 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.customers"
[0m02:10:41.548312 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m02:10:41.548452 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
create or replace transient table jaffle_shop.bruno.customers  as
        (with customers as (

    select * from jaffle_shop.bruno.stg_customers

),

orders as (

    select * from jaffle_shop.bruno.stg_orders

),

payments as (

    select * from jaffle_shop.bruno.stg_payments

),

customer_orders as (

        select
        customer_id,

        min(order_date) as first_order,
        max(order_date) as most_recent_order,
        count(order_id) as number_of_orders
    from orders

    group by customer_id

),

customer_payments as (

    select
        orders.customer_id,
        sum(amount) as total_amount

    from payments

    left join orders on
         payments.order_id = orders.order_id

    group by orders.customer_id

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order,
        customer_orders.most_recent_order,
        customer_orders.number_of_orders,
        customer_payments.total_amount as customer_lifetime_value

    from customers

    left join customer_orders
        on customers.customer_id = customer_orders.customer_id

    left join customer_payments
        on  customers.customer_id = customer_payments.customer_id

)

select * from final
        );
[0m02:10:41.548575 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:10:43.016855 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 1.47 seconds
[0m02:10:43.022547 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:43.023023 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m02:10:43.363571 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '61b7a1e4-52d3-4c84-b034-dba31f823dff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111930a00>]}
[0m02:10:43.365277 [info ] [Thread-1  ]: 15 of 18 OK created sql table model bruno.customers ............................ [[32mSUCCESS 1[0m in 1.87s]
[0m02:10:43.366826 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m02:10:43.367438 [debug] [Thread-1  ]: Began running node model.jaffle_shop.orders
[0m02:10:43.368458 [info ] [Thread-1  ]: 16 of 18 START sql table model bruno.orders .................................... [RUN]
[0m02:10:43.371099 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.orders"
[0m02:10:43.371642 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.orders
[0m02:10:43.372023 [debug] [Thread-1  ]: Compiling model.jaffle_shop.orders
[0m02:10:43.385814 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.orders"
[0m02:10:43.387080 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:43.387411 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.orders
[0m02:10:43.393082 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.orders"
[0m02:10:43.397034 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.orders"
[0m02:10:43.397266 [debug] [Thread-1  ]: On model.jaffle_shop.orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.orders"} */
create or replace transient table jaffle_shop.bruno.orders  as
        (

with orders as (

    select * from jaffle_shop.bruno.stg_orders

),

payments as (

    select * from jaffle_shop.bruno.stg_payments

),

order_payments as (

    select
        order_id,

        sum(case when payment_method = 'credit_card' then amount else 0 end) as credit_card_amount,
        sum(case when payment_method = 'coupon' then amount else 0 end) as coupon_amount,
        sum(case when payment_method = 'bank_transfer' then amount else 0 end) as bank_transfer_amount,
        sum(case when payment_method = 'gift_card' then amount else 0 end) as gift_card_amount,
        sum(amount) as total_amount

    from payments

    group by order_id

),

final as (

    select
        orders.order_id,
        orders.customer_id,
        orders.order_date,
        orders.status,

        order_payments.credit_card_amount,

        order_payments.coupon_amount,

        order_payments.bank_transfer_amount,

        order_payments.gift_card_amount,

        order_payments.total_amount as amount

    from orders


    left join order_payments
        on orders.order_id = order_payments.order_id

)

select * from final
        );
[0m02:10:43.397473 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:10:44.570633 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 1.17 seconds
[0m02:10:44.576664 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:44.577380 [debug] [Thread-1  ]: On model.jaffle_shop.orders: Close
[0m02:10:44.921961 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '61b7a1e4-52d3-4c84-b034-dba31f823dff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1113bb670>]}
[0m02:10:44.922974 [info ] [Thread-1  ]: 16 of 18 OK created sql table model bruno.orders ............................... [[32mSUCCESS 1[0m in 1.55s]
[0m02:10:44.923884 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.orders
[0m02:10:44.924329 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m02:10:44.924650 [info ] [Thread-1  ]: 17 of 18 START test not_null_customers_customer_id ............................. [RUN]
[0m02:10:44.926454 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m02:10:44.927094 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m02:10:44.927544 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m02:10:44.940948 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m02:10:44.942073 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:44.942398 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m02:10:44.947922 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m02:10:44.949727 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m02:10:44.949991 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from jaffle_shop.bruno.customers
where customer_id is null



      
    ) dbt_internal_test
[0m02:10:44.950201 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:10:45.673997 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.72 seconds
[0m02:10:45.683497 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:45.684490 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d: Close
[0m02:10:46.035735 [info ] [Thread-1  ]: 17 of 18 PASS not_null_customers_customer_id ................................... [[32mPASS[0m in 1.11s]
[0m02:10:46.037512 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m02:10:46.038240 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m02:10:46.039100 [info ] [Thread-1  ]: 18 of 18 START test unique_customers_customer_id ............................... [RUN]
[0m02:10:46.040905 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m02:10:46.041416 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m02:10:46.041792 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m02:10:46.056647 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m02:10:46.057734 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:46.058064 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m02:10:46.063543 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m02:10:46.065132 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m02:10:46.065372 [debug] [Thread-1  ]: On test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from jaffle_shop.bruno.customers
where customer_id is not null
group by customer_id
having count(*) > 1



      
    ) dbt_internal_test
[0m02:10:46.065585 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:10:46.775715 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.71 seconds
[0m02:10:46.786343 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:46.787045 [debug] [Thread-1  ]: On test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1: Close
[0m02:10:47.130220 [info ] [Thread-1  ]: 18 of 18 PASS unique_customers_customer_id ..................................... [[32mPASS[0m in 1.09s]
[0m02:10:47.132438 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m02:10:47.136929 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:10:47.138900 [info ] [MainThread]: 
[0m02:10:47.139671 [info ] [MainThread]: Finished running 3 seeds, 3 view models, 10 tests, 2 table models in 0 hours 0 minutes and 30.54 seconds (30.54s).
[0m02:10:47.140325 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:10:47.141171 [debug] [MainThread]: Connection 'test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1' was properly closed.
[0m02:10:47.161628 [info ] [MainThread]: 
[0m02:10:47.162146 [info ] [MainThread]: [32mCompleted successfully[0m
[0m02:10:47.162632 [info ] [MainThread]: 
[0m02:10:47.162984 [info ] [MainThread]: Done. PASS=18 WARN=0 ERROR=0 SKIP=0 TOTAL=18
[0m02:10:47.163473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1101040d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1118ee580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1118ee640>]}
[0m02:10:47.163852 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 02:17:43.859595 | b03da7d5-8845-49d2-9cd6-9ca823c4c9eb ==============================
[0m02:17:43.859648 [info ] [MainThread]: Running with dbt=1.3.2
[0m02:17:43.860396 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.sql', 'orders.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m02:17:43.860544 [debug] [MainThread]: Tracking: tracking
[0m02:17:43.860902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dc9ebe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dc9ec40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dc9eca0>]}
[0m02:17:43.912412 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m02:17:43.912742 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models/customers.py
[0m02:17:43.924460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dd84b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e459b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e459d00>]}
[0m02:17:43.924666 [debug] [MainThread]: Flushing usage events
[0m02:17:44.750066 [error] [MainThread]: Encountered an error:
Parsing Error in model customers (models/customers.py)
  expected an indented block (customers.py, line 3)
  
  


============================== 2023-01-24 02:18:01.476662 | ea99a81f-4703-47de-a4e7-f351daabd583 ==============================
[0m02:18:01.476736 [info ] [MainThread]: Running with dbt=1.3.2
[0m02:18:01.477580 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.sql', 'orders.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m02:18:01.477732 [debug] [MainThread]: Tracking: tracking
[0m02:18:01.478116 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dcd2b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dcd2be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dcd2c40>]}
[0m02:18:01.528503 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:18:01.528697 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:18:01.534493 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ea99a81f-4703-47de-a4e7-f351daabd583', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e270e80>]}
[0m02:18:01.541410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ea99a81f-4703-47de-a4e7-f351daabd583', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ddbb490>]}
[0m02:18:01.541614 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m02:18:01.541795 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ea99a81f-4703-47de-a4e7-f351daabd583', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ddbb520>]}
[0m02:18:01.543093 [info ] [MainThread]: 
[0m02:18:01.543559 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:18:01.544384 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m02:18:01.556642 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m02:18:01.556823 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m02:18:01.556920 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:18:02.618264 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.06 seconds
[0m02:18:02.621429 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m02:18:02.960911 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m02:18:02.974985 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m02:18:02.975329 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m02:18:02.975545 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m02:18:03.537420 [debug] [ThreadPool]: SQL status: SUCCESS 8 in 0.56 seconds
[0m02:18:03.542705 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m02:18:03.890794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ea99a81f-4703-47de-a4e7-f351daabd583', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e3564c0>]}
[0m02:18:03.892192 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:18:03.892755 [info ] [MainThread]: 
[0m02:18:03.900185 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m02:18:03.900862 [info ] [Thread-1  ]: 1 of 2 START sql table model bruno.customers ................................... [RUN]
[0m02:18:03.902711 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m02:18:03.903016 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m02:18:03.903281 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m02:18:03.909633 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m02:18:03.910686 [debug] [Thread-1  ]: finished collecting timing info
[0m02:18:03.910985 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m02:18:03.953503 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.customers"
[0m02:18:03.955945 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m02:18:03.956081 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
create or replace transient table jaffle_shop.bruno.customers  as
        (with customers as (

    select * from jaffle_shop.bruno.stg_customers

),

orders as (

    select * from jaffle_shop.bruno.stg_orders

),

payments as (

    select * from jaffle_shop.bruno.stg_payments

),

customer_orders as (

        select
        customer_id,

        min(order_date) as first_order,
        max(order_date) as most_recent_order,
        count(order_id) as number_of_orders
    from orders

    group by customer_id

),

customer_payments as (

    select
        orders.customer_id,
        sum(amount) as total_amount

    from payments

    left join orders on
         payments.order_id = orders.order_id

    group by orders.customer_id

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order,
        customer_orders.most_recent_order,
        customer_orders.number_of_orders,
        customer_payments.total_amount as customer_lifetime_value

    from customers

    left join customer_orders
        on customers.customer_id = customer_orders.customer_id

    left join customer_payments
        on  customers.customer_id = customer_payments.customer_id

)

select * from final
        );
[0m02:18:03.956202 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:18:05.389463 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 1.43 seconds
[0m02:18:05.435370 [debug] [Thread-1  ]: finished collecting timing info
[0m02:18:05.435700 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m02:18:05.810064 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ea99a81f-4703-47de-a4e7-f351daabd583', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e355520>]}
[0m02:18:05.810978 [info ] [Thread-1  ]: 1 of 2 OK created sql table model bruno.customers .............................. [[32mSUCCESS 1[0m in 1.91s]
[0m02:18:05.812023 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m02:18:05.812393 [debug] [Thread-1  ]: Began running node model.jaffle_shop.orders
[0m02:18:05.813246 [info ] [Thread-1  ]: 2 of 2 START sql table model bruno.orders ...................................... [RUN]
[0m02:18:05.814521 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.orders"
[0m02:18:05.814835 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.orders
[0m02:18:05.815112 [debug] [Thread-1  ]: Compiling model.jaffle_shop.orders
[0m02:18:05.824486 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.orders"
[0m02:18:05.825369 [debug] [Thread-1  ]: finished collecting timing info
[0m02:18:05.825637 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.orders
[0m02:18:05.830374 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.orders"
[0m02:18:05.833974 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.orders"
[0m02:18:05.834250 [debug] [Thread-1  ]: On model.jaffle_shop.orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.orders"} */
create or replace transient table jaffle_shop.bruno.orders  as
        (

with orders as (

    select * from jaffle_shop.bruno.stg_orders

),

payments as (

    select * from jaffle_shop.bruno.stg_payments

),

order_payments as (

    select
        order_id,

        sum(case when payment_method = 'credit_card' then amount else 0 end) as credit_card_amount,
        sum(case when payment_method = 'coupon' then amount else 0 end) as coupon_amount,
        sum(case when payment_method = 'bank_transfer' then amount else 0 end) as bank_transfer_amount,
        sum(case when payment_method = 'gift_card' then amount else 0 end) as gift_card_amount,
        sum(amount) as total_amount

    from payments

    group by order_id

),

final as (

    select
        orders.order_id,
        orders.customer_id,
        orders.order_date,
        orders.status,

        order_payments.credit_card_amount,

        order_payments.coupon_amount,

        order_payments.bank_transfer_amount,

        order_payments.gift_card_amount,

        order_payments.total_amount as amount

    from orders


    left join order_payments
        on orders.order_id = order_payments.order_id

)

select * from final
        );
[0m02:18:05.834431 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:18:07.249240 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 1.41 seconds
[0m02:18:07.257402 [debug] [Thread-1  ]: finished collecting timing info
[0m02:18:07.257810 [debug] [Thread-1  ]: On model.jaffle_shop.orders: Close
[0m02:18:07.597715 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ea99a81f-4703-47de-a4e7-f351daabd583', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f546ee0>]}
[0m02:18:07.598608 [info ] [Thread-1  ]: 2 of 2 OK created sql table model bruno.orders ................................. [[32mSUCCESS 1[0m in 1.78s]
[0m02:18:07.599421 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.orders
[0m02:18:07.601851 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:18:07.602829 [info ] [MainThread]: 
[0m02:18:07.603317 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 6.06 seconds (6.06s).
[0m02:18:07.603781 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:18:07.604016 [debug] [MainThread]: Connection 'model.jaffle_shop.orders' was properly closed.
[0m02:18:07.619376 [info ] [MainThread]: 
[0m02:18:07.619852 [info ] [MainThread]: [32mCompleted successfully[0m
[0m02:18:07.620258 [info ] [MainThread]: 
[0m02:18:07.620565 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m02:18:07.620986 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e270fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ddbb130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f48c580>]}
[0m02:18:07.621303 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 02:26:47.336191 | 20b69895-7dd6-4950-947b-2667636d4d52 ==============================
[0m02:26:47.336249 [info ] [MainThread]: Running with dbt=1.3.2
[0m02:26:47.337051 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['orders.py', 'customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m02:26:47.337188 [debug] [MainThread]: Tracking: tracking
[0m02:26:47.337833 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dea9bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dea9c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dea9c70>]}
[0m02:26:47.393716 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 2 files added, 0 files changed.
[0m02:26:47.394043 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models/orders.py
[0m02:26:47.394183 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models/customers.py
[0m02:26:47.394305 [debug] [MainThread]: Partial parsing: deleted file: jaffle_shop://models/customers.sql
[0m02:26:47.394389 [debug] [MainThread]: Partial parsing: deleted file: jaffle_shop://models/orders.sql
[0m02:26:47.406079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10df8ecd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e659d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e659eb0>]}
[0m02:26:47.406272 [debug] [MainThread]: Flushing usage events
[0m02:26:48.348177 [error] [MainThread]: Encountered an error:
Parsing Error in model orders (models/orders.py)
  unexpected indent (orders.py, line 25)
  
  


============================== 2023-01-24 02:27:34.029309 | 3a7e5744-9f20-4dc1-946d-bd889d56e3a3 ==============================
[0m02:27:34.029376 [info ] [MainThread]: Running with dbt=1.3.2
[0m02:27:34.030222 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['orders.py', 'customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m02:27:34.030360 [debug] [MainThread]: Tracking: tracking
[0m02:27:34.030701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dae6b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dae6bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dae6c10>]}
[0m02:27:34.086644 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 2 files added, 0 files changed.
[0m02:27:34.087009 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models/customers.py
[0m02:27:34.087147 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models/orders.py
[0m02:27:34.087241 [debug] [MainThread]: Partial parsing: deleted file: jaffle_shop://models/orders.sql
[0m02:27:34.087349 [debug] [MainThread]: Partial parsing: deleted file: jaffle_shop://models/customers.sql
[0m02:27:34.146158 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3a7e5744-9f20-4dc1-946d-bd889d56e3a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e11a0d0>]}
[0m02:27:34.153182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3a7e5744-9f20-4dc1-946d-bd889d56e3a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dc0c3a0>]}
[0m02:27:34.153410 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m02:27:34.153589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3a7e5744-9f20-4dc1-946d-bd889d56e3a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d96c0a0>]}
[0m02:27:34.154853 [info ] [MainThread]: 
[0m02:27:34.155317 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:27:34.156058 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m02:27:34.166908 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m02:27:34.167071 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m02:27:34.167172 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:27:35.415291 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.25 seconds
[0m02:27:35.419769 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m02:27:35.881783 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m02:27:35.894780 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m02:27:35.895102 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m02:27:35.895295 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m02:27:36.594414 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 0.7 seconds
[0m02:27:36.600239 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m02:27:36.974368 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3a7e5744-9f20-4dc1-946d-bd889d56e3a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dbc9100>]}
[0m02:27:36.976025 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:27:36.976849 [info ] [MainThread]: 
[0m02:27:36.982206 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m02:27:36.982607 [info ] [Thread-1  ]: 1 of 2 START python table model bruno.customers ................................ [RUN]
[0m02:27:36.983592 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m02:27:36.983764 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m02:27:36.983905 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m02:27:37.002708 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m02:27:37.003623 [debug] [Thread-1  ]: finished collecting timing info
[0m02:27:37.003874 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m02:27:37.037578 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m02:27:37.039953 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m02:27:37.040092 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.customers__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
def model(dbt, session):

    stg_customers = dbt.ref('stg_customers')
    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    customer_orders = stg_orders.groupby('customer_id').agg({
        'first_order': min(stg_orders.order_date),
        'most_recent_order': max(stg_orders.order_date),
        'number_of_orders': count(stg_orders.order_id)
    })

    customer_payments = stg_payments.merge(stg_orders, on='order_id', how='left').groupby('customer_id').agg({
        'total_amount': sum(stg_payments.amount)
    })

    final_df = stg_customers.merge(customer_orders, on='customer_id', how='left').merge(customer_payments, on='customer_id', how='left')

    final_df.select(
        'customer_id',
        'first_name',
        'last_name',
        'first_order',
        'most_recent_order',
        'number_of_orders',
        'customer_lifetime_value'
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle_shop.bruno.stg_customers", "stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle_shop.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.customers", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m02:27:37.040202 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:28:01.350186 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 24.31 seconds
[0m02:28:01.353405 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m02:28:01.354000 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CALL jaffle_shop.bruno.customers__dbt_sp();
[0m02:28:02.381506 [debug] [Thread-1  ]: Snowflake adapter: Snowflake query id: 01a9dab4-0000-0c55-0000-0000468d26c5
[0m02:28:02.381706 [debug] [Thread-1  ]: Snowflake adapter: Snowflake error: 100357 (P0000): Python Interpreter Error:
Traceback (most recent call last):
  File "_udf_code.py", line 109, in main
  File "_udf_code.py", line 11, in model
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 848, in __getattr__
    raise AttributeError(
AttributeError: Table object has no attribute groupby
 in function CUSTOMERS__DBT_SP with handler main
[0m02:28:02.382329 [debug] [Thread-1  ]: finished collecting timing info
[0m02:28:02.382577 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m02:28:02.730286 [debug] [Thread-1  ]: Database Error in model customers (models/customers.py)
  100357 (P0000): Python Interpreter Error:
  Traceback (most recent call last):
    File "_udf_code.py", line 109, in main
    File "_udf_code.py", line 11, in model
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 848, in __getattr__
      raise AttributeError(
  AttributeError: Table object has no attribute groupby
   in function CUSTOMERS__DBT_SP with handler main
  compiled Code at target/run/jaffle_shop/models/customers.py
[0m02:28:02.732533 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3a7e5744-9f20-4dc1-946d-bd889d56e3a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f316370>]}
[0m02:28:02.734712 [error] [Thread-1  ]: 1 of 2 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 25.75s]
[0m02:28:02.737324 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m02:28:02.738680 [debug] [Thread-1  ]: Began running node model.jaffle_shop.orders
[0m02:28:02.740980 [info ] [Thread-1  ]: 2 of 2 START python table model bruno.orders ................................... [RUN]
[0m02:28:02.743060 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.orders"
[0m02:28:02.743537 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.orders
[0m02:28:02.744024 [debug] [Thread-1  ]: Compiling model.jaffle_shop.orders
[0m02:28:02.755853 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.orders"
[0m02:28:02.756833 [debug] [Thread-1  ]: finished collecting timing info
[0m02:28:02.757161 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.orders
[0m02:28:02.762513 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.orders"
[0m02:28:02.766312 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.orders"
[0m02:28:02.766521 [debug] [Thread-1  ]: On model.jaffle_shop.orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.orders"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.orders__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
def model(dbt, session):

    payment_methods = ['credit_card', 'coupon', 'bank_transfer', 'gift_card']

    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    order_payments = stg_payments.groupby('order_id').agg({
        payment_method + '_amount': sum(
            stg_payments.amount if stg_payments.payment_method == payment_method else 0)
        for payment_method in payment_methods
    }).assign(total_amount=stg_payments.amount.sum())

    final_df = stg_orders.merge(order_payments, on='order_id', how='left')

    final_df.select(
        'order_id',
        'customer_id',
        'order_date',
        'status',
        *[payment_method + '_amount' for payment_method in payment_methods],
        'total_amount'
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'orders'
    def __repr__(self):
        return 'jaffle_shop.bruno.orders'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.orders", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m02:28:02.766750 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:28:08.873528 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 6.11 seconds
[0m02:28:08.874766 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.orders"
[0m02:28:08.875140 [debug] [Thread-1  ]: On model.jaffle_shop.orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.orders"} */
CALL jaffle_shop.bruno.orders__dbt_sp();
[0m02:28:09.799169 [debug] [Thread-1  ]: Snowflake adapter: Snowflake query id: 01a9dab4-0000-0c55-0000-0000468d26cd
[0m02:28:09.800745 [debug] [Thread-1  ]: Snowflake adapter: Snowflake error: 100357 (P0000): Python Interpreter Error:
Traceback (most recent call last):
  File "_udf_code.py", line 105, in main
  File "_udf_code.py", line 12, in model
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 848, in __getattr__
    raise AttributeError(
AttributeError: Table object has no attribute groupby
 in function ORDERS__DBT_SP with handler main
[0m02:28:09.801756 [debug] [Thread-1  ]: finished collecting timing info
[0m02:28:09.802193 [debug] [Thread-1  ]: On model.jaffle_shop.orders: Close
[0m02:28:10.155432 [debug] [Thread-1  ]: Database Error in model orders (models/orders.py)
  100357 (P0000): Python Interpreter Error:
  Traceback (most recent call last):
    File "_udf_code.py", line 105, in main
    File "_udf_code.py", line 12, in model
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 848, in __getattr__
      raise AttributeError(
  AttributeError: Table object has no attribute groupby
   in function ORDERS__DBT_SP with handler main
  compiled Code at target/run/jaffle_shop/models/orders.py
[0m02:28:10.156777 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3a7e5744-9f20-4dc1-946d-bd889d56e3a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e1347f0>]}
[0m02:28:10.157888 [error] [Thread-1  ]: 2 of 2 ERROR creating python table model bruno.orders .......................... [[31mERROR[0m in 7.41s]
[0m02:28:10.159062 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.orders
[0m02:28:10.165133 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:28:10.166699 [info ] [MainThread]: 
[0m02:28:10.167372 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 36.01 seconds (36.01s).
[0m02:28:10.168423 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:28:10.168780 [debug] [MainThread]: Connection 'model.jaffle_shop.orders' was properly closed.
[0m02:28:10.187009 [info ] [MainThread]: 
[0m02:28:10.187508 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m02:28:10.187986 [info ] [MainThread]: 
[0m02:28:10.188351 [error] [MainThread]: [33mDatabase Error in model customers (models/customers.py)[0m
[0m02:28:10.188707 [error] [MainThread]:   100357 (P0000): Python Interpreter Error:
[0m02:28:10.189053 [error] [MainThread]:   Traceback (most recent call last):
[0m02:28:10.189381 [error] [MainThread]:     File "_udf_code.py", line 109, in main
[0m02:28:10.189709 [error] [MainThread]:     File "_udf_code.py", line 11, in model
[0m02:28:10.190040 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 848, in __getattr__
[0m02:28:10.190369 [error] [MainThread]:       raise AttributeError(
[0m02:28:10.190693 [error] [MainThread]:   AttributeError: Table object has no attribute groupby
[0m02:28:10.191019 [error] [MainThread]:    in function CUSTOMERS__DBT_SP with handler main
[0m02:28:10.191315 [error] [MainThread]:   compiled Code at target/run/jaffle_shop/models/customers.py
[0m02:28:10.191608 [info ] [MainThread]: 
[0m02:28:10.191892 [error] [MainThread]: [33mDatabase Error in model orders (models/orders.py)[0m
[0m02:28:10.192177 [error] [MainThread]:   100357 (P0000): Python Interpreter Error:
[0m02:28:10.192454 [error] [MainThread]:   Traceback (most recent call last):
[0m02:28:10.192730 [error] [MainThread]:     File "_udf_code.py", line 105, in main
[0m02:28:10.193006 [error] [MainThread]:     File "_udf_code.py", line 12, in model
[0m02:28:10.193277 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 848, in __getattr__
[0m02:28:10.193554 [error] [MainThread]:       raise AttributeError(
[0m02:28:10.193827 [error] [MainThread]:   AttributeError: Table object has no attribute groupby
[0m02:28:10.194117 [error] [MainThread]:    in function ORDERS__DBT_SP with handler main
[0m02:28:10.194388 [error] [MainThread]:   compiled Code at target/run/jaffle_shop/models/orders.py
[0m02:28:10.194694 [info ] [MainThread]: 
[0m02:28:10.194997 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=0 TOTAL=2
[0m02:28:10.195470 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dafa1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f31f1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f3166d0>]}
[0m02:28:10.195790 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 02:31:02.613375 | ab83f6ff-07e9-4d10-9581-51fead052251 ==============================
[0m02:31:02.613459 [info ] [MainThread]: Running with dbt=1.3.2
[0m02:31:02.614517 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['orders.py', 'customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m02:31:02.616234 [debug] [MainThread]: Tracking: tracking
[0m02:31:02.616829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11205bbb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11205bc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11205bc70>]}
[0m02:31:02.686177 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m02:31:02.686808 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/orders.py
[0m02:31:02.687033 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m02:31:02.747910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ab83f6ff-07e9-4d10-9581-51fead052251', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1128bb0d0>]}
[0m02:31:02.755487 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ab83f6ff-07e9-4d10-9581-51fead052251', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1125b40d0>]}
[0m02:31:02.755780 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m02:31:02.755980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ab83f6ff-07e9-4d10-9581-51fead052251', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1125b4220>]}
[0m02:31:02.757300 [info ] [MainThread]: 
[0m02:31:02.757779 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:31:02.758540 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m02:31:02.770165 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m02:31:02.770460 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m02:31:02.770563 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:31:04.065399 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.29 seconds
[0m02:31:04.070759 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m02:31:04.478644 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m02:31:04.493084 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m02:31:04.493456 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m02:31:04.493677 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m02:31:05.288529 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 0.79 seconds
[0m02:31:05.291859 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m02:31:05.627764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ab83f6ff-07e9-4d10-9581-51fead052251', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112186340>]}
[0m02:31:05.629112 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:31:05.629656 [info ] [MainThread]: 
[0m02:31:05.636210 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m02:31:05.636706 [info ] [Thread-1  ]: 1 of 2 START python table model bruno.customers ................................ [RUN]
[0m02:31:05.638399 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m02:31:05.638732 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m02:31:05.639012 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m02:31:05.667574 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m02:31:05.668658 [debug] [Thread-1  ]: finished collecting timing info
[0m02:31:05.668896 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m02:31:05.701959 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m02:31:05.704199 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m02:31:05.704324 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.customers__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
def model(dbt, session):

    stg_customers = dbt.ref('stg_customers')
    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    customer_orders = stg_orders.group_by('customer_id').agg({
        'first_order': min(stg_orders.order_date),
        'most_recent_order': max(stg_orders.order_date),
        'number_of_orders': count(stg_orders.order_id)
    })

    customer_payments = stg_payments.merge(stg_orders, on='order_id', how='left').group_by('customer_id').agg({
        'total_amount': sum(stg_payments.amount)
    })

    final_df = stg_customers.merge(customer_orders, on='customer_id', how='left').merge(customer_payments, on='customer_id', how='left')

    final_df.select(
        'customer_id',
        'first_name',
        'last_name',
        'first_order',
        'most_recent_order',
        'number_of_orders',
        'customer_lifetime_value'
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle_shop.bruno.stg_customers", "stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle_shop.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.customers", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m02:31:05.704428 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:31:11.430339 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.73 seconds
[0m02:31:11.431271 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m02:31:11.431453 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CALL jaffle_shop.bruno.customers__dbt_sp();
[0m02:31:12.450238 [debug] [Thread-1  ]: Snowflake adapter: Snowflake query id: 01a9dab7-0000-0c55-0000-0000468d26d9
[0m02:31:12.450464 [debug] [Thread-1  ]: Snowflake adapter: Snowflake error: 100357 (P0000): Python Interpreter Error:
Traceback (most recent call last):
  File "_udf_code.py", line 109, in main
  File "_udf_code.py", line 12, in model
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/column.py", line 308, in __iter__
    raise TypeError(
TypeError: Column is not iterable. This error can occur when you use the Python built-ins for sum, min and max. Please make sure you use the corresponding function from snowflake.snowpark.functions.
 in function CUSTOMERS__DBT_SP with handler main
[0m02:31:12.450794 [debug] [Thread-1  ]: finished collecting timing info
[0m02:31:12.450960 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m02:31:12.871123 [debug] [Thread-1  ]: Database Error in model customers (models/customers.py)
  100357 (P0000): Python Interpreter Error:
  Traceback (most recent call last):
    File "_udf_code.py", line 109, in main
    File "_udf_code.py", line 12, in model
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/column.py", line 308, in __iter__
      raise TypeError(
  TypeError: Column is not iterable. This error can occur when you use the Python built-ins for sum, min and max. Please make sure you use the corresponding function from snowflake.snowpark.functions.
   in function CUSTOMERS__DBT_SP with handler main
  compiled Code at target/run/jaffle_shop/models/customers.py
[0m02:31:12.872555 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ab83f6ff-07e9-4d10-9581-51fead052251', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1138af4c0>]}
[0m02:31:12.873829 [error] [Thread-1  ]: 1 of 2 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 7.24s]
[0m02:31:12.875428 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m02:31:12.876038 [debug] [Thread-1  ]: Began running node model.jaffle_shop.orders
[0m02:31:12.877383 [info ] [Thread-1  ]: 2 of 2 START python table model bruno.orders ................................... [RUN]
[0m02:31:12.879372 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.orders"
[0m02:31:12.879891 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.orders
[0m02:31:12.880257 [debug] [Thread-1  ]: Compiling model.jaffle_shop.orders
[0m02:31:12.890663 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.orders"
[0m02:31:12.892705 [debug] [Thread-1  ]: finished collecting timing info
[0m02:31:12.893042 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.orders
[0m02:31:12.898150 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.orders"
[0m02:31:12.902172 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.orders"
[0m02:31:12.902407 [debug] [Thread-1  ]: On model.jaffle_shop.orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.orders"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.orders__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
def model(dbt, session):

    payment_methods = ['credit_card', 'coupon', 'bank_transfer', 'gift_card']

    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    order_payments = stg_payments.group_by('order_id').agg({
        payment_method + '_amount': sum(
            stg_payments.amount if stg_payments.payment_method == payment_method else 0)
        for payment_method in payment_methods
    }).assign(total_amount=stg_payments.amount.sum())

    final_df = stg_orders.merge(order_payments, on='order_id', how='left')

    final_df.select(
        'order_id',
        'customer_id',
        'order_date',
        'status',
        *[payment_method + '_amount' for payment_method in payment_methods],
        'total_amount'
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'orders'
    def __repr__(self):
        return 'jaffle_shop.bruno.orders'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.orders", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m02:31:12.902614 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:31:18.826088 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.92 seconds
[0m02:31:18.828604 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.orders"
[0m02:31:18.828889 [debug] [Thread-1  ]: On model.jaffle_shop.orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.orders"} */
CALL jaffle_shop.bruno.orders__dbt_sp();
[0m02:31:19.949244 [debug] [Thread-1  ]: Snowflake adapter: Snowflake query id: 01a9dab7-0000-0c55-0000-0000468d26e5
[0m02:31:19.949548 [debug] [Thread-1  ]: Snowflake adapter: Snowflake error: 100357 (P0000): Python Interpreter Error:
Traceback (most recent call last):
  File "_udf_code.py", line 105, in main
  File "_udf_code.py", line 12, in model
  File "_udf_code.py", line 14, in <dictcomp>
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/column.py", line 302, in __bool__
    raise TypeError(
TypeError: Cannot convert a Column object into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' if you're building DataFrame filter expressions. For example, use df.filter((col1 > 1) & (col2 > 2)) instead of df.filter(col1 > 1 and col2 > 2).
 in function ORDERS__DBT_SP with handler main
[0m02:31:19.950297 [debug] [Thread-1  ]: finished collecting timing info
[0m02:31:19.950627 [debug] [Thread-1  ]: On model.jaffle_shop.orders: Close
[0m02:31:20.345235 [debug] [Thread-1  ]: Database Error in model orders (models/orders.py)
  100357 (P0000): Python Interpreter Error:
  Traceback (most recent call last):
    File "_udf_code.py", line 105, in main
    File "_udf_code.py", line 12, in model
    File "_udf_code.py", line 14, in <dictcomp>
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/column.py", line 302, in __bool__
      raise TypeError(
  TypeError: Cannot convert a Column object into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' if you're building DataFrame filter expressions. For example, use df.filter((col1 > 1) & (col2 > 2)) instead of df.filter(col1 > 1 and col2 > 2).
   in function ORDERS__DBT_SP with handler main
  compiled Code at target/run/jaffle_shop/models/orders.py
[0m02:31:20.346388 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ab83f6ff-07e9-4d10-9581-51fead052251', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112144160>]}
[0m02:31:20.347612 [error] [Thread-1  ]: 2 of 2 ERROR creating python table model bruno.orders .......................... [[31mERROR[0m in 7.47s]
[0m02:31:20.349006 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.orders
[0m02:31:20.353083 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:31:20.354234 [info ] [MainThread]: 
[0m02:31:20.354729 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 17.60 seconds (17.60s).
[0m02:31:20.355149 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:31:20.355353 [debug] [MainThread]: Connection 'model.jaffle_shop.orders' was properly closed.
[0m02:31:20.370196 [info ] [MainThread]: 
[0m02:31:20.370659 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m02:31:20.371023 [info ] [MainThread]: 
[0m02:31:20.371310 [error] [MainThread]: [33mDatabase Error in model customers (models/customers.py)[0m
[0m02:31:20.371589 [error] [MainThread]:   100357 (P0000): Python Interpreter Error:
[0m02:31:20.371842 [error] [MainThread]:   Traceback (most recent call last):
[0m02:31:20.372093 [error] [MainThread]:     File "_udf_code.py", line 109, in main
[0m02:31:20.372538 [error] [MainThread]:     File "_udf_code.py", line 12, in model
[0m02:31:20.373068 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/column.py", line 308, in __iter__
[0m02:31:20.373330 [error] [MainThread]:       raise TypeError(
[0m02:31:20.373578 [error] [MainThread]:   TypeError: Column is not iterable. This error can occur when you use the Python built-ins for sum, min and max. Please make sure you use the corresponding function from snowflake.snowpark.functions.
[0m02:31:20.373821 [error] [MainThread]:    in function CUSTOMERS__DBT_SP with handler main
[0m02:31:20.374063 [error] [MainThread]:   compiled Code at target/run/jaffle_shop/models/customers.py
[0m02:31:20.374318 [info ] [MainThread]: 
[0m02:31:20.374582 [error] [MainThread]: [33mDatabase Error in model orders (models/orders.py)[0m
[0m02:31:20.374834 [error] [MainThread]:   100357 (P0000): Python Interpreter Error:
[0m02:31:20.375073 [error] [MainThread]:   Traceback (most recent call last):
[0m02:31:20.375312 [error] [MainThread]:     File "_udf_code.py", line 105, in main
[0m02:31:20.375547 [error] [MainThread]:     File "_udf_code.py", line 12, in model
[0m02:31:20.375770 [error] [MainThread]:     File "_udf_code.py", line 14, in <dictcomp>
[0m02:31:20.375978 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/column.py", line 302, in __bool__
[0m02:31:20.376189 [error] [MainThread]:       raise TypeError(
[0m02:31:20.376397 [error] [MainThread]:   TypeError: Cannot convert a Column object into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' if you're building DataFrame filter expressions. For example, use df.filter((col1 > 1) & (col2 > 2)) instead of df.filter(col1 > 1 and col2 > 2).
[0m02:31:20.376619 [error] [MainThread]:    in function ORDERS__DBT_SP with handler main
[0m02:31:20.376828 [error] [MainThread]:   compiled Code at target/run/jaffle_shop/models/orders.py
[0m02:31:20.377051 [info ] [MainThread]: 
[0m02:31:20.377290 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=0 TOTAL=2
[0m02:31:20.377777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113e4fe50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112074640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113e57700>]}
[0m02:31:20.378077 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 02:46:36.215724 | 00d2ab02-3c79-41ca-832d-512bf4dd846b ==============================
[0m02:46:36.215791 [info ] [MainThread]: Running with dbt=1.3.2
[0m02:46:36.216596 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m02:46:36.216748 [debug] [MainThread]: Tracking: tracking
[0m02:46:36.217075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10deeebe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10deeec40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10deeeca0>]}
[0m02:46:36.271145 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:46:36.271346 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:46:36.277157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '00d2ab02-3c79-41ca-832d-512bf4dd846b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e48aee0>]}
[0m02:46:36.284329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '00d2ab02-3c79-41ca-832d-512bf4dd846b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dfd2550>]}
[0m02:46:36.284572 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m02:46:36.284765 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '00d2ab02-3c79-41ca-832d-512bf4dd846b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dfd2580>]}
[0m02:46:36.285962 [info ] [MainThread]: 
[0m02:46:36.286439 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:46:36.287198 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m02:46:36.298586 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m02:46:36.298791 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m02:46:36.298903 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:46:37.429252 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.13 seconds
[0m02:46:37.434618 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m02:46:37.859987 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m02:46:37.872107 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m02:46:37.872356 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m02:46:37.872545 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m02:46:38.491162 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 0.62 seconds
[0m02:46:38.495701 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m02:46:38.834893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '00d2ab02-3c79-41ca-832d-512bf4dd846b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e49cb50>]}
[0m02:46:38.836354 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:46:38.836939 [info ] [MainThread]: 
[0m02:46:38.844439 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m02:46:38.845066 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m02:46:38.846960 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m02:46:38.847275 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m02:46:38.847532 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m02:46:38.878873 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m02:46:38.880195 [debug] [Thread-1  ]: finished collecting timing info
[0m02:46:38.880429 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m02:46:38.913302 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m02:46:38.915295 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m02:46:38.915413 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.customers__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
def model(dbt, session):

    stg_customers = dbt.ref('stg_customers')
    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    customer_orders = stg_orders.group_by('customer_id').agg({
        'first_order': min(stg_orders.order_date),
        'most_recent_order': max(stg_orders.order_date),
        'number_of_orders': count(stg_orders.order_id)
    })

    customer_payments = stg_payments.merge(stg_orders, on='order_id', how='left').group_by('customer_id').agg({
        'total_amount': sum(stg_payments.amount)
    })

    final_df = stg_customers.merge(customer_orders, on='customer_id', how='left').merge(customer_payments, on='customer_id', how='left')

    final_df.select(
        'customer_id',
        'first_name',
        'last_name',
        'first_order',
        'most_recent_order',
        'number_of_orders',
        'customer_lifetime_value'
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle_shop.bruno.stg_customers", "stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle_shop.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.customers", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m02:46:38.915516 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:47:02.021137 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 23.11 seconds
[0m02:47:02.021859 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m02:47:02.022129 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CALL jaffle_shop.bruno.customers__dbt_sp();
[0m02:47:03.323617 [debug] [Thread-1  ]: Snowflake adapter: Snowflake query id: 01a9dac7-0000-0c55-0000-0000468d26f5
[0m02:47:03.323838 [debug] [Thread-1  ]: Snowflake adapter: Snowflake error: 100357 (P0000): Python Interpreter Error:
Traceback (most recent call last):
  File "_udf_code.py", line 109, in main
  File "_udf_code.py", line 12, in model
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/column.py", line 308, in __iter__
    raise TypeError(
TypeError: Column is not iterable. This error can occur when you use the Python built-ins for sum, min and max. Please make sure you use the corresponding function from snowflake.snowpark.functions.
 in function CUSTOMERS__DBT_SP with handler main
[0m02:47:03.324118 [debug] [Thread-1  ]: finished collecting timing info
[0m02:47:03.324261 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m02:47:03.711930 [debug] [Thread-1  ]: Database Error in model customers (models/customers.py)
  100357 (P0000): Python Interpreter Error:
  Traceback (most recent call last):
    File "_udf_code.py", line 109, in main
    File "_udf_code.py", line 12, in model
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/column.py", line 308, in __iter__
      raise TypeError(
  TypeError: Column is not iterable. This error can occur when you use the Python built-ins for sum, min and max. Please make sure you use the corresponding function from snowflake.snowpark.functions.
   in function CUSTOMERS__DBT_SP with handler main
  compiled Code at target/run/jaffle_shop/models/customers.py
[0m02:47:03.713739 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '00d2ab02-3c79-41ca-832d-512bf4dd846b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f6c5040>]}
[0m02:47:03.715186 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 24.87s]
[0m02:47:03.717870 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m02:47:03.722996 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:47:03.724461 [info ] [MainThread]: 
[0m02:47:03.725113 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 27.44 seconds (27.44s).
[0m02:47:03.725707 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:47:03.726018 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m02:47:03.742872 [info ] [MainThread]: 
[0m02:47:03.743317 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m02:47:03.743745 [info ] [MainThread]: 
[0m02:47:03.744106 [error] [MainThread]: [33mDatabase Error in model customers (models/customers.py)[0m
[0m02:47:03.744452 [error] [MainThread]:   100357 (P0000): Python Interpreter Error:
[0m02:47:03.744785 [error] [MainThread]:   Traceback (most recent call last):
[0m02:47:03.745115 [error] [MainThread]:     File "_udf_code.py", line 109, in main
[0m02:47:03.745443 [error] [MainThread]:     File "_udf_code.py", line 12, in model
[0m02:47:03.745767 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/column.py", line 308, in __iter__
[0m02:47:03.746078 [error] [MainThread]:       raise TypeError(
[0m02:47:03.746356 [error] [MainThread]:   TypeError: Column is not iterable. This error can occur when you use the Python built-ins for sum, min and max. Please make sure you use the corresponding function from snowflake.snowpark.functions.
[0m02:47:03.746632 [error] [MainThread]:    in function CUSTOMERS__DBT_SP with handler main
[0m02:47:03.746908 [error] [MainThread]:   compiled Code at target/run/jaffle_shop/models/customers.py
[0m02:47:03.747193 [info ] [MainThread]: 
[0m02:47:03.747494 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m02:47:03.747920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e570970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e44fdc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f6c1ac0>]}
[0m02:47:03.748260 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 02:49:32.139665 | 35ef693e-7cf2-46ad-a19b-51e221caf9d3 ==============================
[0m02:49:32.139740 [info ] [MainThread]: Running with dbt=1.3.2
[0m02:49:32.140523 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m02:49:32.140688 [debug] [MainThread]: Tracking: tracking
[0m02:49:32.141013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e0b7b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e0b7be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e0b7c40>]}
[0m02:49:32.193270 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m02:49:32.193695 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m02:49:32.249572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '35ef693e-7cf2-46ad-a19b-51e221caf9d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e844970>]}
[0m02:49:32.256185 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '35ef693e-7cf2-46ad-a19b-51e221caf9d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e1e0400>]}
[0m02:49:32.256394 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m02:49:32.256580 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '35ef693e-7cf2-46ad-a19b-51e221caf9d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e81c0a0>]}
[0m02:49:32.257736 [info ] [MainThread]: 
[0m02:49:32.258189 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:49:32.258949 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m02:49:32.271276 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m02:49:32.271465 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m02:49:32.271566 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:49:33.476829 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.21 seconds
[0m02:49:33.480855 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m02:49:33.867743 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m02:49:33.882287 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m02:49:33.882605 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m02:49:33.882834 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m02:49:34.466514 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 0.58 seconds
[0m02:49:34.469070 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m02:49:34.913968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '35ef693e-7cf2-46ad-a19b-51e221caf9d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e828fd0>]}
[0m02:49:34.915294 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:49:34.915855 [info ] [MainThread]: 
[0m02:49:34.923088 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m02:49:34.923649 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m02:49:34.924851 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m02:49:34.925122 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m02:49:34.926019 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m02:49:34.955054 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m02:49:34.956180 [debug] [Thread-1  ]: finished collecting timing info
[0m02:49:34.956368 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m02:49:34.988608 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m02:49:34.990708 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m02:49:34.990823 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.customers__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
def model(dbt, session):

    stg_customers = dbt.ref('stg_customers')
    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    customer_orders = (
        stg_orders
        .groupby("customer_id")
        .agg(
            first_order = F.min("order_date"),
            most_recent_order = F.max("order_date"),
            number_of_orders = F.count("order_id")
        )
    )

    customer_payments = (
        stg_payments
        .join(orders, payments["order_id"] == orders["order_id"], "left")
        .groupby("orders.customer_id")
        .agg(
            total_amount = F.sum("amount")
        )
    )

    final_df = (
        stg_customers
        .join(customer_orders, customers["customer_id"] == customer_orders["customer_id"], "left")
        .join(customer_payments, customers["customer_id"] == customer_payments["customer_id"], "left")
        .select("customers.customer_id",
                "customers.first_name",
                "customers.last_name",
                "customer_orders.first_order",
                "customer_orders.most_recent_order",
                "customer_orders.number_of_orders",
                F.col("customer_payments.total_amount").alias("customer_lifetime_value")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle_shop.bruno.stg_customers", "stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle_shop.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.customers", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m02:49:34.990926 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:49:40.465191 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.47 seconds
[0m02:49:40.465592 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m02:49:40.465805 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CALL jaffle_shop.bruno.customers__dbt_sp();
[0m02:49:41.415099 [debug] [Thread-1  ]: Snowflake adapter: Snowflake query id: 01a9dac9-0000-0c55-0000-0000468d2701
[0m02:49:41.415295 [debug] [Thread-1  ]: Snowflake adapter: Snowflake error: 100357 (P0000): Python Interpreter Error:
Traceback (most recent call last):
  File "_udf_code.py", line 120, in main
  File "_udf_code.py", line 12, in model
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 848, in __getattr__
    raise AttributeError(
AttributeError: Table object has no attribute groupby
 in function CUSTOMERS__DBT_SP with handler main
[0m02:49:41.415553 [debug] [Thread-1  ]: finished collecting timing info
[0m02:49:41.415685 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m02:49:41.876244 [debug] [Thread-1  ]: Database Error in model customers (models/customers.py)
  100357 (P0000): Python Interpreter Error:
  Traceback (most recent call last):
    File "_udf_code.py", line 120, in main
    File "_udf_code.py", line 12, in model
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 848, in __getattr__
      raise AttributeError(
  AttributeError: Table object has no attribute groupby
   in function CUSTOMERS__DBT_SP with handler main
  compiled Code at target/run/jaffle_shop/models/customers.py
[0m02:49:41.877083 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '35ef693e-7cf2-46ad-a19b-51e221caf9d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f892c70>]}
[0m02:49:41.877769 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 6.95s]
[0m02:49:41.878787 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m02:49:41.881523 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:49:41.882587 [info ] [MainThread]: 
[0m02:49:41.883099 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 9.62 seconds (9.62s).
[0m02:49:41.883569 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:49:41.883806 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m02:49:41.898612 [info ] [MainThread]: 
[0m02:49:41.899041 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m02:49:41.899413 [info ] [MainThread]: 
[0m02:49:41.899726 [error] [MainThread]: [33mDatabase Error in model customers (models/customers.py)[0m
[0m02:49:41.900108 [error] [MainThread]:   100357 (P0000): Python Interpreter Error:
[0m02:49:41.900402 [error] [MainThread]:   Traceback (most recent call last):
[0m02:49:41.900690 [error] [MainThread]:     File "_udf_code.py", line 120, in main
[0m02:49:41.900977 [error] [MainThread]:     File "_udf_code.py", line 12, in model
[0m02:49:41.901448 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 848, in __getattr__
[0m02:49:41.901906 [error] [MainThread]:       raise AttributeError(
[0m02:49:41.902234 [error] [MainThread]:   AttributeError: Table object has no attribute groupby
[0m02:49:41.902539 [error] [MainThread]:    in function CUSTOMERS__DBT_SP with handler main
[0m02:49:41.902833 [error] [MainThread]:   compiled Code at target/run/jaffle_shop/models/customers.py
[0m02:49:41.903171 [info ] [MainThread]: 
[0m02:49:41.903505 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m02:49:41.904010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e8ea640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e8f65b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f8927c0>]}
[0m02:49:41.904359 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 02:49:55.786981 | 14020075-6a63-4a29-8f6e-2726d1d94547 ==============================
[0m02:49:55.787055 [info ] [MainThread]: Running with dbt=1.3.2
[0m02:49:55.787649 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m02:49:55.787790 [debug] [MainThread]: Tracking: tracking
[0m02:49:55.788102 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd8fbe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd8fc40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd8fca0>]}
[0m02:49:55.836712 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m02:49:55.837134 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m02:49:55.890274 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '14020075-6a63-4a29-8f6e-2726d1d94547', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11030ff40>]}
[0m02:49:55.896924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '14020075-6a63-4a29-8f6e-2726d1d94547', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10feb8490>]}
[0m02:49:55.897117 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m02:49:55.897303 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '14020075-6a63-4a29-8f6e-2726d1d94547', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1102e6100>]}
[0m02:49:55.898527 [info ] [MainThread]: 
[0m02:49:55.899089 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:49:55.900053 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m02:49:55.912449 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m02:49:55.912665 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m02:49:55.912768 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:49:56.762615 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 0.85 seconds
[0m02:49:56.771535 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m02:49:57.145545 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m02:49:57.167577 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m02:49:57.168101 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m02:49:57.168379 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m02:49:57.782353 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 0.61 seconds
[0m02:49:57.790917 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m02:49:58.151809 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '14020075-6a63-4a29-8f6e-2726d1d94547', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11030ff70>]}
[0m02:49:58.154395 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:49:58.155065 [info ] [MainThread]: 
[0m02:49:58.163768 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m02:49:58.164572 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m02:49:58.166108 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m02:49:58.166402 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m02:49:58.167437 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m02:49:58.199924 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m02:49:58.200535 [debug] [Thread-1  ]: finished collecting timing info
[0m02:49:58.200733 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m02:49:58.232092 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m02:49:58.233978 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m02:49:58.234080 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.customers__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
def model(dbt, session):

    stg_customers = dbt.ref('stg_customers')
    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    customer_orders = (
        stg_orders
        .group_by("customer_id")
        .agg(
            first_order = F.min("order_date"),
            most_recent_order = F.max("order_date"),
            number_of_orders = F.count("order_id")
        )
    )

    customer_payments = (
        stg_payments
        .join(orders, payments["order_id"] == orders["order_id"], "left")
        .group_by("orders.customer_id")
        .agg(
            total_amount = F.sum("amount")
        )
    )

    final_df = (
        stg_customers
        .join(customer_orders, customers["customer_id"] == customer_orders["customer_id"], "left")
        .join(customer_payments, customers["customer_id"] == customer_payments["customer_id"], "left")
        .select("customers.customer_id",
                "customers.first_name",
                "customers.last_name",
                "customer_orders.first_order",
                "customer_orders.most_recent_order",
                "customer_orders.number_of_orders",
                F.col("customer_payments.total_amount").alias("customer_lifetime_value")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle_shop.bruno.stg_customers", "stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle_shop.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.customers", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m02:49:58.234175 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:50:03.988951 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.75 seconds
[0m02:50:03.990126 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m02:50:03.990403 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CALL jaffle_shop.bruno.customers__dbt_sp();
[0m02:50:04.973006 [debug] [Thread-1  ]: Snowflake adapter: Snowflake query id: 01a9daca-0000-0c55-0000-0000468d270d
[0m02:50:04.973213 [debug] [Thread-1  ]: Snowflake adapter: Snowflake error: 100357 (P0000): Python Interpreter Error:
Traceback (most recent call last):
  File "_udf_code.py", line 120, in main
  File "_udf_code.py", line 15, in model
NameError: name 'F' is not defined
 in function CUSTOMERS__DBT_SP with handler main
[0m02:50:04.973492 [debug] [Thread-1  ]: finished collecting timing info
[0m02:50:04.973633 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m02:50:05.305930 [debug] [Thread-1  ]: Database Error in model customers (models/customers.py)
  100357 (P0000): Python Interpreter Error:
  Traceback (most recent call last):
    File "_udf_code.py", line 120, in main
    File "_udf_code.py", line 15, in model
  NameError: name 'F' is not defined
   in function CUSTOMERS__DBT_SP with handler main
  compiled Code at target/run/jaffle_shop/models/customers.py
[0m02:50:05.306818 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '14020075-6a63-4a29-8f6e-2726d1d94547', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1115541c0>]}
[0m02:50:05.307528 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 7.14s]
[0m02:50:05.308522 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m02:50:05.311396 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:50:05.312438 [info ] [MainThread]: 
[0m02:50:05.312970 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 9.41 seconds (9.41s).
[0m02:50:05.313437 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:50:05.313676 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m02:50:05.328895 [info ] [MainThread]: 
[0m02:50:05.329310 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m02:50:05.329676 [info ] [MainThread]: 
[0m02:50:05.329983 [error] [MainThread]: [33mDatabase Error in model customers (models/customers.py)[0m
[0m02:50:05.330281 [error] [MainThread]:   100357 (P0000): Python Interpreter Error:
[0m02:50:05.330565 [error] [MainThread]:   Traceback (most recent call last):
[0m02:50:05.330859 [error] [MainThread]:     File "_udf_code.py", line 120, in main
[0m02:50:05.331135 [error] [MainThread]:     File "_udf_code.py", line 15, in model
[0m02:50:05.331408 [error] [MainThread]:   NameError: name 'F' is not defined
[0m02:50:05.331681 [error] [MainThread]:    in function CUSTOMERS__DBT_SP with handler main
[0m02:50:05.331953 [error] [MainThread]:   compiled Code at target/run/jaffle_shop/models/customers.py
[0m02:50:05.332241 [info ] [MainThread]: 
[0m02:50:05.332537 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m02:50:05.332957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1102f0c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10feb8160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111579430>]}
[0m02:50:05.333282 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 02:50:20.399630 | e9b2f45e-8b2e-4ab6-89b2-5e2b6de95f10 ==============================
[0m02:50:20.399691 [info ] [MainThread]: Running with dbt=1.3.2
[0m02:50:20.400248 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m02:50:20.400397 [debug] [MainThread]: Tracking: tracking
[0m02:50:20.400783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111effbe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111effc40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111effca0>]}
[0m02:50:20.446573 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m02:50:20.446983 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m02:50:20.500132 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e9b2f45e-8b2e-4ab6-89b2-5e2b6de95f10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1124879a0>]}
[0m02:50:20.506448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e9b2f45e-8b2e-4ab6-89b2-5e2b6de95f10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112028490>]}
[0m02:50:20.506654 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m02:50:20.506835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e9b2f45e-8b2e-4ab6-89b2-5e2b6de95f10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112456100>]}
[0m02:50:20.507974 [info ] [MainThread]: 
[0m02:50:20.508436 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:50:20.509221 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m02:50:20.521643 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m02:50:20.521843 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m02:50:20.521944 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:50:21.598150 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.08 seconds
[0m02:50:21.603349 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m02:50:22.124794 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m02:50:22.139405 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m02:50:22.139703 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m02:50:22.139934 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m02:50:22.887216 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 0.75 seconds
[0m02:50:22.892033 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m02:50:23.238847 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e9b2f45e-8b2e-4ab6-89b2-5e2b6de95f10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111ee8880>]}
[0m02:50:23.240328 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:50:23.240879 [info ] [MainThread]: 
[0m02:50:23.246136 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m02:50:23.246720 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m02:50:23.247971 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m02:50:23.248292 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m02:50:23.249335 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m02:50:23.279254 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m02:50:23.279888 [debug] [Thread-1  ]: finished collecting timing info
[0m02:50:23.280099 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m02:50:23.314106 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m02:50:23.316277 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m02:50:23.316407 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.customers__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
def model(dbt, session):

    stg_customers = dbt.ref('stg_customers')
    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    customer_orders = (
        stg_orders
        .group_by("customer_id")
        .agg(
            first_order = min("order_date"),
            most_recent_order = max("order_date"),
            number_of_orders = count("order_id")
        )
    )

    customer_payments = (
        stg_payments
        .join(orders, payments["order_id"] == orders["order_id"], "left")
        .group_by("orders.customer_id")
        .agg(
            total_amount = sum("amount")
        )
    )

    final_df = (
        stg_customers
        .join(customer_orders, customers["customer_id"] == customer_orders["customer_id"], "left")
        .join(customer_payments, customers["customer_id"] == customer_payments["customer_id"], "left")
        .select("customers.customer_id",
                "customers.first_name",
                "customers.last_name",
                "customer_orders.first_order",
                "customer_orders.most_recent_order",
                "customer_orders.number_of_orders",
                col("customer_payments.total_amount").alias("customer_lifetime_value")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle_shop.bruno.stg_customers", "stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle_shop.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.customers", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m02:50:23.316512 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:50:28.812540 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.5 seconds
[0m02:50:28.813535 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m02:50:28.813905 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CALL jaffle_shop.bruno.customers__dbt_sp();
[0m02:50:29.737345 [debug] [Thread-1  ]: Snowflake adapter: Snowflake query id: 01a9daca-0000-0c56-0000-0000468d370d
[0m02:50:29.737578 [debug] [Thread-1  ]: Snowflake adapter: Snowflake error: 100357 (P0000): Python Interpreter Error:
Traceback (most recent call last):
  File "_udf_code.py", line 120, in main
  File "_udf_code.py", line 17, in model
NameError: name 'count' is not defined
 in function CUSTOMERS__DBT_SP with handler main
[0m02:50:29.737869 [debug] [Thread-1  ]: finished collecting timing info
[0m02:50:29.738029 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m02:50:30.079205 [debug] [Thread-1  ]: Database Error in model customers (models/customers.py)
  100357 (P0000): Python Interpreter Error:
  Traceback (most recent call last):
    File "_udf_code.py", line 120, in main
    File "_udf_code.py", line 17, in model
  NameError: name 'count' is not defined
   in function CUSTOMERS__DBT_SP with handler main
  compiled Code at target/run/jaffle_shop/models/customers.py
[0m02:50:30.080075 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e9b2f45e-8b2e-4ab6-89b2-5e2b6de95f10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1136dd820>]}
[0m02:50:30.080801 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 6.83s]
[0m02:50:30.081787 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m02:50:30.084615 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:50:30.085674 [info ] [MainThread]: 
[0m02:50:30.086205 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 9.58 seconds (9.58s).
[0m02:50:30.086674 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:50:30.086922 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m02:50:30.101642 [info ] [MainThread]: 
[0m02:50:30.102073 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m02:50:30.102449 [info ] [MainThread]: 
[0m02:50:30.102762 [error] [MainThread]: [33mDatabase Error in model customers (models/customers.py)[0m
[0m02:50:30.103081 [error] [MainThread]:   100357 (P0000): Python Interpreter Error:
[0m02:50:30.103369 [error] [MainThread]:   Traceback (most recent call last):
[0m02:50:30.103660 [error] [MainThread]:     File "_udf_code.py", line 120, in main
[0m02:50:30.103942 [error] [MainThread]:     File "_udf_code.py", line 17, in model
[0m02:50:30.104223 [error] [MainThread]:   NameError: name 'count' is not defined
[0m02:50:30.104500 [error] [MainThread]:    in function CUSTOMERS__DBT_SP with handler main
[0m02:50:30.104779 [error] [MainThread]:   compiled Code at target/run/jaffle_shop/models/customers.py
[0m02:50:30.105086 [info ] [MainThread]: 
[0m02:50:30.105392 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m02:50:30.105822 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112461c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112028b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1136dd070>]}
[0m02:50:30.106152 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 02:54:13.553044 | 2edb0cfd-8922-4d92-b6fc-f4e52dbf9db5 ==============================
[0m02:54:13.553124 [info ] [MainThread]: Running with dbt=1.3.2
[0m02:54:13.553925 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m02:54:13.554073 [debug] [MainThread]: Tracking: tracking
[0m02:54:13.554404 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10db4fb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10db4fbb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10db4fc10>]}
[0m02:54:13.608370 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:54:13.608566 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:54:13.614482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2edb0cfd-8922-4d92-b6fc-f4e52dbf9db5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e0e8e50>]}
[0m02:54:13.621753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2edb0cfd-8922-4d92-b6fc-f4e52dbf9db5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dc304c0>]}
[0m02:54:13.622013 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m02:54:13.622218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2edb0cfd-8922-4d92-b6fc-f4e52dbf9db5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d9d60d0>]}
[0m02:54:13.623461 [info ] [MainThread]: 
[0m02:54:13.623936 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:54:13.624664 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m02:54:13.636278 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m02:54:13.636490 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m02:54:13.636587 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:54:14.795068 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.16 seconds
[0m02:54:14.799567 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m02:54:15.151301 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m02:54:15.165701 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m02:54:15.165982 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m02:54:15.166195 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m02:54:15.985761 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 0.82 seconds
[0m02:54:15.990907 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m02:54:16.329521 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2edb0cfd-8922-4d92-b6fc-f4e52dbf9db5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dc30100>]}
[0m02:54:16.330905 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:54:16.331454 [info ] [MainThread]: 
[0m02:54:16.339021 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m02:54:16.339718 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m02:54:16.340872 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m02:54:16.341132 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m02:54:16.342040 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m02:54:16.374165 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m02:54:16.375213 [debug] [Thread-1  ]: finished collecting timing info
[0m02:54:16.375416 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m02:54:16.410399 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m02:54:16.412600 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m02:54:16.412716 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.customers__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
def model(dbt, session):

    stg_customers = dbt.ref('stg_customers')
    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    customer_orders = (
        stg_orders
        .group_by("customer_id")
        .agg(
            first_order = min("order_date"),
            most_recent_order = max("order_date"),
            number_of_orders = count("order_id")
        )
    )

    customer_payments = (
        stg_payments
        .join(orders, payments["order_id"] == orders["order_id"], "left")
        .group_by("orders.customer_id")
        .agg(
            total_amount = sum("amount")
        )
    )

    final_df = (
        stg_customers
        .join(customer_orders, customers["customer_id"] == customer_orders["customer_id"], "left")
        .join(customer_payments, customers["customer_id"] == customer_payments["customer_id"], "left")
        .select("customers.customer_id",
                "customers.first_name",
                "customers.last_name",
                "customer_orders.first_order",
                "customer_orders.most_recent_order",
                "customer_orders.number_of_orders",
                col("customer_payments.total_amount").alias("customer_lifetime_value")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle_shop.bruno.stg_customers", "stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle_shop.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.customers", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m02:54:16.412818 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:54:21.938806 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.53 seconds
[0m02:54:21.939762 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m02:54:21.939971 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CALL jaffle_shop.bruno.customers__dbt_sp();
[0m02:54:22.925752 [debug] [Thread-1  ]: Snowflake adapter: Snowflake query id: 01a9dace-0000-0c55-0000-0000468d2725
[0m02:54:22.925974 [debug] [Thread-1  ]: Snowflake adapter: Snowflake error: 100357 (P0000): Python Interpreter Error:
Traceback (most recent call last):
  File "_udf_code.py", line 120, in main
  File "_udf_code.py", line 17, in model
NameError: name 'count' is not defined
 in function CUSTOMERS__DBT_SP with handler main
[0m02:54:22.926276 [debug] [Thread-1  ]: finished collecting timing info
[0m02:54:22.926430 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m02:54:23.481396 [debug] [Thread-1  ]: Database Error in model customers (models/customers.py)
  100357 (P0000): Python Interpreter Error:
  Traceback (most recent call last):
    File "_udf_code.py", line 120, in main
    File "_udf_code.py", line 17, in model
  NameError: name 'count' is not defined
   in function CUSTOMERS__DBT_SP with handler main
  compiled Code at target/run/jaffle_shop/models/customers.py
[0m02:54:23.482230 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2edb0cfd-8922-4d92-b6fc-f4e52dbf9db5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e61ed30>]}
[0m02:54:23.482960 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 7.14s]
[0m02:54:23.483959 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m02:54:23.486871 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:54:23.487858 [info ] [MainThread]: 
[0m02:54:23.488383 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 9.86 seconds (9.86s).
[0m02:54:23.488843 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:54:23.489087 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m02:54:23.504076 [info ] [MainThread]: 
[0m02:54:23.504563 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m02:54:23.504958 [info ] [MainThread]: 
[0m02:54:23.505271 [error] [MainThread]: [33mDatabase Error in model customers (models/customers.py)[0m
[0m02:54:23.505581 [error] [MainThread]:   100357 (P0000): Python Interpreter Error:
[0m02:54:23.505868 [error] [MainThread]:   Traceback (most recent call last):
[0m02:54:23.506150 [error] [MainThread]:     File "_udf_code.py", line 120, in main
[0m02:54:23.506432 [error] [MainThread]:     File "_udf_code.py", line 17, in model
[0m02:54:23.506716 [error] [MainThread]:   NameError: name 'count' is not defined
[0m02:54:23.506997 [error] [MainThread]:    in function CUSTOMERS__DBT_SP with handler main
[0m02:54:23.507278 [error] [MainThread]:   compiled Code at target/run/jaffle_shop/models/customers.py
[0m02:54:23.507598 [info ] [MainThread]: 
[0m02:54:23.507897 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m02:54:23.508413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e0a2280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10db64ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e61e070>]}
[0m02:54:23.508827 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 02:56:07.006054 | 05cc9f14-3c57-4f65-948d-f973751559c9 ==============================
[0m02:56:07.006176 [info ] [MainThread]: Running with dbt=1.3.2
[0m02:56:07.007470 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m02:56:07.007674 [debug] [MainThread]: Tracking: tracking
[0m02:56:07.008244 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112581a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112581af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112581b50>]}
[0m02:56:07.062896 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m02:56:07.063306 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m02:56:07.120167 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '05cc9f14-3c57-4f65-948d-f973751559c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112afd0d0>]}
[0m02:56:07.131260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '05cc9f14-3c57-4f65-948d-f973751559c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112698400>]}
[0m02:56:07.131589 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m02:56:07.131803 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '05cc9f14-3c57-4f65-948d-f973751559c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112ad1f70>]}
[0m02:56:07.133224 [info ] [MainThread]: 
[0m02:56:07.133886 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:56:07.135745 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m02:56:07.148114 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m02:56:07.148303 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m02:56:07.148401 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:56:08.277318 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.13 seconds
[0m02:56:08.282064 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m02:56:08.635336 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m02:56:08.649698 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m02:56:08.649997 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m02:56:08.650224 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m02:56:09.569721 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 0.92 seconds
[0m02:56:09.575094 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m02:56:09.982485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '05cc9f14-3c57-4f65-948d-f973751559c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112698070>]}
[0m02:56:09.984081 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:56:09.984653 [info ] [MainThread]: 
[0m02:56:09.992005 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m02:56:09.992694 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m02:56:09.993894 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m02:56:09.994153 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m02:56:09.995030 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m02:56:10.024878 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m02:56:10.025768 [debug] [Thread-1  ]: finished collecting timing info
[0m02:56:10.025978 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m02:56:10.059121 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m02:56:10.061280 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m02:56:10.061395 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.customers__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    stg_customers = dbt.ref('stg_customers')
    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    customer_orders = (
        stg_orders
        .group_by("customer_id")
        .agg(
            first_order = F.min("order_date"),
            most_recent_order = F.max("order_date"),
            number_of_orders = F.count("order_id")
        )
    )

    customer_payments = (
        stg_payments
        .join(orders, payments["order_id"] == orders["order_id"], "left")
        .group_by("orders.customer_id")
        .agg(
            total_amount = F.sum("amount")
        )
    )

    final_df = (
        stg_customers
        .join(customer_orders, customers["customer_id"] == customer_orders["customer_id"], "left")
        .join(customer_payments, customers["customer_id"] == customer_payments["customer_id"], "left")
        .select("customers.customer_id",
                "customers.first_name",
                "customers.last_name",
                "customer_orders.first_order",
                "customer_orders.most_recent_order",
                "customer_orders.number_of_orders",
                F.col("customer_payments.total_amount").alias("customer_lifetime_value")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle_shop.bruno.stg_customers", "stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle_shop.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.customers", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m02:56:10.061500 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:56:15.494292 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.43 seconds
[0m02:56:15.495574 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m02:56:15.495903 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CALL jaffle_shop.bruno.customers__dbt_sp();
[0m02:56:16.487656 [debug] [Thread-1  ]: Snowflake adapter: Snowflake query id: 01a9dad0-0000-0c56-0000-0000468d3719
[0m02:56:16.487874 [debug] [Thread-1  ]: Snowflake adapter: Snowflake error: 100357 (P0000): Python Interpreter Error:
Traceback (most recent call last):
  File "_udf_code.py", line 122, in main
  File "_udf_code.py", line 14, in model
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/telemetry.py", line 224, in wrap
    r = func(*args, **kwargs)
TypeError: agg() got an unexpected keyword argument 'first_order'
 in function CUSTOMERS__DBT_SP with handler main
[0m02:56:16.488169 [debug] [Thread-1  ]: finished collecting timing info
[0m02:56:16.488325 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m02:56:17.048141 [debug] [Thread-1  ]: Database Error in model customers (models/customers.py)
  100357 (P0000): Python Interpreter Error:
  Traceback (most recent call last):
    File "_udf_code.py", line 122, in main
    File "_udf_code.py", line 14, in model
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/telemetry.py", line 224, in wrap
      r = func(*args, **kwargs)
  TypeError: agg() got an unexpected keyword argument 'first_order'
   in function CUSTOMERS__DBT_SP with handler main
  compiled Code at target/run/jaffle_shop/models/customers.py
[0m02:56:17.048843 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '05cc9f14-3c57-4f65-948d-f973751559c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113d5ca00>]}
[0m02:56:17.049527 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 7.06s]
[0m02:56:17.050521 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m02:56:17.052920 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m02:56:17.053771 [info ] [MainThread]: 
[0m02:56:17.054206 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 9.92 seconds (9.92s).
[0m02:56:17.054607 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:56:17.054809 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m02:56:17.068982 [info ] [MainThread]: 
[0m02:56:17.069471 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m02:56:17.069889 [info ] [MainThread]: 
[0m02:56:17.070214 [error] [MainThread]: [33mDatabase Error in model customers (models/customers.py)[0m
[0m02:56:17.070525 [error] [MainThread]:   100357 (P0000): Python Interpreter Error:
[0m02:56:17.070819 [error] [MainThread]:   Traceback (most recent call last):
[0m02:56:17.071091 [error] [MainThread]:     File "_udf_code.py", line 122, in main
[0m02:56:17.071340 [error] [MainThread]:     File "_udf_code.py", line 14, in model
[0m02:56:17.071585 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/telemetry.py", line 224, in wrap
[0m02:56:17.071836 [error] [MainThread]:       r = func(*args, **kwargs)
[0m02:56:17.072083 [error] [MainThread]:   TypeError: agg() got an unexpected keyword argument 'first_order'
[0m02:56:17.072328 [error] [MainThread]:    in function CUSTOMERS__DBT_SP with handler main
[0m02:56:17.072571 [error] [MainThread]:   compiled Code at target/run/jaffle_shop/models/customers.py
[0m02:56:17.072828 [info ] [MainThread]: 
[0m02:56:17.073092 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m02:56:17.073479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1126980a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112baf250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113d5c520>]}
[0m02:56:17.073782 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 02:58:43.919649 | 2a2f7cce-76d4-419f-bfca-b7621b1ba1ff ==============================
[0m02:58:43.919738 [info ] [MainThread]: Running with dbt=1.3.2
[0m02:58:43.920903 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m02:58:43.921080 [debug] [MainThread]: Tracking: tracking
[0m02:58:43.921530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e19b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e19bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e19c10>]}
[0m02:58:43.977023 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m02:58:43.977441 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m02:58:43.989838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f01a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1123b2ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1123b2c40>]}
[0m02:58:43.990094 [debug] [MainThread]: Flushing usage events
[0m02:58:44.772931 [error] [MainThread]: Encountered an error:
Parsing Error in model customers (models/customers.py)
  expression cannot contain assignment, perhaps you meant "=="? (customers.py, line 13)
              'first_order' = F.min("order_date"),
  


============================== 2023-01-24 03:02:05.125555 | 1596ecda-549b-4aad-91a4-1bdbcaa2bdbf ==============================
[0m03:02:05.125709 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:02:05.126828 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m03:02:05.127010 [debug] [MainThread]: Tracking: tracking
[0m03:02:05.127541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110949af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110949b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110949bb0>]}
[0m03:02:05.181292 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m03:02:05.181682 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m03:02:05.193526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110a28a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ed9a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ed9be0>]}
[0m03:02:05.193734 [debug] [MainThread]: Flushing usage events
[0m03:02:06.338758 [error] [MainThread]: Encountered an error:
Parsing Error in model customers (models/customers.py)
  invalid syntax (customers.py, line 13)
              F.min("order_date").as('first_order'),
  


============================== 2023-01-24 03:04:34.282883 | 528cfbc4-1573-451e-b078-b2f46a9e26ac ==============================
[0m03:04:34.282930 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:04:34.283703 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m03:04:34.283845 [debug] [MainThread]: Tracking: tracking
[0m03:04:34.284174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e86ebe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e86ec40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e86eca0>]}
[0m03:04:34.338106 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m03:04:34.338587 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m03:04:34.395818 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '528cfbc4-1573-451e-b078-b2f46a9e26ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f001f40>]}
[0m03:04:34.402453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '528cfbc4-1573-451e-b078-b2f46a9e26ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e998490>]}
[0m03:04:34.402708 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m03:04:34.402902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '528cfbc4-1573-451e-b078-b2f46a9e26ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10edc6100>]}
[0m03:04:34.404055 [info ] [MainThread]: 
[0m03:04:34.404532 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:04:34.405290 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m03:04:34.417111 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m03:04:34.417421 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m03:04:34.417525 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:04:35.631884 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.21 seconds
[0m03:04:35.634597 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m03:04:35.976356 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m03:04:35.991627 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m03:04:35.992011 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m03:04:35.992233 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:04:36.604734 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 0.61 seconds
[0m03:04:36.608614 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m03:04:36.958642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '528cfbc4-1573-451e-b078-b2f46a9e26ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e8864f0>]}
[0m03:04:36.960648 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:04:36.961627 [info ] [MainThread]: 
[0m03:04:36.969641 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m03:04:36.970408 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m03:04:36.971848 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m03:04:36.972004 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m03:04:36.972512 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m03:04:37.001708 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m03:04:37.002893 [debug] [Thread-1  ]: finished collecting timing info
[0m03:04:37.003139 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m03:04:37.037690 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m03:04:37.039977 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:04:37.040108 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.customers__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    stg_customers = dbt.ref('stg_customers')
    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    customer_orders = (
        stg_orders
        .group_by("customer_id")
        .agg(
            min(col("order_date")).name('first_order'),
            max(col("order_date")).name('most_recent_order'),
            count(col("order_id")).name('number_of_orders')
        )
    )

    customer_payments = (
        stg_payments
        .join(orders, payments["order_id"] == orders["order_id"], "left")
        .group_by("orders.customer_id")
        .agg(
            total_amount = F.sum("amount")
        )
    )

    final_df = (
        stg_customers
        .join(customer_orders, customers["customer_id"] == customer_orders["customer_id"], "left")
        .join(customer_payments, customers["customer_id"] == customer_payments["customer_id"], "left")
        .select("customers.customer_id",
                "customers.first_name",
                "customers.last_name",
                "customer_orders.first_order",
                "customer_orders.most_recent_order",
                "customer_orders.number_of_orders",
                F.col("customer_payments.total_amount").alias("customer_lifetime_value")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle_shop.bruno.stg_customers", "stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle_shop.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.customers", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m03:04:37.040212 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:04:42.492223 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.45 seconds
[0m03:04:42.492604 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:04:42.492778 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CALL jaffle_shop.bruno.customers__dbt_sp();
[0m03:04:43.483525 [debug] [Thread-1  ]: Snowflake adapter: Snowflake query id: 01a9dad8-0000-0c56-0000-0000468d3729
[0m03:04:43.483745 [debug] [Thread-1  ]: Snowflake adapter: Snowflake error: 100357 (P0000): Python Interpreter Error:
Traceback (most recent call last):
  File "_udf_code.py", line 122, in main
  File "_udf_code.py", line 17, in model
NameError: name 'col' is not defined
 in function CUSTOMERS__DBT_SP with handler main
[0m03:04:43.484045 [debug] [Thread-1  ]: finished collecting timing info
[0m03:04:43.484206 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m03:04:43.950361 [debug] [Thread-1  ]: Database Error in model customers (models/customers.py)
  100357 (P0000): Python Interpreter Error:
  Traceback (most recent call last):
    File "_udf_code.py", line 122, in main
    File "_udf_code.py", line 17, in model
  NameError: name 'col' is not defined
   in function CUSTOMERS__DBT_SP with handler main
  compiled Code at target/run/jaffle_shop/models/customers.py
[0m03:04:43.951265 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '528cfbc4-1573-451e-b078-b2f46a9e26ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110054460>]}
[0m03:04:43.952061 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 6.98s]
[0m03:04:43.953127 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m03:04:43.956667 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:04:43.957802 [info ] [MainThread]: 
[0m03:04:43.958357 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 9.55 seconds (9.55s).
[0m03:04:43.958824 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:04:43.959068 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m03:04:43.974217 [info ] [MainThread]: 
[0m03:04:43.974711 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m03:04:43.975099 [info ] [MainThread]: 
[0m03:04:43.975414 [error] [MainThread]: [33mDatabase Error in model customers (models/customers.py)[0m
[0m03:04:43.975720 [error] [MainThread]:   100357 (P0000): Python Interpreter Error:
[0m03:04:43.976006 [error] [MainThread]:   Traceback (most recent call last):
[0m03:04:43.976292 [error] [MainThread]:     File "_udf_code.py", line 122, in main
[0m03:04:43.976682 [error] [MainThread]:     File "_udf_code.py", line 17, in model
[0m03:04:43.977042 [error] [MainThread]:   NameError: name 'col' is not defined
[0m03:04:43.977347 [error] [MainThread]:    in function CUSTOMERS__DBT_SP with handler main
[0m03:04:43.977639 [error] [MainThread]:   compiled Code at target/run/jaffle_shop/models/customers.py
[0m03:04:43.977968 [info ] [MainThread]: 
[0m03:04:43.978288 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m03:04:43.978752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10edc60a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e858820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110054700>]}
[0m03:04:43.979105 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 03:06:16.365112 | bd26bc3b-cdcf-4ec6-a7d8-709856e4dd52 ==============================
[0m03:06:16.365183 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:06:16.366076 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m03:06:16.366217 [debug] [MainThread]: Tracking: tracking
[0m03:06:16.366564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110939b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110939be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110939c40>]}
[0m03:06:16.419805 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m03:06:16.420210 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m03:06:16.474868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bd26bc3b-cdcf-4ec6-a7d8-709856e4dd52', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ebc970>]}
[0m03:06:16.481218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bd26bc3b-cdcf-4ec6-a7d8-709856e4dd52', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110a5e400>]}
[0m03:06:16.481419 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m03:06:16.481609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bd26bc3b-cdcf-4ec6-a7d8-709856e4dd52', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110e8b0a0>]}
[0m03:06:16.482699 [info ] [MainThread]: 
[0m03:06:16.483163 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:06:16.483865 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m03:06:16.494667 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m03:06:16.494794 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m03:06:16.494890 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:06:17.482125 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 0.99 seconds
[0m03:06:17.487476 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m03:06:17.824313 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m03:06:17.838924 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m03:06:17.839224 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m03:06:17.839446 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:06:18.512317 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 0.67 seconds
[0m03:06:18.517985 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m03:06:18.932420 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bd26bc3b-cdcf-4ec6-a7d8-709856e4dd52', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11091f820>]}
[0m03:06:18.933801 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:06:18.934355 [info ] [MainThread]: 
[0m03:06:18.941319 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m03:06:18.942013 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m03:06:18.943210 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m03:06:18.943475 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m03:06:18.944310 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m03:06:18.974694 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m03:06:18.975841 [debug] [Thread-1  ]: finished collecting timing info
[0m03:06:18.976060 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m03:06:19.009443 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m03:06:19.011616 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:06:19.011736 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.customers__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    stg_customers = dbt.ref('stg_customers')
    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    customer_orders = (
        stg_orders
        .group_by("customer_id")
        .agg(
            F.min(F.col("order_date")).name('first_order'),
            F.max(F.col("order_date")).name('most_recent_order'),
            F.count(F.col("order_id")).name('number_of_orders')
        )
    )

    customer_payments = (
        stg_payments
        .join(orders, payments["order_id"] == orders["order_id"], "left")
        .group_by("orders.customer_id")
        .agg(
            total_amount = F.sum("amount")
        )
    )

    final_df = (
        stg_customers
        .join(customer_orders, customers["customer_id"] == customer_orders["customer_id"], "left")
        .join(customer_payments, customers["customer_id"] == customer_payments["customer_id"], "left")
        .select("customers.customer_id",
                "customers.first_name",
                "customers.last_name",
                "customer_orders.first_order",
                "customer_orders.most_recent_order",
                "customer_orders.number_of_orders",
                F.col("customer_payments.total_amount").alias("customer_lifetime_value")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle_shop.bruno.stg_customers", "stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle_shop.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.customers", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m03:06:19.011839 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:06:24.451406 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.44 seconds
[0m03:06:24.452283 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:06:24.452621 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CALL jaffle_shop.bruno.customers__dbt_sp();
[0m03:06:25.541844 [debug] [Thread-1  ]: Snowflake adapter: Snowflake query id: 01a9dada-0000-0c55-0000-0000468d273d
[0m03:06:25.542068 [debug] [Thread-1  ]: Snowflake adapter: Snowflake error: 100357 (P0000): Python Interpreter Error:
Traceback (most recent call last):
  File "_udf_code.py", line 122, in main
  File "_udf_code.py", line 25, in model
NameError: name 'orders' is not defined
 in function CUSTOMERS__DBT_SP with handler main
[0m03:06:25.542379 [debug] [Thread-1  ]: finished collecting timing info
[0m03:06:25.542535 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m03:06:26.099725 [debug] [Thread-1  ]: Database Error in model customers (models/customers.py)
  100357 (P0000): Python Interpreter Error:
  Traceback (most recent call last):
    File "_udf_code.py", line 122, in main
    File "_udf_code.py", line 25, in model
  NameError: name 'orders' is not defined
   in function CUSTOMERS__DBT_SP with handler main
  compiled Code at target/run/jaffle_shop/models/customers.py
[0m03:06:26.100534 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd26bc3b-cdcf-4ec6-a7d8-709856e4dd52', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112112e20>]}
[0m03:06:26.101262 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 7.16s]
[0m03:06:26.102273 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m03:06:26.105517 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:06:26.106572 [info ] [MainThread]: 
[0m03:06:26.107096 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 9.62 seconds (9.62s).
[0m03:06:26.107582 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:06:26.107827 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m03:06:26.123227 [info ] [MainThread]: 
[0m03:06:26.123686 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m03:06:26.124085 [info ] [MainThread]: 
[0m03:06:26.124411 [error] [MainThread]: [33mDatabase Error in model customers (models/customers.py)[0m
[0m03:06:26.124728 [error] [MainThread]:   100357 (P0000): Python Interpreter Error:
[0m03:06:26.125017 [error] [MainThread]:   Traceback (most recent call last):
[0m03:06:26.125302 [error] [MainThread]:     File "_udf_code.py", line 122, in main
[0m03:06:26.125596 [error] [MainThread]:     File "_udf_code.py", line 25, in model
[0m03:06:26.125876 [error] [MainThread]:   NameError: name 'orders' is not defined
[0m03:06:26.126160 [error] [MainThread]:    in function CUSTOMERS__DBT_SP with handler main
[0m03:06:26.126444 [error] [MainThread]:   compiled Code at target/run/jaffle_shop/models/customers.py
[0m03:06:26.126758 [info ] [MainThread]: 
[0m03:06:26.127071 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m03:06:26.127534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110a5e2e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110f67520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112112d00>]}
[0m03:06:26.127904 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 03:06:54.781925 | 7e9596d2-9a22-489f-9b89-f88179733fc6 ==============================
[0m03:06:54.781990 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:06:54.782854 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m03:06:54.782980 [debug] [MainThread]: Tracking: tracking
[0m03:06:54.783357 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e723b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e723bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e723c10>]}
[0m03:06:54.834032 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m03:06:54.834486 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m03:06:54.890097 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7e9596d2-9a22-489f-9b89-f88179733fc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ed32fa0>]}
[0m03:06:54.896763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7e9596d2-9a22-489f-9b89-f88179733fc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e849400>]}
[0m03:06:54.896974 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m03:06:54.897162 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7e9596d2-9a22-489f-9b89-f88179733fc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ec73fd0>]}
[0m03:06:54.898393 [info ] [MainThread]: 
[0m03:06:54.898897 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:06:54.899666 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m03:06:54.911548 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m03:06:54.911735 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m03:06:54.911832 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:06:55.995765 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.08 seconds
[0m03:06:55.998891 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m03:06:56.329466 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m03:06:56.341698 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m03:06:56.342078 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m03:06:56.342271 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:06:57.327559 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 0.99 seconds
[0m03:06:57.332138 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m03:06:57.655618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7e9596d2-9a22-489f-9b89-f88179733fc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e849910>]}
[0m03:06:57.656610 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:06:57.657000 [info ] [MainThread]: 
[0m03:06:57.661631 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m03:06:57.662000 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m03:06:57.662765 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m03:06:57.662942 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m03:06:57.663534 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m03:06:57.686808 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m03:06:57.687925 [debug] [Thread-1  ]: finished collecting timing info
[0m03:06:57.688115 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m03:06:57.717445 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m03:06:57.719605 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:06:57.719747 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.customers__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    stg_customers = dbt.ref('stg_customers')
    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    customer_orders = (
        stg_orders
        .group_by("customer_id")
        .agg(
            F.min(F.col("order_date")).name('first_order'),
            F.max(F.col("order_date")).name('most_recent_order'),
            F.count(F.col("order_id")).name('number_of_orders')
        )
    )

    customer_payments = (
        stg_payments
        .join(stg_orders, stg_payments["order_id"] == orders["order_id"], "left")
        .group_by("orders.customer_id")
        .agg(
            total_amount = F.sum("amount")
        )
    )

    final_df = (
        stg_customers
        .join(customer_orders, customers["customer_id"] == customer_orders["customer_id"], "left")
        .join(customer_payments, customers["customer_id"] == customer_payments["customer_id"], "left")
        .select("customers.customer_id",
                "customers.first_name",
                "customers.last_name",
                "customer_orders.first_order",
                "customer_orders.most_recent_order",
                "customer_orders.number_of_orders",
                F.col("customer_payments.total_amount").alias("customer_lifetime_value")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle_shop.bruno.stg_customers", "stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle_shop.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.customers", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m03:06:57.719850 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:07:03.261547 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.54 seconds
[0m03:07:03.262572 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:07:03.262713 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CALL jaffle_shop.bruno.customers__dbt_sp();
[0m03:07:04.246234 [debug] [Thread-1  ]: Snowflake adapter: Snowflake query id: 01a9dadb-0000-0c55-0000-0000468d274d
[0m03:07:04.246442 [debug] [Thread-1  ]: Snowflake adapter: Snowflake error: 100357 (P0000): Python Interpreter Error:
Traceback (most recent call last):
  File "_udf_code.py", line 122, in main
  File "_udf_code.py", line 25, in model
NameError: name 'orders' is not defined
 in function CUSTOMERS__DBT_SP with handler main
[0m03:07:04.246718 [debug] [Thread-1  ]: finished collecting timing info
[0m03:07:04.246862 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m03:07:04.602012 [debug] [Thread-1  ]: Database Error in model customers (models/customers.py)
  100357 (P0000): Python Interpreter Error:
  Traceback (most recent call last):
    File "_udf_code.py", line 122, in main
    File "_udf_code.py", line 25, in model
  NameError: name 'orders' is not defined
   in function CUSTOMERS__DBT_SP with handler main
  compiled Code at target/run/jaffle_shop/models/customers.py
[0m03:07:04.602787 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7e9596d2-9a22-489f-9b89-f88179733fc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fefcf70>]}
[0m03:07:04.603482 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 6.94s]
[0m03:07:04.604417 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m03:07:04.607282 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:07:04.608267 [info ] [MainThread]: 
[0m03:07:04.608783 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 9.71 seconds (9.71s).
[0m03:07:04.609246 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:07:04.609485 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m03:07:04.624556 [info ] [MainThread]: 
[0m03:07:04.625014 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m03:07:04.625404 [info ] [MainThread]: 
[0m03:07:04.625711 [error] [MainThread]: [33mDatabase Error in model customers (models/customers.py)[0m
[0m03:07:04.626015 [error] [MainThread]:   100357 (P0000): Python Interpreter Error:
[0m03:07:04.626302 [error] [MainThread]:   Traceback (most recent call last):
[0m03:07:04.626581 [error] [MainThread]:     File "_udf_code.py", line 122, in main
[0m03:07:04.626860 [error] [MainThread]:     File "_udf_code.py", line 25, in model
[0m03:07:04.627138 [error] [MainThread]:   NameError: name 'orders' is not defined
[0m03:07:04.627413 [error] [MainThread]:    in function CUSTOMERS__DBT_SP with handler main
[0m03:07:04.627686 [error] [MainThread]:   compiled Code at target/run/jaffle_shop/models/customers.py
[0m03:07:04.627974 [info ] [MainThread]: 
[0m03:07:04.628266 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m03:07:04.628678 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ecdc940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e5a3160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fefc130>]}
[0m03:07:04.629003 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 03:08:00.813541 | b714f6aa-a2b4-45c2-a087-057ee09c031d ==============================
[0m03:08:00.813603 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:08:00.814377 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m03:08:00.814569 [debug] [MainThread]: Tracking: tracking
[0m03:08:00.814998 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dbf2b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dbf2b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dbf2be0>]}
[0m03:08:00.867909 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m03:08:00.868356 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m03:08:00.923177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b714f6aa-a2b4-45c2-a087-057ee09c031d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e418f70>]}
[0m03:08:00.929553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b714f6aa-a2b4-45c2-a087-057ee09c031d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dd1c400>]}
[0m03:08:00.929762 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m03:08:00.929946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b714f6aa-a2b4-45c2-a087-057ee09c031d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10da86850>]}
[0m03:08:00.931043 [info ] [MainThread]: 
[0m03:08:00.931522 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:08:00.932285 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m03:08:00.943966 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m03:08:00.944149 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m03:08:00.944245 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:08:02.042195 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.1 seconds
[0m03:08:02.047520 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m03:08:02.406461 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m03:08:02.420687 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m03:08:02.420994 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m03:08:02.421227 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:08:03.064846 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 0.64 seconds
[0m03:08:03.068080 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m03:08:03.507152 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b714f6aa-a2b4-45c2-a087-057ee09c031d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e14a070>]}
[0m03:08:03.508046 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:08:03.508406 [info ] [MainThread]: 
[0m03:08:03.513159 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m03:08:03.513534 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m03:08:03.514301 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m03:08:03.514489 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m03:08:03.515068 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m03:08:03.537164 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m03:08:03.538133 [debug] [Thread-1  ]: finished collecting timing info
[0m03:08:03.538315 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m03:08:03.566412 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m03:08:03.568421 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:08:03.568532 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.customers__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    stg_customers = dbt.ref('stg_customers')
    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    customer_orders = (
        stg_orders
        .group_by("customer_id")
        .agg(
            F.min(F.col("order_date")).name('first_order'),
            F.max(F.col("order_date")).name('most_recent_order'),
            F.count(F.col("order_id")).name('number_of_orders')
        )
    )

    customer_payments = (
        stg_payments
        .join(stg_orders, stg_payments["order_id"] == stg_orders["order_id"], "left")
        .group_by("stg_orders.customer_id")
        .agg(
            F.sum(F.col("amount")).name('total_amount')
        )
    )

    final_df = (
        stg_customers
        .join(customer_orders, customers["customer_id"] == customer_orders["customer_id"], "left")
        .join(customer_payments, customers["customer_id"] == customer_payments["customer_id"], "left")
        .select("customer_orders.customer_id",
                "customer_orders.first_name",
                "customer_orders.last_name",
                "customer_orders.first_order",
                "customer_orders.most_recent_order",
                "customer_orders.number_of_orders",
                F.col("customer_payments.total_amount").alias("customer_lifetime_value")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle_shop.bruno.stg_customers", "stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle_shop.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.customers", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m03:08:03.568628 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:08:09.311303 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.74 seconds
[0m03:08:09.312523 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:08:09.312708 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CALL jaffle_shop.bruno.customers__dbt_sp();
[0m03:08:10.380774 [debug] [Thread-1  ]: Snowflake adapter: Snowflake query id: 01a9dadc-0000-0c56-0000-0000468d3745
[0m03:08:10.380984 [debug] [Thread-1  ]: Snowflake adapter: Snowflake error: 100357 (P0000): Python Interpreter Error:
Traceback (most recent call last):
  File "_udf_code.py", line 122, in main
  File "_udf_code.py", line 24, in model
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/telemetry.py", line 213, in wrap
    r = func(*args, **kwargs)
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 1314, in group_by
    grouping_exprs = self._convert_cols_to_exprs("group_by()", *cols)
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3427, in _convert_cols_to_exprs
    exprs = [convert(col) for col in parse_positional_args_to_list(*cols)]
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3427, in <listcomp>
    exprs = [convert(col) for col in parse_positional_args_to_list(*cols)]
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3418, in convert
    return self._resolve(col)
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3387, in _resolve
    raise SnowparkClientExceptionMessages.DF_CANNOT_RESOLVE_COLUMN_NAME(
snowflake.snowpark.exceptions.SnowparkColumnException: (1105): The DataFrame does not contain the column named stg_orders.customer_id.
 in function CUSTOMERS__DBT_SP with handler main
[0m03:08:10.381273 [debug] [Thread-1  ]: finished collecting timing info
[0m03:08:10.381419 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m03:08:10.699271 [debug] [Thread-1  ]: Database Error in model customers (models/customers.py)
  100357 (P0000): Python Interpreter Error:
  Traceback (most recent call last):
    File "_udf_code.py", line 122, in main
    File "_udf_code.py", line 24, in model
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/telemetry.py", line 213, in wrap
      r = func(*args, **kwargs)
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 1314, in group_by
      grouping_exprs = self._convert_cols_to_exprs("group_by()", *cols)
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3427, in _convert_cols_to_exprs
      exprs = [convert(col) for col in parse_positional_args_to_list(*cols)]
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3427, in <listcomp>
      exprs = [convert(col) for col in parse_positional_args_to_list(*cols)]
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3418, in convert
      return self._resolve(col)
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3387, in _resolve
      raise SnowparkClientExceptionMessages.DF_CANNOT_RESOLVE_COLUMN_NAME(
  snowflake.snowpark.exceptions.SnowparkColumnException: (1105): The DataFrame does not contain the column named stg_orders.customer_id.
   in function CUSTOMERS__DBT_SP with handler main
  compiled Code at target/run/jaffle_shop/models/customers.py
[0m03:08:10.700276 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b714f6aa-a2b4-45c2-a087-057ee09c031d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f3d4580>]}
[0m03:08:10.701139 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 7.19s]
[0m03:08:10.703385 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m03:08:10.709147 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:08:10.710079 [info ] [MainThread]: 
[0m03:08:10.710530 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 9.78 seconds (9.78s).
[0m03:08:10.710870 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:08:10.711001 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m03:08:10.722753 [info ] [MainThread]: 
[0m03:08:10.723095 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m03:08:10.723428 [info ] [MainThread]: 
[0m03:08:10.723637 [error] [MainThread]: [33mDatabase Error in model customers (models/customers.py)[0m
[0m03:08:10.723827 [error] [MainThread]:   100357 (P0000): Python Interpreter Error:
[0m03:08:10.724000 [error] [MainThread]:   Traceback (most recent call last):
[0m03:08:10.724289 [error] [MainThread]:     File "_udf_code.py", line 122, in main
[0m03:08:10.724457 [error] [MainThread]:     File "_udf_code.py", line 24, in model
[0m03:08:10.724613 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/telemetry.py", line 213, in wrap
[0m03:08:10.724783 [error] [MainThread]:       r = func(*args, **kwargs)
[0m03:08:10.724940 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 1314, in group_by
[0m03:08:10.725098 [error] [MainThread]:       grouping_exprs = self._convert_cols_to_exprs("group_by()", *cols)
[0m03:08:10.725253 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3427, in _convert_cols_to_exprs
[0m03:08:10.725413 [error] [MainThread]:       exprs = [convert(col) for col in parse_positional_args_to_list(*cols)]
[0m03:08:10.725566 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3427, in <listcomp>
[0m03:08:10.725758 [error] [MainThread]:       exprs = [convert(col) for col in parse_positional_args_to_list(*cols)]
[0m03:08:10.725914 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3418, in convert
[0m03:08:10.726068 [error] [MainThread]:       return self._resolve(col)
[0m03:08:10.726222 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3387, in _resolve
[0m03:08:10.726458 [error] [MainThread]:       raise SnowparkClientExceptionMessages.DF_CANNOT_RESOLVE_COLUMN_NAME(
[0m03:08:10.726641 [error] [MainThread]:   snowflake.snowpark.exceptions.SnowparkColumnException: (1105): The DataFrame does not contain the column named stg_orders.customer_id.
[0m03:08:10.726812 [error] [MainThread]:    in function CUSTOMERS__DBT_SP with handler main
[0m03:08:10.726972 [error] [MainThread]:   compiled Code at target/run/jaffle_shop/models/customers.py
[0m03:08:10.727170 [info ] [MainThread]: 
[0m03:08:10.727359 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m03:08:10.727651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10956e790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e155b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f3b99a0>]}
[0m03:08:10.727897 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 03:08:48.692040 | e2c0bea5-8b1f-48af-afdb-5f1a1091b1c9 ==============================
[0m03:08:48.692097 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:08:48.692973 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m03:08:48.693123 [debug] [MainThread]: Tracking: tracking
[0m03:08:48.693566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112545bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112545c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112545c70>]}
[0m03:08:48.746210 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m03:08:48.746627 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m03:08:48.804197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e2c0bea5-8b1f-48af-afdb-5f1a1091b1c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112ace9a0>]}
[0m03:08:48.811130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e2c0bea5-8b1f-48af-afdb-5f1a1091b1c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11266f430>]}
[0m03:08:48.811355 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m03:08:48.811547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e2c0bea5-8b1f-48af-afdb-5f1a1091b1c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a9d0d0>]}
[0m03:08:48.812732 [info ] [MainThread]: 
[0m03:08:48.813208 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:08:48.813911 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m03:08:48.826211 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m03:08:48.826422 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m03:08:48.826522 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:08:49.749762 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 0.92 seconds
[0m03:08:49.754624 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m03:08:50.181155 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m03:08:50.195881 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m03:08:50.196173 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m03:08:50.196404 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:08:51.090804 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 0.89 seconds
[0m03:08:51.098038 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m03:08:51.437420 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e2c0bea5-8b1f-48af-afdb-5f1a1091b1c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112aa8f40>]}
[0m03:08:51.438392 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:08:51.438757 [info ] [MainThread]: 
[0m03:08:51.443873 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m03:08:51.444279 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m03:08:51.445044 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m03:08:51.445207 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m03:08:51.445728 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m03:08:51.468303 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m03:08:51.469270 [debug] [Thread-1  ]: finished collecting timing info
[0m03:08:51.469445 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m03:08:51.498212 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m03:08:51.500309 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:08:51.500454 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.customers__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    stg_customers = dbt.ref('stg_customers')
    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    customer_orders = (
        stg_orders
        .group_by("customer_id")
        .agg(
            F.min(F.col("order_date")).name('first_order'),
            F.max(F.col("order_date")).name('most_recent_order'),
            F.count(F.col("order_id")).name('number_of_orders')
        )
    )

    customer_payments = (
        stg_payments
        .join(stg_orders, stg_payments["order_id"] == stg_orders["order_id"], "left")
        .group_by("stg_orders.customer_id")
        .agg(
            F.sum(F.col("amount")).name('total_amount')
        )
    )

    final_df = (
        stg_customers
        .join(customer_orders, customers["customer_id"] == customer_orders["customer_id"], "left")
        .join(customer_payments, customers["customer_id"] == customer_payments["customer_id"], "left")
        .select("customer_orders.customer_id",
                "customer_orders.first_name",
                "customer_orders.last_name",
                "customer_orders.first_order",
                "customer_orders.most_recent_order",
                "customer_orders.number_of_orders",
                F.col("customer_payments.total_amount").name("customer_lifetime_value")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle_shop.bruno.stg_customers", "stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle_shop.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.customers", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m03:08:51.500553 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:08:56.944202 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.44 seconds
[0m03:08:56.945681 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:08:56.946262 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CALL jaffle_shop.bruno.customers__dbt_sp();
[0m03:08:58.009916 [debug] [Thread-1  ]: Snowflake adapter: Snowflake query id: 01a9dadc-0000-0c55-0000-0000468d2765
[0m03:08:58.010139 [debug] [Thread-1  ]: Snowflake adapter: Snowflake error: 100357 (P0000): Python Interpreter Error:
Traceback (most recent call last):
  File "_udf_code.py", line 122, in main
  File "_udf_code.py", line 24, in model
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/telemetry.py", line 213, in wrap
    r = func(*args, **kwargs)
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 1314, in group_by
    grouping_exprs = self._convert_cols_to_exprs("group_by()", *cols)
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3427, in _convert_cols_to_exprs
    exprs = [convert(col) for col in parse_positional_args_to_list(*cols)]
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3427, in <listcomp>
    exprs = [convert(col) for col in parse_positional_args_to_list(*cols)]
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3418, in convert
    return self._resolve(col)
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3387, in _resolve
    raise SnowparkClientExceptionMessages.DF_CANNOT_RESOLVE_COLUMN_NAME(
snowflake.snowpark.exceptions.SnowparkColumnException: (1105): The DataFrame does not contain the column named stg_orders.customer_id.
 in function CUSTOMERS__DBT_SP with handler main
[0m03:08:58.010441 [debug] [Thread-1  ]: finished collecting timing info
[0m03:08:58.010595 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m03:08:58.345069 [debug] [Thread-1  ]: Database Error in model customers (models/customers.py)
  100357 (P0000): Python Interpreter Error:
  Traceback (most recent call last):
    File "_udf_code.py", line 122, in main
    File "_udf_code.py", line 24, in model
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/telemetry.py", line 213, in wrap
      r = func(*args, **kwargs)
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 1314, in group_by
      grouping_exprs = self._convert_cols_to_exprs("group_by()", *cols)
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3427, in _convert_cols_to_exprs
      exprs = [convert(col) for col in parse_positional_args_to_list(*cols)]
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3427, in <listcomp>
      exprs = [convert(col) for col in parse_positional_args_to_list(*cols)]
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3418, in convert
      return self._resolve(col)
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3387, in _resolve
      raise SnowparkClientExceptionMessages.DF_CANNOT_RESOLVE_COLUMN_NAME(
  snowflake.snowpark.exceptions.SnowparkColumnException: (1105): The DataFrame does not contain the column named stg_orders.customer_id.
   in function CUSTOMERS__DBT_SP with handler main
  compiled Code at target/run/jaffle_shop/models/customers.py
[0m03:08:58.345827 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e2c0bea5-8b1f-48af-afdb-5f1a1091b1c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113037310>]}
[0m03:08:58.346563 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 6.90s]
[0m03:08:58.347547 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m03:08:58.350466 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:08:58.351464 [info ] [MainThread]: 
[0m03:08:58.352001 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 9.54 seconds (9.54s).
[0m03:08:58.352478 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:08:58.352717 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m03:08:58.367605 [info ] [MainThread]: 
[0m03:08:58.368031 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m03:08:58.368409 [info ] [MainThread]: 
[0m03:08:58.368712 [error] [MainThread]: [33mDatabase Error in model customers (models/customers.py)[0m
[0m03:08:58.369020 [error] [MainThread]:   100357 (P0000): Python Interpreter Error:
[0m03:08:58.369308 [error] [MainThread]:   Traceback (most recent call last):
[0m03:08:58.369594 [error] [MainThread]:     File "_udf_code.py", line 122, in main
[0m03:08:58.369885 [error] [MainThread]:     File "_udf_code.py", line 24, in model
[0m03:08:58.370170 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/telemetry.py", line 213, in wrap
[0m03:08:58.370464 [error] [MainThread]:       r = func(*args, **kwargs)
[0m03:08:58.370748 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 1314, in group_by
[0m03:08:58.371032 [error] [MainThread]:       grouping_exprs = self._convert_cols_to_exprs("group_by()", *cols)
[0m03:08:58.371319 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3427, in _convert_cols_to_exprs
[0m03:08:58.371602 [error] [MainThread]:       exprs = [convert(col) for col in parse_positional_args_to_list(*cols)]
[0m03:08:58.371888 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3427, in <listcomp>
[0m03:08:58.372306 [error] [MainThread]:       exprs = [convert(col) for col in parse_positional_args_to_list(*cols)]
[0m03:08:58.372588 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3418, in convert
[0m03:08:58.372872 [error] [MainThread]:       return self._resolve(col)
[0m03:08:58.373154 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 3387, in _resolve
[0m03:08:58.373437 [error] [MainThread]:       raise SnowparkClientExceptionMessages.DF_CANNOT_RESOLVE_COLUMN_NAME(
[0m03:08:58.373718 [error] [MainThread]:   snowflake.snowpark.exceptions.SnowparkColumnException: (1105): The DataFrame does not contain the column named stg_orders.customer_id.
[0m03:08:58.374000 [error] [MainThread]:    in function CUSTOMERS__DBT_SP with handler main
[0m03:08:58.374283 [error] [MainThread]:   compiled Code at target/run/jaffle_shop/models/customers.py
[0m03:08:58.374595 [info ] [MainThread]: 
[0m03:08:58.374914 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m03:08:58.375398 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a9d070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11255d100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113037130>]}
[0m03:08:58.375774 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 03:10:52.560843 | b799ff73-311a-4098-ac2b-ea17c4d61492 ==============================
[0m03:10:52.560902 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:10:52.561752 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m03:10:52.561882 [debug] [MainThread]: Tracking: tracking
[0m03:10:52.562270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a30bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a30c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a30c70>]}
[0m03:10:52.618211 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m03:10:52.618830 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m03:10:52.676038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b799ff73-311a-4098-ac2b-ea17c4d61492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112fb89a0>]}
[0m03:10:52.682624 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b799ff73-311a-4098-ac2b-ea17c4d61492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112b58430>]}
[0m03:10:52.682856 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m03:10:52.683048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b799ff73-311a-4098-ac2b-ea17c4d61492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112f860d0>]}
[0m03:10:52.684445 [info ] [MainThread]: 
[0m03:10:52.685356 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:10:52.686290 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m03:10:52.697447 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m03:10:52.697585 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m03:10:52.697681 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:10:53.921875 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.22 seconds
[0m03:10:53.929031 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m03:10:54.267689 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m03:10:54.287018 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m03:10:54.287447 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m03:10:54.287680 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:10:54.898013 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 0.61 seconds
[0m03:10:54.901953 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m03:10:55.243204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b799ff73-311a-4098-ac2b-ea17c4d61492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112f91f40>]}
[0m03:10:55.244722 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:10:55.245270 [info ] [MainThread]: 
[0m03:10:55.253253 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m03:10:55.254015 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m03:10:55.255506 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m03:10:55.256127 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m03:10:55.257843 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m03:10:55.294472 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m03:10:55.295489 [debug] [Thread-1  ]: finished collecting timing info
[0m03:10:55.295738 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m03:10:55.329453 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m03:10:55.331520 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:10:55.331632 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.customers__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    stg_customers = dbt.ref('stg_customers')
    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    customer_orders = (
        stg_orders
        .group_by("customer_id")
        .agg(
            F.min(F.col("order_date")).name('first_order'),
            F.max(F.col("order_date")).name('most_recent_order'),
            F.count(F.col("order_id")).name('number_of_orders')
        )
    )

    customer_payments = (
        stg_payments
        .join(stg_orders, stg_payments.order_id == stg_orders.order_id, "left")
        .group_by(stg_orders.customer_id)
        .agg(
            F.sum(F.col("amount")).name('total_amount')
        )
    )

    final_df = (
        stg_customers
        .join(customer_orders, customers["customer_id"] == customer_orders["customer_id"], "left")
        .join(customer_payments, customers["customer_id"] == customer_payments["customer_id"], "left")
        .select("customer_orders.customer_id",
                "customer_orders.first_name",
                "customer_orders.last_name",
                "customer_orders.first_order",
                "customer_orders.most_recent_order",
                "customer_orders.number_of_orders",
                F.col("customer_payments.total_amount").name("customer_lifetime_value")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle_shop.bruno.stg_customers", "stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle_shop.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.customers", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m03:10:55.331780 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:11:00.938659 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.61 seconds
[0m03:11:00.940979 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:11:00.941631 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CALL jaffle_shop.bruno.customers__dbt_sp();
[0m03:11:02.026488 [debug] [Thread-1  ]: Snowflake adapter: Snowflake query id: 01a9dadf-0000-0c55-0000-0000468d2785
[0m03:11:02.026729 [debug] [Thread-1  ]: Snowflake adapter: Snowflake error: 100357 (P0000): Python Interpreter Error:
Traceback (most recent call last):
  File "_udf_code.py", line 122, in main
  File "_udf_code.py", line 34, in model
NameError: name 'customers' is not defined
 in function CUSTOMERS__DBT_SP with handler main
[0m03:11:02.027062 [debug] [Thread-1  ]: finished collecting timing info
[0m03:11:02.027219 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m03:11:02.359972 [debug] [Thread-1  ]: Database Error in model customers (models/customers.py)
  100357 (P0000): Python Interpreter Error:
  Traceback (most recent call last):
    File "_udf_code.py", line 122, in main
    File "_udf_code.py", line 34, in model
  NameError: name 'customers' is not defined
   in function CUSTOMERS__DBT_SP with handler main
  compiled Code at target/run/jaffle_shop/models/customers.py
[0m03:11:02.361152 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b799ff73-311a-4098-ac2b-ea17c4d61492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11463f790>]}
[0m03:11:02.362204 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 7.11s]
[0m03:11:02.363753 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m03:11:02.366918 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:11:02.368372 [info ] [MainThread]: 
[0m03:11:02.369117 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 9.68 seconds (9.68s).
[0m03:11:02.369908 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:11:02.370376 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m03:11:02.385547 [info ] [MainThread]: 
[0m03:11:02.386004 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m03:11:02.386333 [info ] [MainThread]: 
[0m03:11:02.386593 [error] [MainThread]: [33mDatabase Error in model customers (models/customers.py)[0m
[0m03:11:02.386920 [error] [MainThread]:   100357 (P0000): Python Interpreter Error:
[0m03:11:02.387350 [error] [MainThread]:   Traceback (most recent call last):
[0m03:11:02.387682 [error] [MainThread]:     File "_udf_code.py", line 122, in main
[0m03:11:02.387925 [error] [MainThread]:     File "_udf_code.py", line 34, in model
[0m03:11:02.388152 [error] [MainThread]:   NameError: name 'customers' is not defined
[0m03:11:02.388375 [error] [MainThread]:    in function CUSTOMERS__DBT_SP with handler main
[0m03:11:02.388713 [error] [MainThread]:   compiled Code at target/run/jaffle_shop/models/customers.py
[0m03:11:02.388965 [info ] [MainThread]: 
[0m03:11:02.389214 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m03:11:02.389565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112f86070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a47160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11463f910>]}
[0m03:11:02.389878 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 03:12:49.531262 | 781d3df1-7cdb-4263-a9c7-2f316f87418e ==============================
[0m03:12:49.531318 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:12:49.532077 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m03:12:49.532225 [debug] [MainThread]: Tracking: tracking
[0m03:12:49.532570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d01bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d01c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d01c70>]}
[0m03:12:49.588015 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m03:12:49.588414 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m03:12:49.642680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '781d3df1-7cdb-4263-a9c7-2f316f87418e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11227d9a0>]}
[0m03:12:49.648980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '781d3df1-7cdb-4263-a9c7-2f316f87418e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e2a430>]}
[0m03:12:49.649182 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m03:12:49.649366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '781d3df1-7cdb-4263-a9c7-2f316f87418e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1122580d0>]}
[0m03:12:49.650471 [info ] [MainThread]: 
[0m03:12:49.650972 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:12:49.651740 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m03:12:49.663666 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m03:12:49.663845 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m03:12:49.663950 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:12:50.717571 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.05 seconds
[0m03:12:50.722929 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m03:12:51.235993 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m03:12:51.250640 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m03:12:51.250947 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m03:12:51.251182 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:12:52.141213 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 0.89 seconds
[0m03:12:52.146302 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m03:12:52.477606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '781d3df1-7cdb-4263-a9c7-2f316f87418e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112264f40>]}
[0m03:12:52.478918 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:12:52.479484 [info ] [MainThread]: 
[0m03:12:52.486690 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m03:12:52.487385 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m03:12:52.488786 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m03:12:52.489035 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m03:12:52.489509 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m03:12:52.509677 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m03:12:52.510941 [debug] [Thread-1  ]: finished collecting timing info
[0m03:12:52.511177 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m03:12:52.548056 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m03:12:52.550374 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:12:52.550505 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.customers__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    stg_customers = dbt.ref('stg_customers')
    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    customer_orders = (
        stg_orders
        .group_by("customer_id")
        .agg(
            F.min(F.col("order_date")).alias('first_order'),
            F.max(F.col("order_date")).alias('most_recent_order'),
            F.count(F.col("order_id")).alias('number_of_orders')
        )
    )

    customer_payments = (
        stg_payments
        .join(stg_orders, stg_payments.order_id == stg_orders.order_id, "left")
        .group_by(stg_orders.customer_id)
        .agg(
            F.sum(F.col("amount")).alias('total_amount')
        )
    )

    final_df = (
        stg_customers
        .join(customer_orders, stg_customers.customer_id == customer_orders.customer_id, "left")
        .join(customer_payments, stg_customers.customer_id == customer_payments.customer_id, "left")
        .select(customer_orders.customer_id,
                customer_orders.first_name,
                customer_orders.last_name,
                customer_orders.first_order,
                customer_orders.most_recent_order,
                customer_orders.number_of_orders,
                F.col("customer_payments.total_amount").alias("customer_lifetime_value")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle_shop.bruno.stg_customers", "stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle_shop.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.customers", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m03:12:52.550616 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:12:58.000185 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.45 seconds
[0m03:12:58.001895 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:12:58.002492 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CALL jaffle_shop.bruno.customers__dbt_sp();
[0m03:12:59.308665 [debug] [Thread-1  ]: Snowflake adapter: Snowflake query id: 01a9dae0-0000-0c55-0000-0000468d27a1
[0m03:12:59.308874 [debug] [Thread-1  ]: Snowflake adapter: Snowflake error: 100357 (P0000): Python Interpreter Error:
Traceback (most recent call last):
  File "_udf_code.py", line 122, in main
  File "_udf_code.py", line 37, in model
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 848, in __getattr__
    raise AttributeError(
AttributeError: DataFrame object has no attribute first_name
 in function CUSTOMERS__DBT_SP with handler main
[0m03:12:59.309166 [debug] [Thread-1  ]: finished collecting timing info
[0m03:12:59.309318 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m03:12:59.659279 [debug] [Thread-1  ]: Database Error in model customers (models/customers.py)
  100357 (P0000): Python Interpreter Error:
  Traceback (most recent call last):
    File "_udf_code.py", line 122, in main
    File "_udf_code.py", line 37, in model
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 848, in __getattr__
      raise AttributeError(
  AttributeError: DataFrame object has no attribute first_name
   in function CUSTOMERS__DBT_SP with handler main
  compiled Code at target/run/jaffle_shop/models/customers.py
[0m03:12:59.660283 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '781d3df1-7cdb-4263-a9c7-2f316f87418e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1134e0460>]}
[0m03:12:59.661196 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 7.17s]
[0m03:12:59.662252 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m03:12:59.665190 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:12:59.666215 [info ] [MainThread]: 
[0m03:12:59.666726 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 10.02 seconds (10.02s).
[0m03:12:59.667203 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:12:59.667445 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m03:12:59.682491 [info ] [MainThread]: 
[0m03:12:59.682969 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m03:12:59.683393 [info ] [MainThread]: 
[0m03:12:59.683701 [error] [MainThread]: [33mDatabase Error in model customers (models/customers.py)[0m
[0m03:12:59.683997 [error] [MainThread]:   100357 (P0000): Python Interpreter Error:
[0m03:12:59.684279 [error] [MainThread]:   Traceback (most recent call last):
[0m03:12:59.684561 [error] [MainThread]:     File "_udf_code.py", line 122, in main
[0m03:12:59.684844 [error] [MainThread]:     File "_udf_code.py", line 37, in model
[0m03:12:59.685126 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 848, in __getattr__
[0m03:12:59.685409 [error] [MainThread]:       raise AttributeError(
[0m03:12:59.685687 [error] [MainThread]:   AttributeError: DataFrame object has no attribute first_name
[0m03:12:59.685966 [error] [MainThread]:    in function CUSTOMERS__DBT_SP with handler main
[0m03:12:59.686244 [error] [MainThread]:   compiled Code at target/run/jaffle_shop/models/customers.py
[0m03:12:59.686545 [info ] [MainThread]: 
[0m03:12:59.686845 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m03:12:59.687286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112258070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d19130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1134e05e0>]}
[0m03:12:59.687668 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 03:13:31.783977 | 396ac1c1-6e2f-4b1c-bbb9-beb292b94b18 ==============================
[0m03:13:31.784026 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:13:31.784758 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m03:13:31.784908 [debug] [MainThread]: Tracking: tracking
[0m03:13:31.785243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111b39b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111b39be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111b39c40>]}
[0m03:13:31.836413 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m03:13:31.836836 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m03:13:31.891040 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '396ac1c1-6e2f-4b1c-bbb9-beb292b94b18', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120b6970>]}
[0m03:13:31.897486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '396ac1c1-6e2f-4b1c-bbb9-beb292b94b18', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111c63400>]}
[0m03:13:31.897680 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m03:13:31.897867 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '396ac1c1-6e2f-4b1c-bbb9-beb292b94b18', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120910a0>]}
[0m03:13:31.899212 [info ] [MainThread]: 
[0m03:13:31.899739 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:13:31.900584 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m03:13:31.912280 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m03:13:31.912459 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m03:13:31.912560 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:13:32.794390 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 0.88 seconds
[0m03:13:32.798750 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m03:13:33.217966 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m03:13:33.232456 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m03:13:33.232805 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m03:13:33.233029 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:13:34.029497 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 0.8 seconds
[0m03:13:34.035421 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m03:13:34.445229 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '396ac1c1-6e2f-4b1c-bbb9-beb292b94b18', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111b24820>]}
[0m03:13:34.446492 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:13:34.447037 [info ] [MainThread]: 
[0m03:13:34.454067 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m03:13:34.454761 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m03:13:34.455984 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m03:13:34.456263 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m03:13:34.457113 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m03:13:34.488430 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m03:13:34.489255 [debug] [Thread-1  ]: finished collecting timing info
[0m03:13:34.489475 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m03:13:34.522774 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m03:13:34.524967 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:13:34.525084 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.customers__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    stg_customers = dbt.ref('stg_customers')
    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    customer_orders = (
        stg_orders
        .group_by("customer_id")
        .agg(
            F.min(F.col("order_date")).alias('first_order'),
            F.max(F.col("order_date")).alias('most_recent_order'),
            F.count(F.col("order_id")).alias('number_of_orders')
        )
    )

    customer_payments = (
        stg_payments
        .join(stg_orders, stg_payments.order_id == stg_orders.order_id, "left")
        .group_by(stg_orders.customer_id)
        .agg(
            F.sum(F.col("amount")).alias('total_amount')
        )
    )

    final_df = (
        stg_customers
        .join(customer_orders, stg_customers.customer_id == customer_orders.customer_id, "left")
        .join(customer_payments, stg_customers.customer_id == customer_payments.customer_id, "left")
        .select(stg_customers.customer_id,
                stg_customers.first_name,
                stg_customers.last_name,
                customer_orders.first_order,
                customer_orders.most_recent_order,
                customer_orders.number_of_orders,
                F.col("customer_payments.total_amount").alias("customer_lifetime_value")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle_shop.bruno.stg_customers", "stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle_shop.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.customers", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m03:13:34.525191 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:13:39.927314 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.4 seconds
[0m03:13:39.928319 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:13:39.928670 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CALL jaffle_shop.bruno.customers__dbt_sp();
[0m03:13:41.299243 [debug] [Thread-1  ]: Snowflake adapter: Snowflake query id: 01a9dae1-0000-0c56-0000-0000468d3775
[0m03:13:41.299444 [debug] [Thread-1  ]: Snowflake adapter: Snowflake error: 100357 (P0000): Python Interpreter Error:
Traceback (most recent call last):
  File "_udf_code.py", line 123, in main
  File "_udf_code.py", line 118, in materialize
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/telemetry.py", line 161, in wrap
    result = func(*args, **kwargs)
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe_writer.py", line 193, in save_as_table
    result = session._conn.execute(
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py", line 406, in execute
    result_set, result_meta = self.get_result_set(
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py", line 149, in wrap
    raise ne.with_traceback(tb) from None
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py", line 87, in wrap
    return func(*args, **kwargs)
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py", line 494, in get_result_set
    result = self.run_query(
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py", line 104, in wrap
    raise ex
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py", line 98, in wrap
    return func(*args, **kwargs)
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py", line 333, in run_query
    raise ex
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py", line 317, in run_query
    results_cursor = self._cursor.execute(query, **kwargs)
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/connector/cursor.py", line 822, in execute
    Error.errorhandler_wrapper(
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/connector/errors.py", line 229, in errorhandler_wrapper
    handed_over = Error.hand_to_other_handler(
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/connector/errors.py", line 284, in hand_to_other_handler
    cursor.errorhandler(connection, cursor, error_class, error_value)
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/connector/errors.py", line 163, in default_errorhandler
    raise error_class(
snowflake.snowpark.exceptions.SnowparkSQLException: (1304): 01a9dae1-0000-0c56-0000-0000468d37a9: 000904 (42000): SQL compilation error: error line 1 at position 192
invalid identifier '"customer_payments.total_amount"'
 in function CUSTOMERS__DBT_SP with handler main
[0m03:13:41.299713 [debug] [Thread-1  ]: finished collecting timing info
[0m03:13:41.299845 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m03:13:41.645327 [debug] [Thread-1  ]: Database Error in model customers (models/customers.py)
  100357 (P0000): Python Interpreter Error:
  Traceback (most recent call last):
    File "_udf_code.py", line 123, in main
    File "_udf_code.py", line 118, in materialize
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/telemetry.py", line 161, in wrap
      result = func(*args, **kwargs)
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe_writer.py", line 193, in save_as_table
      result = session._conn.execute(
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py", line 406, in execute
      result_set, result_meta = self.get_result_set(
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py", line 149, in wrap
      raise ne.with_traceback(tb) from None
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py", line 87, in wrap
      return func(*args, **kwargs)
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py", line 494, in get_result_set
      result = self.run_query(
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py", line 104, in wrap
      raise ex
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py", line 98, in wrap
      return func(*args, **kwargs)
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py", line 333, in run_query
      raise ex
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py", line 317, in run_query
      results_cursor = self._cursor.execute(query, **kwargs)
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/connector/cursor.py", line 822, in execute
      Error.errorhandler_wrapper(
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/connector/errors.py", line 229, in errorhandler_wrapper
      handed_over = Error.hand_to_other_handler(
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/connector/errors.py", line 284, in hand_to_other_handler
      cursor.errorhandler(connection, cursor, error_class, error_value)
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/connector/errors.py", line 163, in default_errorhandler
      raise error_class(
  snowflake.snowpark.exceptions.SnowparkSQLException: (1304): 01a9dae1-0000-0c56-0000-0000468d37a9: 000904 (42000): SQL compilation error: error line 1 at position 192
  invalid identifier '"customer_payments.total_amount"'
   in function CUSTOMERS__DBT_SP with handler main
  compiled Code at target/run/jaffle_shop/models/customers.py
[0m03:13:41.646316 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '396ac1c1-6e2f-4b1c-bbb9-beb292b94b18', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113320280>]}
[0m03:13:41.647037 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 7.19s]
[0m03:13:41.648067 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m03:13:41.650972 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:13:41.652072 [info ] [MainThread]: 
[0m03:13:41.652629 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 9.75 seconds (9.75s).
[0m03:13:41.653135 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:13:41.653385 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m03:13:41.668302 [info ] [MainThread]: 
[0m03:13:41.668714 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m03:13:41.669092 [info ] [MainThread]: 
[0m03:13:41.669409 [error] [MainThread]: [33mDatabase Error in model customers (models/customers.py)[0m
[0m03:13:41.669720 [error] [MainThread]:   100357 (P0000): Python Interpreter Error:
[0m03:13:41.670017 [error] [MainThread]:   Traceback (most recent call last):
[0m03:13:41.670304 [error] [MainThread]:     File "_udf_code.py", line 123, in main
[0m03:13:41.670593 [error] [MainThread]:     File "_udf_code.py", line 118, in materialize
[0m03:13:41.670879 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/telemetry.py", line 161, in wrap
[0m03:13:41.671165 [error] [MainThread]:       result = func(*args, **kwargs)
[0m03:13:41.671538 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe_writer.py", line 193, in save_as_table
[0m03:13:41.671879 [error] [MainThread]:       result = session._conn.execute(
[0m03:13:41.672183 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py", line 406, in execute
[0m03:13:41.672479 [error] [MainThread]:       result_set, result_meta = self.get_result_set(
[0m03:13:41.672771 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py", line 149, in wrap
[0m03:13:41.673069 [error] [MainThread]:       raise ne.with_traceback(tb) from None
[0m03:13:41.673368 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py", line 87, in wrap
[0m03:13:41.673662 [error] [MainThread]:       return func(*args, **kwargs)
[0m03:13:41.673948 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py", line 494, in get_result_set
[0m03:13:41.674234 [error] [MainThread]:       result = self.run_query(
[0m03:13:41.674524 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py", line 104, in wrap
[0m03:13:41.674808 [error] [MainThread]:       raise ex
[0m03:13:41.675091 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py", line 98, in wrap
[0m03:13:41.675373 [error] [MainThread]:       return func(*args, **kwargs)
[0m03:13:41.675655 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py", line 333, in run_query
[0m03:13:41.675936 [error] [MainThread]:       raise ex
[0m03:13:41.676218 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py", line 317, in run_query
[0m03:13:41.676501 [error] [MainThread]:       results_cursor = self._cursor.execute(query, **kwargs)
[0m03:13:41.676782 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/connector/cursor.py", line 822, in execute
[0m03:13:41.677065 [error] [MainThread]:       Error.errorhandler_wrapper(
[0m03:13:41.677347 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/connector/errors.py", line 229, in errorhandler_wrapper
[0m03:13:41.677629 [error] [MainThread]:       handed_over = Error.hand_to_other_handler(
[0m03:13:41.677881 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/connector/errors.py", line 284, in hand_to_other_handler
[0m03:13:41.678132 [error] [MainThread]:       cursor.errorhandler(connection, cursor, error_class, error_value)
[0m03:13:41.678383 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/connector/errors.py", line 163, in default_errorhandler
[0m03:13:41.678633 [error] [MainThread]:       raise error_class(
[0m03:13:41.678910 [error] [MainThread]:   snowflake.snowpark.exceptions.SnowparkSQLException: (1304): 01a9dae1-0000-0c56-0000-0000468d37a9: 000904 (42000): SQL compilation error: error line 1 at position 192
[0m03:13:41.679157 [error] [MainThread]:   invalid identifier '"customer_payments.total_amount"'
[0m03:13:41.679406 [error] [MainThread]:    in function CUSTOMERS__DBT_SP with handler main
[0m03:13:41.679758 [error] [MainThread]:   compiled Code at target/run/jaffle_shop/models/customers.py
[0m03:13:41.680033 [info ] [MainThread]: 
[0m03:13:41.680309 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m03:13:41.680702 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120da4c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111c1e310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11337e490>]}
[0m03:13:41.681007 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 03:13:55.175242 | 1731049e-aeb6-458d-914c-a087ad974d10 ==============================
[0m03:13:55.175315 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:13:55.176220 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m03:13:55.176359 [debug] [MainThread]: Tracking: tracking
[0m03:13:55.176700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11294bbe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11294bc40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11294bca0>]}
[0m03:13:55.228245 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m03:13:55.228639 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m03:13:55.291465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1731049e-aeb6-458d-914c-a087ad974d10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112ec7f40>]}
[0m03:13:55.298067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1731049e-aeb6-458d-914c-a087ad974d10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a75490>]}
[0m03:13:55.298311 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m03:13:55.298519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1731049e-aeb6-458d-914c-a087ad974d10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112ea3100>]}
[0m03:13:55.299689 [info ] [MainThread]: 
[0m03:13:55.300153 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:13:55.300845 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m03:13:55.311596 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m03:13:55.311726 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m03:13:55.311822 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:13:56.658903 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.35 seconds
[0m03:13:56.663207 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m03:13:57.039979 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m03:13:57.050260 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m03:13:57.050527 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m03:13:57.050690 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:13:57.623282 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 0.57 seconds
[0m03:13:57.625036 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m03:13:58.091978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1731049e-aeb6-458d-914c-a087ad974d10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129644f0>]}
[0m03:13:58.093003 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:13:58.093373 [info ] [MainThread]: 
[0m03:13:58.097333 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m03:13:58.097732 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m03:13:58.098547 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m03:13:58.098732 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m03:13:58.099344 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m03:13:58.122258 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m03:13:58.123490 [debug] [Thread-1  ]: finished collecting timing info
[0m03:13:58.123666 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m03:13:58.152577 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m03:13:58.154903 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:13:58.155023 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.customers__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    stg_customers = dbt.ref('stg_customers')
    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    customer_orders = (
        stg_orders
        .group_by("customer_id")
        .agg(
            F.min(F.col("order_date")).alias('first_order'),
            F.max(F.col("order_date")).alias('most_recent_order'),
            F.count(F.col("order_id")).alias('number_of_orders')
        )
    )

    customer_payments = (
        stg_payments
        .join(stg_orders, stg_payments.order_id == stg_orders.order_id, "left")
        .group_by(stg_orders.customer_id)
        .agg(
            F.sum(F.col("amount")).alias('total_amount')
        )
    )

    final_df = (
        stg_customers
        .join(customer_orders, stg_customers.customer_id == customer_orders.customer_id, "left")
        .join(customer_payments, stg_customers.customer_id == customer_payments.customer_id, "left")
        .select(stg_customers.customer_id,
                stg_customers.first_name,
                stg_customers.last_name,
                customer_orders.first_order,
                customer_orders.most_recent_order,
                customer_orders.number_of_orders,
                F.col(customer_payments.total_amount).alias("customer_lifetime_value")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle_shop.bruno.stg_customers", "stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle_shop.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.customers", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m03:13:58.155119 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:14:03.537532 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.38 seconds
[0m03:14:03.538090 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:14:03.538373 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CALL jaffle_shop.bruno.customers__dbt_sp();
[0m03:14:04.914208 [debug] [Thread-1  ]: Snowflake adapter: Snowflake query id: 01a9dae2-0000-0c55-0000-0000468d27d9
[0m03:14:04.914432 [debug] [Thread-1  ]: Snowflake adapter: Snowflake error: 100357 (P0000): Python Interpreter Error:
Traceback (most recent call last):
  File "_udf_code.py", line 122, in main
  File "_udf_code.py", line 42, in model
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/functions.py", line 207, in col
    return Column(col_name)
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/column.py", line 222, in __init__
    raise TypeError("Column constructor only accepts str or expression.")
TypeError: Column constructor only accepts str or expression.
 in function CUSTOMERS__DBT_SP with handler main
[0m03:14:04.914727 [debug] [Thread-1  ]: finished collecting timing info
[0m03:14:04.914884 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m03:14:05.368391 [debug] [Thread-1  ]: Database Error in model customers (models/customers.py)
  100357 (P0000): Python Interpreter Error:
  Traceback (most recent call last):
    File "_udf_code.py", line 122, in main
    File "_udf_code.py", line 42, in model
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/functions.py", line 207, in col
      return Column(col_name)
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/column.py", line 222, in __init__
      raise TypeError("Column constructor only accepts str or expression.")
  TypeError: Column constructor only accepts str or expression.
   in function CUSTOMERS__DBT_SP with handler main
  compiled Code at target/run/jaffle_shop/models/customers.py
[0m03:14:05.369322 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1731049e-aeb6-458d-914c-a087ad974d10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11412af70>]}
[0m03:14:05.370103 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 7.27s]
[0m03:14:05.371117 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m03:14:05.374021 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:14:05.375115 [info ] [MainThread]: 
[0m03:14:05.375665 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 10.08 seconds (10.08s).
[0m03:14:05.376161 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:14:05.376405 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m03:14:05.391551 [info ] [MainThread]: 
[0m03:14:05.391994 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m03:14:05.392378 [info ] [MainThread]: 
[0m03:14:05.392693 [error] [MainThread]: [33mDatabase Error in model customers (models/customers.py)[0m
[0m03:14:05.392989 [error] [MainThread]:   100357 (P0000): Python Interpreter Error:
[0m03:14:05.393272 [error] [MainThread]:   Traceback (most recent call last):
[0m03:14:05.393550 [error] [MainThread]:     File "_udf_code.py", line 122, in main
[0m03:14:05.393829 [error] [MainThread]:     File "_udf_code.py", line 42, in model
[0m03:14:05.394104 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/functions.py", line 207, in col
[0m03:14:05.394381 [error] [MainThread]:       return Column(col_name)
[0m03:14:05.394659 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/column.py", line 222, in __init__
[0m03:14:05.394933 [error] [MainThread]:       raise TypeError("Column constructor only accepts str or expression.")
[0m03:14:05.395211 [error] [MainThread]:   TypeError: Column constructor only accepts str or expression.
[0m03:14:05.395494 [error] [MainThread]:    in function CUSTOMERS__DBT_SP with handler main
[0m03:14:05.395768 [error] [MainThread]:   compiled Code at target/run/jaffle_shop/models/customers.py
[0m03:14:05.396052 [info ] [MainThread]: 
[0m03:14:05.396347 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m03:14:05.396750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112935880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a75160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114130070>]}
[0m03:14:05.397073 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 03:14:17.444023 | 21bf5df4-9aa9-4357-b751-295f69cd5610 ==============================
[0m03:14:17.444108 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:14:17.445203 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m03:14:17.445362 [debug] [MainThread]: Tracking: tracking
[0m03:14:17.445745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110a45b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110a45be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110a45c40>]}
[0m03:14:17.495609 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m03:14:17.496028 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m03:14:17.549122 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '21bf5df4-9aa9-4357-b751-295f69cd5610', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110fce970>]}
[0m03:14:17.555590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '21bf5df4-9aa9-4357-b751-295f69cd5610', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110b6f400>]}
[0m03:14:17.555789 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m03:14:17.555972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '21bf5df4-9aa9-4357-b751-295f69cd5610', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110f9c0a0>]}
[0m03:14:17.557136 [info ] [MainThread]: 
[0m03:14:17.557585 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:14:17.558333 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m03:14:17.570485 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m03:14:17.570682 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m03:14:17.570779 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:14:18.375275 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 0.8 seconds
[0m03:14:18.381306 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m03:14:18.780719 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m03:14:18.793994 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m03:14:18.794246 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m03:14:18.794434 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:14:19.486811 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 0.69 seconds
[0m03:14:19.491087 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m03:14:19.823215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '21bf5df4-9aa9-4357-b751-295f69cd5610', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110a2f820>]}
[0m03:14:19.824639 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:14:19.825221 [info ] [MainThread]: 
[0m03:14:19.832159 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m03:14:19.832769 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m03:14:19.834065 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m03:14:19.834346 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m03:14:19.835327 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m03:14:19.865512 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m03:14:19.866074 [debug] [Thread-1  ]: finished collecting timing info
[0m03:14:19.866262 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m03:14:19.899430 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m03:14:19.901747 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:14:19.901942 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.customers__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    stg_customers = dbt.ref('stg_customers')
    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    customer_orders = (
        stg_orders
        .group_by("customer_id")
        .agg(
            F.min(F.col("order_date")).alias('first_order'),
            F.max(F.col("order_date")).alias('most_recent_order'),
            F.count(F.col("order_id")).alias('number_of_orders')
        )
    )

    customer_payments = (
        stg_payments
        .join(stg_orders, stg_payments.order_id == stg_orders.order_id, "left")
        .group_by(stg_orders.customer_id)
        .agg(
            F.sum(F.col("amount")).alias('total_amount')
        )
    )

    final_df = (
        stg_customers
        .join(customer_orders, stg_customers.customer_id == customer_orders.customer_id, "left")
        .join(customer_payments, stg_customers.customer_id == customer_payments.customer_id, "left")
        .select(stg_customers.customer_id,
                stg_customers.first_name,
                stg_customers.last_name,
                customer_orders.first_order,
                customer_orders.most_recent_order,
                customer_orders.number_of_orders,
                customer_payments.total_amount.alias("customer_lifetime_value")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle_shop.bruno.stg_customers", "stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle_shop.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.customers", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m03:14:19.902067 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:14:25.537672 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.64 seconds
[0m03:14:25.539034 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:14:25.539560 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CALL jaffle_shop.bruno.customers__dbt_sp();
[0m03:14:27.888633 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 2.35 seconds
[0m03:14:27.891134 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:14:27.891361 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
drop procedure if exists jaffle_shop.bruno.customers__dbt_sp()
[0m03:14:28.096556 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.2 seconds
[0m03:14:28.139985 [debug] [Thread-1  ]: finished collecting timing info
[0m03:14:28.140386 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m03:14:28.561601 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '21bf5df4-9aa9-4357-b751-295f69cd5610', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1126a0760>]}
[0m03:14:28.562558 [info ] [Thread-1  ]: 1 of 1 OK created python table model bruno.customers ........................... [[32mSUCCESS 1[0m in 8.73s]
[0m03:14:28.563646 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m03:14:28.566473 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:14:28.567489 [info ] [MainThread]: 
[0m03:14:28.567992 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 11.01 seconds (11.01s).
[0m03:14:28.568462 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:14:28.568702 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m03:14:28.584345 [info ] [MainThread]: 
[0m03:14:28.584854 [info ] [MainThread]: [32mCompleted successfully[0m
[0m03:14:28.585266 [info ] [MainThread]: 
[0m03:14:28.585591 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m03:14:28.586062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110b6f2e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110b2d490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11263ee20>]}
[0m03:14:28.586447 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 03:17:40.403450 | e045ab28-3eeb-4871-86e2-5b6dca794a4f ==============================
[0m03:17:40.403509 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:17:40.404349 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m03:17:40.404505 [debug] [MainThread]: Tracking: tracking
[0m03:17:40.404846 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112114b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112114be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112114c40>]}
[0m03:17:40.460391 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m03:17:40.460796 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m03:17:40.515609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e045ab28-3eeb-4871-86e2-5b6dca794a4f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112696970>]}
[0m03:17:40.522247 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e045ab28-3eeb-4871-86e2-5b6dca794a4f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11223a400>]}
[0m03:17:40.522444 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m03:17:40.522627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e045ab28-3eeb-4871-86e2-5b6dca794a4f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1126680a0>]}
[0m03:17:40.523759 [info ] [MainThread]: 
[0m03:17:40.524224 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:17:40.524968 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m03:17:40.536866 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m03:17:40.537060 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m03:17:40.537157 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:17:41.779248 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.24 seconds
[0m03:17:41.783879 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m03:17:42.145444 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m03:17:42.159809 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m03:17:42.160100 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m03:17:42.160322 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:17:42.750092 [debug] [ThreadPool]: SQL status: SUCCESS 7 in 0.59 seconds
[0m03:17:42.755082 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m03:17:43.093164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e045ab28-3eeb-4871-86e2-5b6dca794a4f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120fb820>]}
[0m03:17:43.094707 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:17:43.095264 [info ] [MainThread]: 
[0m03:17:43.101730 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m03:17:43.102304 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m03:17:43.103427 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m03:17:43.103687 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m03:17:43.104512 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m03:17:43.132803 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m03:17:43.134039 [debug] [Thread-1  ]: finished collecting timing info
[0m03:17:43.134234 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m03:17:43.165596 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m03:17:43.167833 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:17:43.167949 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.customers__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    stg_customers = dbt.ref('stg_customers')
    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    customer_orders = (
        stg_orders
        .group_by("customer_id")
        .agg(
            F.min(F.col("order_date")).alias('first_order'),
            F.max(F.col("order_date")).alias('most_recent_order'),
            F.count(F.col("order_id")).alias('number_of_orders')
        )
    )

    customer_payments = (
        stg_payments
        .join(stg_orders, stg_payments.order_id == stg_orders.order_id, "left")
        .group_by(stg_orders.customer_id)
        .agg(
            F.sum(F.col("amount")).alias('total_amount')
        )
    )

    final_df = (
        stg_customers
        .join(customer_orders, stg_customers.customer_id == customer_orders.customer_id, "left")
        .join(customer_payments, stg_customers.customer_id == customer_payments.customer_id, "left")
        .select(stg_customers.customer_id.alias("customer_id"),
                stg_customers.first_name.alias("first_name"),
                stg_customers.last_name.alias("last_name"),
                customer_orders.first_order.alias("first_order"),
                customer_orders.most_recent_order.alias("most_recent_order"),
                customer_orders.number_of_orders.alias("number_of_orders"),
                customer_payments.total_amount.alias("customer_lifetime_value")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle_shop.bruno.stg_customers", "stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle_shop.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.customers", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m03:17:43.168052 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:17:48.706518 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.54 seconds
[0m03:17:48.707566 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:17:48.708215 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CALL jaffle_shop.bruno.customers__dbt_sp();
[0m03:17:50.918784 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 2.21 seconds
[0m03:17:50.922930 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:17:50.923463 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
drop procedure if exists jaffle_shop.bruno.customers__dbt_sp()
[0m03:17:51.104636 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.18 seconds
[0m03:17:51.151654 [debug] [Thread-1  ]: finished collecting timing info
[0m03:17:51.151951 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m03:17:51.496279 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e045ab28-3eeb-4871-86e2-5b6dca794a4f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1138f69a0>]}
[0m03:17:51.496974 [info ] [Thread-1  ]: 1 of 1 OK created python table model bruno.customers ........................... [[32mSUCCESS 1[0m in 8.39s]
[0m03:17:51.497857 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m03:17:51.500426 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:17:51.501268 [info ] [MainThread]: 
[0m03:17:51.501682 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 10.98 seconds (10.98s).
[0m03:17:51.502071 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:17:51.502261 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m03:17:51.516219 [info ] [MainThread]: 
[0m03:17:51.516624 [info ] [MainThread]: [32mCompleted successfully[0m
[0m03:17:51.516964 [info ] [MainThread]: 
[0m03:17:51.517234 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m03:17:51.517616 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120f3190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1127425b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11394aeb0>]}
[0m03:17:51.517939 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 03:23:37.919535 | 3634acb7-181b-48bd-b2ab-cc77405806f7 ==============================
[0m03:23:37.919600 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:23:37.920583 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['orders.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m03:23:37.920727 [debug] [MainThread]: Tracking: tracking
[0m03:23:37.921083 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e68bbb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e68bc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e68bc70>]}
[0m03:23:37.976249 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m03:23:37.976639 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/orders.py
[0m03:23:38.014065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3634acb7-181b-48bd-b2ab-cc77405806f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ee440a0>]}
[0m03:23:38.020300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3634acb7-181b-48bd-b2ab-cc77405806f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e6757f0>]}
[0m03:23:38.020500 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m03:23:38.020678 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3634acb7-181b-48bd-b2ab-cc77405806f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ee1c1c0>]}
[0m03:23:38.021823 [info ] [MainThread]: 
[0m03:23:38.022270 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:23:38.023016 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m03:23:38.035254 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m03:23:38.035438 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m03:23:38.035535 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:23:39.093258 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.06 seconds
[0m03:23:39.097719 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m03:23:39.449939 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m03:23:39.460120 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m03:23:39.460332 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m03:23:39.460500 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:23:40.142234 [debug] [ThreadPool]: SQL status: SUCCESS 7 in 0.68 seconds
[0m03:23:40.147874 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m03:23:40.484718 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3634acb7-181b-48bd-b2ab-cc77405806f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eef5730>]}
[0m03:23:40.486355 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:23:40.486921 [info ] [MainThread]: 
[0m03:23:40.494200 [debug] [Thread-1  ]: Began running node model.jaffle_shop.orders
[0m03:23:40.494884 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.orders ................................... [RUN]
[0m03:23:40.496108 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.orders"
[0m03:23:40.496376 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.orders
[0m03:23:40.497309 [debug] [Thread-1  ]: Compiling model.jaffle_shop.orders
[0m03:23:40.527525 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.orders"
[0m03:23:40.528707 [debug] [Thread-1  ]: finished collecting timing info
[0m03:23:40.528920 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.orders
[0m03:23:40.562850 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.orders"
[0m03:23:40.564959 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.orders"
[0m03:23:40.565074 [debug] [Thread-1  ]: On model.jaffle_shop.orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.orders"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.orders__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    payment_methods = ['credit_card', 'coupon', 'bank_transfer', 'gift_card']

    agg_list = [F.sum(F.when(stg_payments.payment_method == payment_method, stg_payments.amount).otherwise(0)).alias(payment_method + "_amount") for payment_method in payment_methods]

    agg_list.append(F.sum(F.col("amount")).alias("total_amount"))

    order_payments = (
        stg_payments
        .groupby("order_id")
        .agg(*agg_list)
    )

    final_df = (
        stg_orders
        .join(order_payments, stg_orders.order_id == order_payments.order_id, "left")
        .select(stg_orders.order_id.alias("order_id"),
                stg_orders.customer_id.alias("customer_id"),
                stg_orders.order_date.alias("order_date"),
                stg_orders.status.alias("status"),
                *[F.col(payment_method + "_amount") for payment_method in payment_methods],
                order_payments.total_amount.alias("amount")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'orders'
    def __repr__(self):
        return 'jaffle_shop.bruno.orders'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.orders", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m03:23:40.565181 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:23:46.285572 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.72 seconds
[0m03:23:46.286825 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.orders"
[0m03:23:46.287145 [debug] [Thread-1  ]: On model.jaffle_shop.orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.orders"} */
CALL jaffle_shop.bruno.orders__dbt_sp();
[0m03:23:47.240306 [debug] [Thread-1  ]: Snowflake adapter: Snowflake query id: 01a9daeb-0000-0c55-0000-0000468d2899
[0m03:23:47.240526 [debug] [Thread-1  ]: Snowflake adapter: Snowflake error: 100357 (P0000): Python Interpreter Error:
Traceback (most recent call last):
  File "_udf_code.py", line 112, in main
  File "_udf_code.py", line 19, in model
  File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 848, in __getattr__
    raise AttributeError(
AttributeError: Table object has no attribute groupby
 in function ORDERS__DBT_SP with handler main
[0m03:23:47.240824 [debug] [Thread-1  ]: finished collecting timing info
[0m03:23:47.240977 [debug] [Thread-1  ]: On model.jaffle_shop.orders: Close
[0m03:23:47.620927 [debug] [Thread-1  ]: Database Error in model orders (models/orders.py)
  100357 (P0000): Python Interpreter Error:
  Traceback (most recent call last):
    File "_udf_code.py", line 112, in main
    File "_udf_code.py", line 19, in model
    File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 848, in __getattr__
      raise AttributeError(
  AttributeError: Table object has no attribute groupby
   in function ORDERS__DBT_SP with handler main
  compiled Code at target/run/jaffle_shop/models/orders.py
[0m03:23:47.621780 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3634acb7-181b-48bd-b2ab-cc77405806f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fe63d30>]}
[0m03:23:47.622483 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.orders .......................... [[31mERROR[0m in 7.13s]
[0m03:23:47.623378 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.orders
[0m03:23:47.626567 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:23:47.627614 [info ] [MainThread]: 
[0m03:23:47.628141 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 9.61 seconds (9.61s).
[0m03:23:47.628615 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:23:47.628856 [debug] [MainThread]: Connection 'model.jaffle_shop.orders' was properly closed.
[0m03:23:47.643789 [info ] [MainThread]: 
[0m03:23:47.644237 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m03:23:47.644619 [info ] [MainThread]: 
[0m03:23:47.644934 [error] [MainThread]: [33mDatabase Error in model orders (models/orders.py)[0m
[0m03:23:47.645233 [error] [MainThread]:   100357 (P0000): Python Interpreter Error:
[0m03:23:47.645511 [error] [MainThread]:   Traceback (most recent call last):
[0m03:23:47.645789 [error] [MainThread]:     File "_udf_code.py", line 112, in main
[0m03:23:47.646064 [error] [MainThread]:     File "_udf_code.py", line 19, in model
[0m03:23:47.646339 [error] [MainThread]:     File "/usr/lib/python_udf/6cd8c5edb748108566a6331991a1dbb5d219039edc8195c97d7681b3bda325d2/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py", line 848, in __getattr__
[0m03:23:47.646620 [error] [MainThread]:       raise AttributeError(
[0m03:23:47.646896 [error] [MainThread]:   AttributeError: Table object has no attribute groupby
[0m03:23:47.647172 [error] [MainThread]:    in function ORDERS__DBT_SP with handler main
[0m03:23:47.647447 [error] [MainThread]:   compiled Code at target/run/jaffle_shop/models/orders.py
[0m03:23:47.647745 [info ] [MainThread]: 
[0m03:23:47.648057 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m03:23:47.648509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e6c7760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fe52610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fe63f40>]}
[0m03:23:47.648851 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 03:24:05.914571 | 84a5a658-43b4-4bf7-993b-8f20cc8e3e7c ==============================
[0m03:24:05.914659 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:24:05.915709 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['orders.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m03:24:05.915868 [debug] [MainThread]: Tracking: tracking
[0m03:24:05.916268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fcfdbb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fcfdc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fcfdc70>]}
[0m03:24:05.970412 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m03:24:05.970782 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/orders.py
[0m03:24:06.007217 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '84a5a658-43b4-4bf7-993b-8f20cc8e3e7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11027a7c0>]}
[0m03:24:06.013474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '84a5a658-43b4-4bf7-993b-8f20cc8e3e7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fe28430>]}
[0m03:24:06.013669 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m03:24:06.013855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '84a5a658-43b4-4bf7-993b-8f20cc8e3e7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1102561c0>]}
[0m03:24:06.014975 [info ] [MainThread]: 
[0m03:24:06.015446 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:24:06.016203 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m03:24:06.028017 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m03:24:06.028232 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m03:24:06.028343 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:24:07.066660 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.04 seconds
[0m03:24:07.071708 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m03:24:07.489835 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m03:24:07.503750 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m03:24:07.504037 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m03:24:07.504269 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:24:08.295414 [debug] [ThreadPool]: SQL status: SUCCESS 7 in 0.79 seconds
[0m03:24:08.301134 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m03:24:08.630491 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '84a5a658-43b4-4bf7-993b-8f20cc8e3e7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1102eb220>]}
[0m03:24:08.631957 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:24:08.632547 [info ] [MainThread]: 
[0m03:24:08.639572 [debug] [Thread-1  ]: Began running node model.jaffle_shop.orders
[0m03:24:08.640079 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.orders ................................... [RUN]
[0m03:24:08.641157 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.orders"
[0m03:24:08.641410 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.orders
[0m03:24:08.642226 [debug] [Thread-1  ]: Compiling model.jaffle_shop.orders
[0m03:24:08.671765 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.orders"
[0m03:24:08.672374 [debug] [Thread-1  ]: finished collecting timing info
[0m03:24:08.672556 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.orders
[0m03:24:08.705980 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.orders"
[0m03:24:08.708023 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.orders"
[0m03:24:08.708137 [debug] [Thread-1  ]: On model.jaffle_shop.orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.orders"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.orders__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    payment_methods = ['credit_card', 'coupon', 'bank_transfer', 'gift_card']

    agg_list = [F.sum(F.when(stg_payments.payment_method == payment_method, stg_payments.amount).otherwise(0)).alias(payment_method + "_amount") for payment_method in payment_methods]

    agg_list.append(F.sum(F.col("amount")).alias("total_amount"))

    order_payments = (
        stg_payments
        .group_by("order_id")
        .agg(*agg_list)
    )

    final_df = (
        stg_orders
        .join(order_payments, stg_orders.order_id == order_payments.order_id, "left")
        .select(stg_orders.order_id.alias("order_id"),
                stg_orders.customer_id.alias("customer_id"),
                stg_orders.order_date.alias("order_date"),
                stg_orders.status.alias("status"),
                *[F.col(payment_method + "_amount") for payment_method in payment_methods],
                order_payments.total_amount.alias("amount")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'orders'
    def __repr__(self):
        return 'jaffle_shop.bruno.orders'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.orders", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m03:24:08.708238 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:24:14.159124 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.45 seconds
[0m03:24:14.160350 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.orders"
[0m03:24:14.160805 [debug] [Thread-1  ]: On model.jaffle_shop.orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.orders"} */
CALL jaffle_shop.bruno.orders__dbt_sp();
[0m03:24:15.981672 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 1.82 seconds
[0m03:24:15.985143 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.orders"
[0m03:24:15.985948 [debug] [Thread-1  ]: On model.jaffle_shop.orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.orders"} */
drop procedure if exists jaffle_shop.bruno.orders__dbt_sp()
[0m03:24:16.286429 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.3 seconds
[0m03:24:16.325674 [debug] [Thread-1  ]: finished collecting timing info
[0m03:24:16.325964 [debug] [Thread-1  ]: On model.jaffle_shop.orders: Close
[0m03:24:16.783429 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '84a5a658-43b4-4bf7-993b-8f20cc8e3e7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111522160>]}
[0m03:24:16.784365 [info ] [Thread-1  ]: 1 of 1 OK created python table model bruno.orders .............................. [[32mSUCCESS 1[0m in 8.14s]
[0m03:24:16.785326 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.orders
[0m03:24:16.788053 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:24:16.789114 [info ] [MainThread]: 
[0m03:24:16.789638 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 10.77 seconds (10.77s).
[0m03:24:16.790114 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:24:16.790360 [debug] [MainThread]: Connection 'model.jaffle_shop.orders' was properly closed.
[0m03:24:16.805570 [info ] [MainThread]: 
[0m03:24:16.806048 [info ] [MainThread]: [32mCompleted successfully[0m
[0m03:24:16.806431 [info ] [MainThread]: 
[0m03:24:16.806749 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m03:24:16.807165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd39760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110262f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114d0d00>]}
[0m03:24:16.807582 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 03:36:49.447428 | aef5d30e-1a6a-4c81-bdfe-668c4d18ddc6 ==============================
[0m03:36:49.447493 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:36:49.448508 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'indirect_selection': 'eager', 'resource_types': [], 'which': 'build', 'rpc_method': 'build'}
[0m03:36:49.448667 [debug] [MainThread]: Tracking: tracking
[0m03:36:49.449120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e59db50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e59dbb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e59dc10>]}
[0m03:36:49.502971 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m03:36:49.503175 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m03:36:49.509377 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'aef5d30e-1a6a-4c81-bdfe-668c4d18ddc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eb36e50>]}
[0m03:36:49.516945 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'aef5d30e-1a6a-4c81-bdfe-668c4d18ddc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e684430>]}
[0m03:36:49.517181 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m03:36:49.517366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aef5d30e-1a6a-4c81-bdfe-668c4d18ddc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e41d160>]}
[0m03:36:49.518900 [info ] [MainThread]: 
[0m03:36:49.519405 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:36:49.520367 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m03:36:49.531587 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m03:36:49.531716 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m03:36:49.531808 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:36:50.663524 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.13 seconds
[0m03:36:50.665697 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m03:36:51.014843 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m03:36:51.023083 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m03:36:51.023491 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m03:36:51.023598 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:36:51.765272 [debug] [ThreadPool]: SQL status: SUCCESS 8 in 0.74 seconds
[0m03:36:51.768855 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m03:36:52.115388 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aef5d30e-1a6a-4c81-bdfe-668c4d18ddc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e684340>]}
[0m03:36:52.116575 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:36:52.117028 [info ] [MainThread]: 
[0m03:36:52.123119 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_customers
[0m03:36:52.123509 [info ] [Thread-1  ]: 1 of 18 START seed file bruno.raw_customers .................................... [RUN]
[0m03:36:52.124883 [debug] [Thread-1  ]: Acquiring new snowflake connection "seed.jaffle_shop.raw_customers"
[0m03:36:52.125095 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_customers
[0m03:36:52.125325 [debug] [Thread-1  ]: finished collecting timing info
[0m03:36:52.125527 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_customers
[0m03:36:52.175432 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m03:36:52.175623 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
BEGIN
[0m03:36:52.175733 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:36:52.821295 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.65 seconds
[0m03:36:52.822444 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m03:36:52.822985 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
truncate table "JAFFLE_SHOP"."BRUNO"."RAW_CUSTOMERS"
  ;
[0m03:36:53.332190 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.51 seconds
[0m03:36:53.333340 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m03:36:53.333736 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
COMMIT
[0m03:36:53.718295 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.38 seconds
[0m03:36:53.755510 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m03:36:53.755824 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
BEGIN
[0m03:36:53.946578 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.19 seconds
[0m03:36:53.953644 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m03:36:53.954123 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
insert into jaffle_shop.bruno.raw_customers (id, first_name, last_name) values
            (%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s...
[0m03:36:55.180783 [debug] [Thread-1  ]: SQL status: SUCCESS 100 in 1.23 seconds
[0m03:36:55.182121 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m03:36:55.182590 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
COMMIT
[0m03:36:55.462025 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.28 seconds
[0m03:36:55.479137 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_customers"
[0m03:36:55.513283 [debug] [Thread-1  ]: finished collecting timing info
[0m03:36:55.513497 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: Close
[0m03:36:55.898036 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aef5d30e-1a6a-4c81-bdfe-668c4d18ddc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e684340>]}
[0m03:36:55.898929 [info ] [Thread-1  ]: 1 of 18 OK loaded seed file bruno.raw_customers ................................ [[32mINSERT 100[0m in 3.77s]
[0m03:36:55.899863 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_customers
[0m03:36:55.900265 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_orders
[0m03:36:55.901228 [info ] [Thread-1  ]: 2 of 18 START seed file bruno.raw_orders ....................................... [RUN]
[0m03:36:55.902856 [debug] [Thread-1  ]: Acquiring new snowflake connection "seed.jaffle_shop.raw_orders"
[0m03:36:55.903201 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_orders
[0m03:36:55.903493 [debug] [Thread-1  ]: finished collecting timing info
[0m03:36:55.903764 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_orders
[0m03:36:55.923058 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m03:36:55.923375 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
BEGIN
[0m03:36:55.923563 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:36:56.707584 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.78 seconds
[0m03:36:56.708840 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m03:36:56.709067 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
truncate table "JAFFLE_SHOP"."BRUNO"."RAW_ORDERS"
  ;
[0m03:36:57.219561 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.51 seconds
[0m03:36:57.220448 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m03:36:57.220991 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
COMMIT
[0m03:36:57.498338 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.28 seconds
[0m03:36:57.509475 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m03:36:57.509975 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
BEGIN
[0m03:36:57.834960 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.32 seconds
[0m03:36:57.843116 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m03:36:57.843665 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
insert into jaffle_shop.bruno.raw_orders (id, user_id, order_date, status) values
            (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s...
[0m03:36:58.635520 [debug] [Thread-1  ]: SQL status: SUCCESS 99 in 0.79 seconds
[0m03:36:58.636737 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m03:36:58.637138 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
COMMIT
[0m03:36:58.918856 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.28 seconds
[0m03:36:58.922192 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_orders"
[0m03:36:58.929382 [debug] [Thread-1  ]: finished collecting timing info
[0m03:36:58.929809 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: Close
[0m03:36:59.273738 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aef5d30e-1a6a-4c81-bdfe-668c4d18ddc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f06deb0>]}
[0m03:36:59.274973 [info ] [Thread-1  ]: 2 of 18 OK loaded seed file bruno.raw_orders ................................... [[32mINSERT 99[0m in 3.37s]
[0m03:36:59.276313 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_orders
[0m03:36:59.276801 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_payments
[0m03:36:59.277509 [info ] [Thread-1  ]: 3 of 18 START seed file bruno.raw_payments ..................................... [RUN]
[0m03:36:59.279222 [debug] [Thread-1  ]: Acquiring new snowflake connection "seed.jaffle_shop.raw_payments"
[0m03:36:59.279595 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_payments
[0m03:36:59.279899 [debug] [Thread-1  ]: finished collecting timing info
[0m03:36:59.280174 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_payments
[0m03:36:59.300450 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m03:36:59.300768 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
BEGIN
[0m03:36:59.300956 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:00.089370 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.79 seconds
[0m03:37:00.090367 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m03:37:00.090808 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
truncate table "JAFFLE_SHOP"."BRUNO"."RAW_PAYMENTS"
  ;
[0m03:37:00.570306 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.48 seconds
[0m03:37:00.571327 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m03:37:00.571679 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
COMMIT
[0m03:37:00.905740 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.33 seconds
[0m03:37:00.916868 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m03:37:00.917312 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
BEGIN
[0m03:37:01.215252 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.3 seconds
[0m03:37:01.224820 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m03:37:01.225282 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
insert into jaffle_shop.bruno.raw_payments (id, order_id, payment_method, amount) values
            (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s...
[0m03:37:02.134572 [debug] [Thread-1  ]: SQL status: SUCCESS 113 in 0.91 seconds
[0m03:37:02.135282 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m03:37:02.135566 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
COMMIT
[0m03:37:02.428648 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.29 seconds
[0m03:37:02.431604 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_payments"
[0m03:37:02.438209 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:02.438698 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: Close
[0m03:37:02.774495 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aef5d30e-1a6a-4c81-bdfe-668c4d18ddc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fdccdf0>]}
[0m03:37:02.775463 [info ] [Thread-1  ]: 3 of 18 OK loaded seed file bruno.raw_payments ................................. [[32mINSERT 113[0m in 3.50s]
[0m03:37:02.776321 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_payments
[0m03:37:02.776750 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m03:37:02.777441 [info ] [Thread-1  ]: 4 of 18 START sql view model bruno.stg_customers ............................... [RUN]
[0m03:37:02.779146 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.stg_customers"
[0m03:37:02.779516 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m03:37:02.779811 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m03:37:02.787266 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m03:37:02.788515 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:02.788887 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m03:37:02.820336 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_customers"
[0m03:37:02.821462 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.stg_customers"
[0m03:37:02.821625 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace  view jaffle_shop.bruno.stg_customers
  
   as (
    with source as (
    select * from jaffle_shop.bruno.raw_customers

),

renamed as (

    select
        id as customer_id,
        first_name,
        last_name

    from source

)

select * from renamed
  );
[0m03:37:02.821766 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:03.775016 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.95 seconds
[0m03:37:03.787710 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:03.788187 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m03:37:04.291796 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aef5d30e-1a6a-4c81-bdfe-668c4d18ddc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fda4ee0>]}
[0m03:37:04.292774 [info ] [Thread-1  ]: 4 of 18 OK created sql view model bruno.stg_customers .......................... [[32mSUCCESS 1[0m in 1.51s]
[0m03:37:04.293640 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m03:37:04.294013 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m03:37:04.294605 [info ] [Thread-1  ]: 5 of 18 START sql view model bruno.stg_orders .................................. [RUN]
[0m03:37:04.296079 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.stg_orders"
[0m03:37:04.296391 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m03:37:04.296670 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m03:37:04.303056 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m03:37:04.305022 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:04.305372 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m03:37:04.310984 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_orders"
[0m03:37:04.312525 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.stg_orders"
[0m03:37:04.312760 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace  view jaffle_shop.bruno.stg_orders
  
   as (
    with source as (
    select * from jaffle_shop.bruno.raw_orders

),

renamed as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from source

)

select * from renamed
  );
[0m03:37:04.312969 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:05.184131 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.87 seconds
[0m03:37:05.191433 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:05.191997 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m03:37:05.725325 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aef5d30e-1a6a-4c81-bdfe-668c4d18ddc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f06dfd0>]}
[0m03:37:05.726104 [info ] [Thread-1  ]: 5 of 18 OK created sql view model bruno.stg_orders ............................. [[32mSUCCESS 1[0m in 1.43s]
[0m03:37:05.726943 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m03:37:05.727303 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m03:37:05.727901 [info ] [Thread-1  ]: 6 of 18 START sql view model bruno.stg_payments ................................ [RUN]
[0m03:37:05.729401 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.stg_payments"
[0m03:37:05.729719 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m03:37:05.730041 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m03:37:05.736481 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m03:37:05.737369 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:05.737655 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m03:37:05.743196 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_payments"
[0m03:37:05.744855 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.stg_payments"
[0m03:37:05.745088 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace  view jaffle_shop.bruno.stg_payments
  
   as (
    with source as (
    select * from jaffle_shop.bruno.raw_payments

),

renamed as (

    select
        id as payment_id,
        order_id,
        payment_method,

        -- `amount` is currently stored in cents, so we convert it to dollars
        amount / 100 as amount

    from source

)

select * from renamed
  );
[0m03:37:05.745314 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:06.493835 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.75 seconds
[0m03:37:06.497394 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:06.497682 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m03:37:06.828556 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aef5d30e-1a6a-4c81-bdfe-668c4d18ddc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f06dfa0>]}
[0m03:37:06.829104 [info ] [Thread-1  ]: 6 of 18 OK created sql view model bruno.stg_payments ........................... [[32mSUCCESS 1[0m in 1.10s]
[0m03:37:06.829603 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m03:37:06.829829 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:37:06.830115 [info ] [Thread-1  ]: 7 of 18 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m03:37:06.830939 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m03:37:06.831106 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:37:06.831256 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:37:06.852275 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m03:37:06.852933 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:06.853161 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:37:06.875966 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m03:37:06.877017 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m03:37:06.877180 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from jaffle_shop.bruno.stg_customers
where customer_id is null



      
    ) dbt_internal_test
[0m03:37:06.877321 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:08.074729 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 1.2 seconds
[0m03:37:08.090862 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:08.091202 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m03:37:08.490934 [info ] [Thread-1  ]: 7 of 18 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 1.66s]
[0m03:37:08.491979 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:37:08.492361 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m03:37:08.492803 [info ] [Thread-1  ]: 8 of 18 START test unique_stg_customers_customer_id ............................ [RUN]
[0m03:37:08.494082 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m03:37:08.494392 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m03:37:08.494670 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m03:37:08.508613 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m03:37:08.510675 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:08.510942 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m03:37:08.515207 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m03:37:08.516600 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m03:37:08.516797 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from jaffle_shop.bruno.stg_customers
where customer_id is not null
group by customer_id
having count(*) > 1



      
    ) dbt_internal_test
[0m03:37:08.516975 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:09.202094 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.69 seconds
[0m03:37:09.207704 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:09.208200 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: Close
[0m03:37:09.549196 [info ] [Thread-1  ]: 8 of 18 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 1.06s]
[0m03:37:09.550204 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m03:37:09.550575 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m03:37:09.550995 [info ] [Thread-1  ]: 9 of 18 START test accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned  [RUN]
[0m03:37:09.552211 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m03:37:09.552530 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m03:37:09.552821 [debug] [Thread-1  ]: Compiling test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m03:37:09.574140 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m03:37:09.574884 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:09.575114 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m03:37:09.579232 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m03:37:09.580683 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m03:37:09.580879 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from jaffle_shop.bruno.stg_orders
    group by status

)

select *
from all_values
where value_field not in (
    'placed','shipped','completed','return_pending','returned'
)



      
    ) dbt_internal_test
[0m03:37:09.581059 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:10.429233 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.85 seconds
[0m03:37:10.431946 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:10.432146 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad: Close
[0m03:37:10.948694 [info ] [Thread-1  ]: 9 of 18 PASS accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned  [[32mPASS[0m in 1.40s]
[0m03:37:10.949981 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m03:37:10.950385 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m03:37:10.950857 [info ] [Thread-1  ]: 10 of 18 START test not_null_stg_orders_order_id ............................... [RUN]
[0m03:37:10.952168 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m03:37:10.952485 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m03:37:10.952778 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m03:37:10.962617 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m03:37:10.963445 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:10.963703 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m03:37:10.968298 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m03:37:10.969730 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m03:37:10.969962 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from jaffle_shop.bruno.stg_orders
where order_id is null



      
    ) dbt_internal_test
[0m03:37:10.970182 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:11.968004 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 1.0 seconds
[0m03:37:11.974320 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:11.974826 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: Close
[0m03:37:12.393039 [info ] [Thread-1  ]: 10 of 18 PASS not_null_stg_orders_order_id ..................................... [[32mPASS[0m in 1.44s]
[0m03:37:12.394435 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m03:37:12.394840 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m03:37:12.395286 [info ] [Thread-1  ]: 11 of 18 START test unique_stg_orders_order_id ................................. [RUN]
[0m03:37:12.396653 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m03:37:12.396961 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m03:37:12.397235 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m03:37:12.406514 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m03:37:12.408305 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:12.408602 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m03:37:12.413308 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m03:37:12.415011 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m03:37:12.415246 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    order_id as unique_field,
    count(*) as n_records

from jaffle_shop.bruno.stg_orders
where order_id is not null
group by order_id
having count(*) > 1



      
    ) dbt_internal_test
[0m03:37:12.415461 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:13.199797 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.78 seconds
[0m03:37:13.206095 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:13.206582 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: Close
[0m03:37:13.612027 [info ] [Thread-1  ]: 11 of 18 PASS unique_stg_orders_order_id ....................................... [[32mPASS[0m in 1.22s]
[0m03:37:13.613003 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m03:37:13.613383 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m03:37:13.613833 [info ] [Thread-1  ]: 12 of 18 START test accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card  [RUN]
[0m03:37:13.615048 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m03:37:13.615314 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m03:37:13.615537 [debug] [Thread-1  ]: Compiling test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m03:37:13.627153 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m03:37:13.628511 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:13.628733 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m03:37:13.632543 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m03:37:13.634006 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m03:37:13.634202 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        payment_method as value_field,
        count(*) as n_records

    from jaffle_shop.bruno.stg_payments
    group by payment_method

)

select *
from all_values
where value_field not in (
    'credit_card','coupon','bank_transfer','gift_card'
)



      
    ) dbt_internal_test
[0m03:37:13.634377 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:14.833256 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 1.2 seconds
[0m03:37:14.836857 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:14.837115 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278: Close
[0m03:37:15.250636 [info ] [Thread-1  ]: 12 of 18 PASS accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card  [[32mPASS[0m in 1.64s]
[0m03:37:15.251755 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m03:37:15.252162 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m03:37:15.252683 [info ] [Thread-1  ]: 13 of 18 START test not_null_stg_payments_payment_id ........................... [RUN]
[0m03:37:15.254024 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m03:37:15.254329 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m03:37:15.254640 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m03:37:15.264334 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m03:37:15.265172 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:15.265424 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m03:37:15.270234 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m03:37:15.271780 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m03:37:15.272042 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select payment_id
from jaffle_shop.bruno.stg_payments
where payment_id is null



      
    ) dbt_internal_test
[0m03:37:15.272258 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:16.167446 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.9 seconds
[0m03:37:16.174076 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:16.174567 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075: Close
[0m03:37:16.550665 [info ] [Thread-1  ]: 13 of 18 PASS not_null_stg_payments_payment_id ................................. [[32mPASS[0m in 1.30s]
[0m03:37:16.552021 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m03:37:16.552549 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m03:37:16.552967 [info ] [Thread-1  ]: 14 of 18 START test unique_stg_payments_payment_id ............................. [RUN]
[0m03:37:16.554417 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m03:37:16.554946 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m03:37:16.555317 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m03:37:16.564794 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m03:37:16.565620 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:16.565885 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m03:37:16.570568 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m03:37:16.572440 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m03:37:16.572767 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_payments_payment_id.3744510712: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    payment_id as unique_field,
    count(*) as n_records

from jaffle_shop.bruno.stg_payments
where payment_id is not null
group by payment_id
having count(*) > 1



      
    ) dbt_internal_test
[0m03:37:16.572989 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:17.394773 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.82 seconds
[0m03:37:17.400772 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:17.401220 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_payments_payment_id.3744510712: Close
[0m03:37:17.808983 [info ] [Thread-1  ]: 14 of 18 PASS unique_stg_payments_payment_id ................................... [[32mPASS[0m in 1.26s]
[0m03:37:17.810075 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m03:37:17.811518 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m03:37:17.812162 [info ] [Thread-1  ]: 15 of 18 START python table model bruno.customers .............................. [RUN]
[0m03:37:17.813333 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m03:37:17.813617 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m03:37:17.813887 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m03:37:17.847578 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m03:37:17.849283 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:17.849569 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m03:37:17.876208 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m03:37:17.878565 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:37:17.878685 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.customers__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    stg_customers = dbt.ref('stg_customers')
    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    customer_orders = (
        stg_orders
        .group_by("customer_id")
        .agg(
            F.min(F.col("order_date")).alias('first_order'),
            F.max(F.col("order_date")).alias('most_recent_order'),
            F.count(F.col("order_id")).alias('number_of_orders')
        )
    )

    customer_payments = (
        stg_payments
        .join(stg_orders, stg_payments.order_id == stg_orders.order_id, "left")
        .group_by(stg_orders.customer_id)
        .agg(
            F.sum(F.col("amount")).alias('total_amount')
        )
    )

    final_df = (
        stg_customers
        .join(customer_orders, stg_customers.customer_id == customer_orders.customer_id, "left")
        .join(customer_payments, stg_customers.customer_id == customer_payments.customer_id, "left")
        .select(stg_customers.customer_id.alias("customer_id"),
                stg_customers.first_name.alias("first_name"),
                stg_customers.last_name.alias("last_name"),
                customer_orders.first_order.alias("first_order"),
                customer_orders.most_recent_order.alias("most_recent_order"),
                customer_orders.number_of_orders.alias("number_of_orders"),
                customer_payments.total_amount.alias("customer_lifetime_value")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle_shop.bruno.stg_customers", "stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle_shop.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.customers", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m03:37:17.878796 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:23.539495 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.66 seconds
[0m03:37:23.541206 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:37:23.541591 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
CALL jaffle_shop.bruno.customers__dbt_sp();
[0m03:37:25.690985 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 2.15 seconds
[0m03:37:25.694888 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:37:25.695530 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
drop procedure if exists jaffle_shop.bruno.customers__dbt_sp()
[0m03:37:25.998145 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.3 seconds
[0m03:37:26.006791 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:26.007197 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m03:37:26.411326 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aef5d30e-1a6a-4c81-bdfe-668c4d18ddc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f069760>]}
[0m03:37:26.412267 [info ] [Thread-1  ]: 15 of 18 OK created python table model bruno.customers ......................... [[32mSUCCESS 1[0m in 8.60s]
[0m03:37:26.413155 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m03:37:26.413532 [debug] [Thread-1  ]: Began running node model.jaffle_shop.orders
[0m03:37:26.414105 [info ] [Thread-1  ]: 16 of 18 START python table model bruno.orders ................................. [RUN]
[0m03:37:26.415538 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.orders"
[0m03:37:26.415901 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.orders
[0m03:37:26.416288 [debug] [Thread-1  ]: Compiling model.jaffle_shop.orders
[0m03:37:26.424126 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.orders"
[0m03:37:26.425821 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:26.426092 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.orders
[0m03:37:26.431041 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.orders"
[0m03:37:26.434835 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.orders"
[0m03:37:26.435131 [debug] [Thread-1  ]: On model.jaffle_shop.orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.orders"} */
CREATE OR REPLACE PROCEDURE jaffle_shop.bruno.orders__dbt_sp ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    payment_methods = ['credit_card', 'coupon', 'bank_transfer', 'gift_card']

    agg_list = [F.sum(F.when(stg_payments.payment_method == payment_method, stg_payments.amount).otherwise(0)).alias(payment_method + "_amount") for payment_method in payment_methods]

    agg_list.append(F.sum(F.col("amount")).alias("total_amount"))

    order_payments = (
        stg_payments
        .group_by("order_id")
        .agg(*agg_list)
    )

    final_df = (
        stg_orders
        .join(order_payments, stg_orders.order_id == order_payments.order_id, "left")
        .select(stg_orders.order_id.alias("order_id"),
                stg_orders.customer_id.alias("customer_id"),
                stg_orders.order_date.alias("order_date"),
                stg_orders.status.alias("status"),
                *[F.col(payment_method + "_amount") for payment_method in payment_methods],
                order_payments.total_amount.alias("amount")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'orders'
    def __repr__(self):
        return 'jaffle_shop.bruno.orders'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.orders", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$;
[0m03:37:26.435330 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:32.039237 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 5.6 seconds
[0m03:37:32.039968 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.orders"
[0m03:37:32.040309 [debug] [Thread-1  ]: On model.jaffle_shop.orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.orders"} */
CALL jaffle_shop.bruno.orders__dbt_sp();
[0m03:37:33.881457 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 1.84 seconds
[0m03:37:33.884825 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.orders"
[0m03:37:33.885515 [debug] [Thread-1  ]: On model.jaffle_shop.orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.orders"} */
drop procedure if exists jaffle_shop.bruno.orders__dbt_sp()
[0m03:37:34.187187 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.3 seconds
[0m03:37:34.196095 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:34.196592 [debug] [Thread-1  ]: On model.jaffle_shop.orders: Close
[0m03:37:34.546093 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aef5d30e-1a6a-4c81-bdfe-668c4d18ddc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1102a6b80>]}
[0m03:37:34.547152 [info ] [Thread-1  ]: 16 of 18 OK created python table model bruno.orders ............................ [[32mSUCCESS 1[0m in 8.13s]
[0m03:37:34.547988 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.orders
[0m03:37:34.548373 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m03:37:34.548807 [info ] [Thread-1  ]: 17 of 18 START test not_null_customers_customer_id ............................. [RUN]
[0m03:37:34.550083 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m03:37:34.550390 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m03:37:34.550668 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m03:37:34.559961 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m03:37:34.560901 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:34.561316 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m03:37:34.566090 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m03:37:34.567567 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m03:37:34.567806 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from jaffle_shop.bruno.customers
where customer_id is null



      
    ) dbt_internal_test
[0m03:37:34.568022 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:35.417326 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.85 seconds
[0m03:37:35.424268 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:35.424811 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d: Close
[0m03:37:35.812821 [info ] [Thread-1  ]: 17 of 18 PASS not_null_customers_customer_id ................................... [[32mPASS[0m in 1.26s]
[0m03:37:35.813777 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m03:37:35.814156 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m03:37:35.814612 [info ] [Thread-1  ]: 18 of 18 START test unique_customers_customer_id ............................... [RUN]
[0m03:37:35.815903 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m03:37:35.816218 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m03:37:35.816499 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m03:37:35.825183 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m03:37:35.825942 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:35.826208 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m03:37:35.830692 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m03:37:35.832037 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m03:37:35.832251 [debug] [Thread-1  ]: On test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from jaffle_shop.bruno.customers
where customer_id is not null
group by customer_id
having count(*) > 1



      
    ) dbt_internal_test
[0m03:37:35.832431 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:36.750224 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.92 seconds
[0m03:37:36.757010 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:36.757486 [debug] [Thread-1  ]: On test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1: Close
[0m03:37:37.133390 [info ] [Thread-1  ]: 18 of 18 PASS unique_customers_customer_id ..................................... [[32mPASS[0m in 1.32s]
[0m03:37:37.134348 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m03:37:37.136709 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:37:37.137916 [info ] [MainThread]: 
[0m03:37:37.138473 [info ] [MainThread]: Finished running 3 seeds, 3 view models, 10 tests, 2 table models in 0 hours 0 minutes and 47.62 seconds (47.62s).
[0m03:37:37.138958 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:37:37.139199 [debug] [MainThread]: Connection 'test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1' was properly closed.
[0m03:37:37.156740 [info ] [MainThread]: 
[0m03:37:37.157319 [info ] [MainThread]: [32mCompleted successfully[0m
[0m03:37:37.157759 [info ] [MainThread]: 
[0m03:37:37.158095 [info ] [MainThread]: Done. PASS=18 WARN=0 ERROR=0 SKIP=0 TOTAL=18
[0m03:37:37.158666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1102770a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f065be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ee60550>]}
[0m03:37:37.159061 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 03:41:20.675313 | f50e8d1b-f442-4902-ac19-2d430a329182 ==============================
[0m03:41:20.675365 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:41:20.676165 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'indirect_selection': 'eager', 'resource_types': [], 'which': 'build', 'rpc_method': 'build'}
[0m03:41:20.676295 [debug] [MainThread]: Tracking: tracking
[0m03:41:20.676596 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129ccb20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129ccb80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129ccbe0>]}
[0m03:41:20.708863 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m03:41:20.709169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'f50e8d1b-f442-4902-ac19-2d430a329182', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112861790>]}
[0m03:41:20.730942 [debug] [MainThread]: Parsing macros/catalog.sql
[0m03:41:20.732982 [debug] [MainThread]: Parsing macros/adapters.sql
[0m03:41:20.768609 [debug] [MainThread]: Parsing macros/apply_grants.sql
[0m03:41:20.769512 [debug] [MainThread]: Parsing macros/materializations/test.sql
[0m03:41:20.770228 [debug] [MainThread]: Parsing macros/materializations/merge.sql
[0m03:41:20.773052 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m03:41:20.776943 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m03:41:20.777887 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m03:41:20.782260 [debug] [MainThread]: Parsing macros/materializations/incremental.sql
[0m03:41:20.789878 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m03:41:20.790540 [debug] [MainThread]: Parsing macros/utils/timestamps.sql
[0m03:41:20.791866 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m03:41:20.792251 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m03:41:20.792695 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m03:41:20.793039 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m03:41:20.793332 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m03:41:20.793691 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m03:41:20.796573 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m03:41:20.798307 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m03:41:20.799495 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m03:41:20.811538 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m03:41:20.822120 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m03:41:20.831808 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m03:41:20.835123 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m03:41:20.836392 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m03:41:20.837652 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m03:41:20.843585 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m03:41:20.856311 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m03:41:20.857399 [debug] [MainThread]: Parsing macros/materializations/models/incremental/strategies.sql
[0m03:41:20.862310 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m03:41:20.870260 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m03:41:20.884163 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m03:41:20.888317 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m03:41:20.890805 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m03:41:20.894943 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m03:41:20.895844 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m03:41:20.898297 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m03:41:20.899887 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m03:41:20.905189 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m03:41:20.919938 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m03:41:20.920978 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m03:41:20.922762 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m03:41:20.923864 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m03:41:20.924487 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m03:41:20.925039 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m03:41:20.925507 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m03:41:20.926478 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m03:41:20.930312 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m03:41:20.936720 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m03:41:20.937308 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m03:41:20.938167 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m03:41:20.938825 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m03:41:20.939478 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m03:41:20.940360 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m03:41:20.940924 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m03:41:20.941643 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m03:41:20.942661 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m03:41:20.944370 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m03:41:20.945232 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m03:41:20.945973 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m03:41:20.946703 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m03:41:20.947413 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m03:41:20.948045 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m03:41:20.948790 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m03:41:20.949415 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m03:41:20.955290 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m03:41:20.956024 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m03:41:20.956659 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m03:41:20.957925 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m03:41:20.959598 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m03:41:20.960310 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m03:41:20.961343 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m03:41:20.962065 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m03:41:20.963543 [debug] [MainThread]: Parsing macros/adapters/timestamps.sql
[0m03:41:20.965872 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m03:41:20.967812 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m03:41:20.978947 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m03:41:20.980322 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m03:41:20.990085 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m03:41:20.993220 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m03:41:20.998456 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m03:41:21.005669 [debug] [MainThread]: Parsing macros/python_model/python.sql
[0m03:41:21.010156 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m03:41:21.204271 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_customers.sql
[0m03:41:21.213085 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_payments.sql
[0m03:41:21.215057 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_orders.sql
[0m03:41:21.370529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f50e8d1b-f442-4902-ac19-2d430a329182', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a7a9a0>]}
[0m03:41:21.377700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f50e8d1b-f442-4902-ac19-2d430a329182', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112f83c70>]}
[0m03:41:21.377900 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 303 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m03:41:21.378074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f50e8d1b-f442-4902-ac19-2d430a329182', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d630790>]}
[0m03:41:21.379628 [info ] [MainThread]: 
[0m03:41:21.380155 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:41:21.381137 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop"
[0m03:41:21.392681 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop"
[0m03:41:21.392862 [debug] [ThreadPool]: On list_jaffle_shop: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop"} */
show terse schemas in database jaffle_shop
    limit 10000
[0m03:41:21.392962 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:41:22.452505 [debug] [ThreadPool]: SQL status: SUCCESS 3 in 1.06 seconds
[0m03:41:22.456962 [debug] [ThreadPool]: On list_jaffle_shop: Close
[0m03:41:22.795013 [debug] [ThreadPool]: Acquiring new snowflake connection "list_jaffle_shop_bruno"
[0m03:41:22.808064 [debug] [ThreadPool]: Using snowflake connection "list_jaffle_shop_bruno"
[0m03:41:22.808399 [debug] [ThreadPool]: On list_jaffle_shop_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_jaffle_shop_bruno"} */
show terse objects in jaffle_shop.bruno
[0m03:41:22.808589 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:41:23.768590 [debug] [ThreadPool]: SQL status: SUCCESS 8 in 0.96 seconds
[0m03:41:23.770521 [debug] [ThreadPool]: On list_jaffle_shop_bruno: Close
[0m03:41:24.287829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f50e8d1b-f442-4902-ac19-2d430a329182', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1132001c0>]}
[0m03:41:24.289484 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:41:24.289951 [info ] [MainThread]: 
[0m03:41:24.296335 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_customers
[0m03:41:24.297117 [info ] [Thread-1  ]: 1 of 28 START seed file bruno.raw_customers .................................... [RUN]
[0m03:41:24.297881 [debug] [Thread-1  ]: Acquiring new snowflake connection "seed.jaffle_shop.raw_customers"
[0m03:41:24.298084 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_customers
[0m03:41:24.298288 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:24.298461 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_customers
[0m03:41:24.354667 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m03:41:24.354879 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
BEGIN
[0m03:41:24.354998 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:41:25.103107 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.75 seconds
[0m03:41:25.103933 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m03:41:25.104272 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
truncate table "JAFFLE_SHOP"."BRUNO"."RAW_CUSTOMERS"
  ;
[0m03:41:25.611281 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.51 seconds
[0m03:41:25.611801 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m03:41:25.612042 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
COMMIT
[0m03:41:25.919625 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.31 seconds
[0m03:41:25.956322 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m03:41:25.956633 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
BEGIN
[0m03:41:26.152041 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.2 seconds
[0m03:41:26.158499 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m03:41:26.159023 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
insert into jaffle_shop.bruno.raw_customers (id, first_name, last_name) values
            (%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s,%s,%s),(%s...
[0m03:41:26.842624 [debug] [Thread-1  ]: SQL status: SUCCESS 100 in 0.68 seconds
[0m03:41:26.844905 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_customers"
[0m03:41:26.845382 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */
COMMIT
[0m03:41:27.148186 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.3 seconds
[0m03:41:27.159925 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_customers"
[0m03:41:27.187837 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:27.188071 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: Close
[0m03:41:27.616806 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f50e8d1b-f442-4902-ac19-2d430a329182', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113200130>]}
[0m03:41:27.617411 [info ] [Thread-1  ]: 1 of 28 OK loaded seed file bruno.raw_customers ................................ [[32mINSERT 100[0m in 3.32s]
[0m03:41:27.617811 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_customers
[0m03:41:27.617974 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_orders
[0m03:41:27.618430 [info ] [Thread-1  ]: 2 of 28 START seed file bruno.raw_orders ....................................... [RUN]
[0m03:41:27.619087 [debug] [Thread-1  ]: Acquiring new snowflake connection "seed.jaffle_shop.raw_orders"
[0m03:41:27.619270 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_orders
[0m03:41:27.619394 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:27.619504 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_orders
[0m03:41:27.628505 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m03:41:27.628679 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
BEGIN
[0m03:41:27.628785 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:41:28.179905 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.55 seconds
[0m03:41:28.180191 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m03:41:28.180309 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
truncate table "JAFFLE_SHOP"."BRUNO"."RAW_ORDERS"
  ;
[0m03:41:28.576107 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.4 seconds
[0m03:41:28.576763 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m03:41:28.577047 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
COMMIT
[0m03:41:28.857860 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.28 seconds
[0m03:41:28.865366 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m03:41:28.865622 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
BEGIN
[0m03:41:29.057297 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.19 seconds
[0m03:41:29.065943 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m03:41:29.066373 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
insert into jaffle_shop.bruno.raw_orders (id, user_id, order_date, status) values
            (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s...
[0m03:41:29.814244 [debug] [Thread-1  ]: SQL status: SUCCESS 99 in 0.75 seconds
[0m03:41:29.815430 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_orders"
[0m03:41:29.815921 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */
COMMIT
[0m03:41:30.110474 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.29 seconds
[0m03:41:30.112721 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_orders"
[0m03:41:30.120338 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:30.120840 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: Close
[0m03:41:30.456559 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f50e8d1b-f442-4902-ac19-2d430a329182', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114601250>]}
[0m03:41:30.456996 [info ] [Thread-1  ]: 2 of 28 OK loaded seed file bruno.raw_orders ................................... [[32mINSERT 99[0m in 2.84s]
[0m03:41:30.457487 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_orders
[0m03:41:30.457718 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_payments
[0m03:41:30.458137 [info ] [Thread-1  ]: 3 of 28 START seed file bruno.raw_payments ..................................... [RUN]
[0m03:41:30.459159 [debug] [Thread-1  ]: Acquiring new snowflake connection "seed.jaffle_shop.raw_payments"
[0m03:41:30.459339 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_payments
[0m03:41:30.459512 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:30.459672 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_payments
[0m03:41:30.471774 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m03:41:30.471994 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
BEGIN
[0m03:41:30.472127 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:41:31.135521 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.66 seconds
[0m03:41:31.136065 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m03:41:31.136371 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
truncate table "JAFFLE_SHOP"."BRUNO"."RAW_PAYMENTS"
  ;
[0m03:41:31.548939 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.41 seconds
[0m03:41:31.550196 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m03:41:31.550924 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
COMMIT
[0m03:41:31.839067 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.29 seconds
[0m03:41:31.850082 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m03:41:31.850516 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
BEGIN
[0m03:41:32.040847 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.19 seconds
[0m03:41:32.049045 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m03:41:32.049400 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
insert into jaffle_shop.bruno.raw_payments (id, order_id, payment_method, amount) values
            (%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s,%s,%s,%s),(%s...
[0m03:41:32.782948 [debug] [Thread-1  ]: SQL status: SUCCESS 113 in 0.73 seconds
[0m03:41:32.784308 [debug] [Thread-1  ]: Using snowflake connection "seed.jaffle_shop.raw_payments"
[0m03:41:32.784757 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */
COMMIT
[0m03:41:33.189371 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.4 seconds
[0m03:41:33.191617 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_payments"
[0m03:41:33.197379 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:33.197784 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: Close
[0m03:41:33.603571 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f50e8d1b-f442-4902-ac19-2d430a329182', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1146015e0>]}
[0m03:41:33.604674 [info ] [Thread-1  ]: 3 of 28 OK loaded seed file bruno.raw_payments ................................. [[32mINSERT 113[0m in 3.14s]
[0m03:41:33.605511 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_payments
[0m03:41:33.605902 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m03:41:33.606517 [info ] [Thread-1  ]: 4 of 28 START sql view model bruno.stg_customers ............................... [RUN]
[0m03:41:33.607914 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.stg_customers"
[0m03:41:33.608229 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m03:41:33.608502 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m03:41:33.616225 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m03:41:33.617290 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:33.617578 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m03:41:33.647573 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_customers"
[0m03:41:33.649084 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.stg_customers"
[0m03:41:33.649271 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace  view jaffle_shop.bruno.stg_customers
  
   as (
    with source as (
    select * from jaffle_shop.bruno.raw_customers

),

renamed as (

    select
        id as customer_id,
        first_name,
        last_name

    from source

)

select * from renamed
  );
[0m03:41:33.649420 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:41:34.627357 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.98 seconds
[0m03:41:34.639869 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:34.640399 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m03:41:35.137812 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f50e8d1b-f442-4902-ac19-2d430a329182', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114601460>]}
[0m03:41:35.138574 [info ] [Thread-1  ]: 4 of 28 OK created sql view model bruno.stg_customers .......................... [[32mSUCCESS 1[0m in 1.53s]
[0m03:41:35.139138 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m03:41:35.139398 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m03:41:35.139879 [info ] [Thread-1  ]: 5 of 28 START sql view model bruno.stg_orders .................................. [RUN]
[0m03:41:35.141002 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.stg_orders"
[0m03:41:35.141338 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m03:41:35.141544 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m03:41:35.145868 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m03:41:35.146680 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:35.146941 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m03:41:35.152267 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_orders"
[0m03:41:35.153495 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.stg_orders"
[0m03:41:35.153671 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace  view jaffle_shop.bruno.stg_orders
  
   as (
    with source as (
    select * from jaffle_shop.bruno.raw_orders

),

renamed as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from source

)

select * from renamed
  );
[0m03:41:35.153813 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:41:36.013890 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.86 seconds
[0m03:41:36.020707 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:36.021166 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m03:41:36.399164 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f50e8d1b-f442-4902-ac19-2d430a329182', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114601160>]}
[0m03:41:36.400562 [info ] [Thread-1  ]: 5 of 28 OK created sql view model bruno.stg_orders ............................. [[32mSUCCESS 1[0m in 1.26s]
[0m03:41:36.401606 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m03:41:36.401988 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m03:41:36.402600 [info ] [Thread-1  ]: 6 of 28 START sql view model bruno.stg_payments ................................ [RUN]
[0m03:41:36.404080 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.stg_payments"
[0m03:41:36.404381 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m03:41:36.404649 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m03:41:36.411260 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m03:41:36.412266 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:36.412527 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m03:41:36.417977 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_payments"
[0m03:41:36.419901 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.stg_payments"
[0m03:41:36.420217 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace  view jaffle_shop.bruno.stg_payments
  
   as (
    with source as (
    select * from jaffle_shop.bruno.raw_payments

),

renamed as (

    select
        id as payment_id,
        order_id,
        payment_method,

        -- `amount` is currently stored in cents, so we convert it to dollars
        amount / 100 as amount

    from source

)

select * from renamed
  );
[0m03:41:36.420459 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:41:37.187495 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.77 seconds
[0m03:41:37.193971 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:37.194467 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m03:41:37.525052 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f50e8d1b-f442-4902-ac19-2d430a329182', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114745190>]}
[0m03:41:37.526355 [info ] [Thread-1  ]: 6 of 28 OK created sql view model bruno.stg_payments ........................... [[32mSUCCESS 1[0m in 1.12s]
[0m03:41:37.527328 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m03:41:37.527702 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:41:37.528208 [info ] [Thread-1  ]: 7 of 28 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m03:41:37.529833 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m03:41:37.530163 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:41:37.530476 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:41:37.551600 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m03:41:37.552652 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:37.552918 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:41:37.575444 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m03:41:37.576581 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m03:41:37.576744 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from jaffle_shop.bruno.stg_customers
where customer_id is null



      
    ) dbt_internal_test
[0m03:41:37.576889 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:41:38.412420 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.84 seconds
[0m03:41:38.428317 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:38.428654 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m03:41:38.823401 [info ] [Thread-1  ]: 7 of 28 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 1.29s]
[0m03:41:38.824155 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:41:38.824442 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m03:41:38.824782 [info ] [Thread-1  ]: 8 of 28 START test unique_stg_customers_customer_id ............................ [RUN]
[0m03:41:38.825791 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m03:41:38.826013 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m03:41:38.826214 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m03:41:38.836643 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m03:41:38.838089 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:38.838284 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m03:41:38.841368 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m03:41:38.842413 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m03:41:38.842565 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from jaffle_shop.bruno.stg_customers
where customer_id is not null
group by customer_id
having count(*) > 1



      
    ) dbt_internal_test
[0m03:41:38.842704 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:41:39.538972 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.7 seconds
[0m03:41:39.544256 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:39.544597 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: Close
[0m03:41:39.883640 [info ] [Thread-1  ]: 8 of 28 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 1.06s]
[0m03:41:39.884814 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m03:41:39.885228 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m03:41:39.885756 [info ] [Thread-1  ]: 9 of 28 START test accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned  [RUN]
[0m03:41:39.887098 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m03:41:39.887407 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m03:41:39.887723 [debug] [Thread-1  ]: Compiling test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m03:41:39.902141 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m03:41:39.903124 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:39.903394 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m03:41:39.907600 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m03:41:39.909146 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m03:41:39.909345 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from jaffle_shop.bruno.stg_orders
    group by status

)

select *
from all_values
where value_field not in (
    'placed','shipped','completed','return_pending','returned'
)



      
    ) dbt_internal_test
[0m03:41:39.909541 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:41:40.675455 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.77 seconds
[0m03:41:40.682070 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:40.682580 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad: Close
[0m03:41:41.080449 [info ] [Thread-1  ]: 9 of 28 PASS accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned  [[32mPASS[0m in 1.19s]
[0m03:41:41.081480 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m03:41:41.081912 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m03:41:41.082347 [info ] [Thread-1  ]: 10 of 28 START test not_null_stg_orders_order_id ............................... [RUN]
[0m03:41:41.083635 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m03:41:41.083938 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m03:41:41.084208 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m03:41:41.095884 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m03:41:41.096706 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:41.096957 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m03:41:41.101604 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m03:41:41.103069 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m03:41:41.103310 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from jaffle_shop.bruno.stg_orders
where order_id is null



      
    ) dbt_internal_test
[0m03:41:41.103511 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:41:41.794756 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.69 seconds
[0m03:41:41.801259 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:41.801762 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: Close
[0m03:41:42.235849 [info ] [Thread-1  ]: 10 of 28 PASS not_null_stg_orders_order_id ..................................... [[32mPASS[0m in 1.15s]
[0m03:41:42.236814 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m03:41:42.237192 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m03:41:42.237631 [info ] [Thread-1  ]: 11 of 28 START test unique_stg_orders_order_id ................................. [RUN]
[0m03:41:42.238912 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m03:41:42.239215 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m03:41:42.239494 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m03:41:42.248363 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m03:41:42.249295 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:42.249566 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m03:41:42.254322 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m03:41:42.256131 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m03:41:42.256351 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    order_id as unique_field,
    count(*) as n_records

from jaffle_shop.bruno.stg_orders
where order_id is not null
group by order_id
having count(*) > 1



      
    ) dbt_internal_test
[0m03:41:42.256538 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:41:43.102328 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.85 seconds
[0m03:41:43.108035 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:43.108608 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: Close
[0m03:41:43.440808 [info ] [Thread-1  ]: 11 of 28 PASS unique_stg_orders_order_id ....................................... [[32mPASS[0m in 1.20s]
[0m03:41:43.441765 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m03:41:43.442139 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m03:41:43.442584 [info ] [Thread-1  ]: 12 of 28 START test accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card  [RUN]
[0m03:41:43.443653 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m03:41:43.443903 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m03:41:43.444128 [debug] [Thread-1  ]: Compiling test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m03:41:43.456399 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m03:41:43.457155 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:43.457392 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m03:41:43.461367 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m03:41:43.462854 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m03:41:43.463084 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        payment_method as value_field,
        count(*) as n_records

    from jaffle_shop.bruno.stg_payments
    group by payment_method

)

select *
from all_values
where value_field not in (
    'credit_card','coupon','bank_transfer','gift_card'
)



      
    ) dbt_internal_test
[0m03:41:43.463264 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:41:44.252687 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.79 seconds
[0m03:41:44.259320 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:44.259810 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278: Close
[0m03:41:44.592301 [info ] [Thread-1  ]: 12 of 28 PASS accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card  [[32mPASS[0m in 1.15s]
[0m03:41:44.593495 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m03:41:44.593905 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m03:41:44.594349 [info ] [Thread-1  ]: 13 of 28 START test not_null_stg_payments_payment_id ........................... [RUN]
[0m03:41:44.595617 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m03:41:44.595916 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m03:41:44.596191 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m03:41:44.607558 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m03:41:44.608384 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:44.608639 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m03:41:44.613131 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m03:41:44.614465 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m03:41:44.614724 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select payment_id
from jaffle_shop.bruno.stg_payments
where payment_id is null



      
    ) dbt_internal_test
[0m03:41:44.614929 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:41:45.480766 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.87 seconds
[0m03:41:45.487530 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:45.487994 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075: Close
[0m03:41:45.819884 [info ] [Thread-1  ]: 13 of 28 PASS not_null_stg_payments_payment_id ................................. [[32mPASS[0m in 1.22s]
[0m03:41:45.821007 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m03:41:45.821380 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m03:41:45.821827 [info ] [Thread-1  ]: 14 of 28 START test unique_stg_payments_payment_id ............................. [RUN]
[0m03:41:45.823085 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m03:41:45.823380 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m03:41:45.823642 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m03:41:45.833054 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m03:41:45.834743 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:45.835087 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m03:41:45.840063 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m03:41:45.842125 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m03:41:45.842347 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_payments_payment_id.3744510712: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    payment_id as unique_field,
    count(*) as n_records

from jaffle_shop.bruno.stg_payments
where payment_id is not null
group by payment_id
having count(*) > 1



      
    ) dbt_internal_test
[0m03:41:45.842525 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:41:46.606226 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.76 seconds
[0m03:41:46.612626 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:46.613072 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_payments_payment_id.3744510712: Close
[0m03:41:47.035447 [info ] [Thread-1  ]: 14 of 28 PASS unique_stg_payments_payment_id ................................... [[32mPASS[0m in 1.21s]
[0m03:41:47.036601 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m03:41:47.038033 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m03:41:47.038703 [info ] [Thread-1  ]: 15 of 28 START python table model bruno.customers .............................. [RUN]
[0m03:41:47.040181 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.customers"
[0m03:41:47.040502 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m03:41:47.040779 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m03:41:47.075064 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m03:41:47.076642 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:47.076865 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m03:41:47.103538 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m03:41:47.105878 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.customers"
[0m03:41:47.106005 [debug] [Thread-1  ]: On model.jaffle_shop.customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.customers"} */
WITH customers__dbt_sp AS PROCEDURE ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    stg_customers = dbt.ref('stg_customers')
    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    customer_orders = (
        stg_orders
        .group_by("customer_id")
        .agg(
            F.min(F.col("order_date")).alias('first_order'),
            F.max(F.col("order_date")).alias('most_recent_order'),
            F.count(F.col("order_id")).alias('number_of_orders')
        )
    )

    customer_payments = (
        stg_payments
        .join(stg_orders, stg_payments.order_id == stg_orders.order_id, "left")
        .group_by(stg_orders.customer_id)
        .agg(
            F.sum(F.col("amount")).alias('total_amount')
        )
    )

    final_df = (
        stg_customers
        .join(customer_orders, stg_customers.customer_id == customer_orders.customer_id, "left")
        .join(customer_payments, stg_customers.customer_id == customer_payments.customer_id, "left")
        .select(stg_customers.customer_id.alias("customer_id"),
                stg_customers.first_name.alias("first_name"),
                stg_customers.last_name.alias("last_name"),
                customer_orders.first_order.alias("first_order"),
                customer_orders.most_recent_order.alias("most_recent_order"),
                customer_orders.number_of_orders.alias("number_of_orders"),
                customer_payments.total_amount.alias("customer_lifetime_value")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle_shop.bruno.stg_customers", "stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle_shop.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.customers", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$
CALL customers__dbt_sp();
[0m03:41:47.106113 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:41:56.949346 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 9.84 seconds
[0m03:41:56.955025 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:56.955327 [debug] [Thread-1  ]: On model.jaffle_shop.customers: Close
[0m03:41:57.291720 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f50e8d1b-f442-4902-ac19-2d430a329182', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1147e72e0>]}
[0m03:41:57.292648 [info ] [Thread-1  ]: 15 of 28 OK created python table model bruno.customers ......................... [[32mSUCCESS 1[0m in 10.25s]
[0m03:41:57.293494 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m03:41:57.293863 [debug] [Thread-1  ]: Began running node model.jaffle_shop.orders
[0m03:41:57.294438 [info ] [Thread-1  ]: 16 of 28 START python table model bruno.orders ................................. [RUN]
[0m03:41:57.295877 [debug] [Thread-1  ]: Acquiring new snowflake connection "model.jaffle_shop.orders"
[0m03:41:57.296217 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.orders
[0m03:41:57.296526 [debug] [Thread-1  ]: Compiling model.jaffle_shop.orders
[0m03:41:57.304323 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.orders"
[0m03:41:57.306105 [debug] [Thread-1  ]: finished collecting timing info
[0m03:41:57.306389 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.orders
[0m03:41:57.311386 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.orders"
[0m03:41:57.315008 [debug] [Thread-1  ]: Using snowflake connection "model.jaffle_shop.orders"
[0m03:41:57.315256 [debug] [Thread-1  ]: On model.jaffle_shop.orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.orders"} */
WITH orders__dbt_sp AS PROCEDURE ()

RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')

HANDLER = 'main'
EXECUTE AS CALLER
AS
$$

  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    stg_orders = dbt.ref('stg_orders')
    stg_payments = dbt.ref('stg_payments')

    payment_methods = ['credit_card', 'coupon', 'bank_transfer', 'gift_card']

    agg_list = [F.sum(F.when(stg_payments.payment_method == payment_method, stg_payments.amount).otherwise(0)).alias(payment_method + "_amount") for payment_method in payment_methods]

    agg_list.append(F.sum(F.col("amount")).alias("total_amount"))

    order_payments = (
        stg_payments
        .group_by("order_id")
        .agg(*agg_list)
    )

    final_df = (
        stg_orders
        .join(order_payments, stg_orders.order_id == order_payments.order_id, "left")
        .select(stg_orders.order_id.alias("order_id"),
                stg_orders.customer_id.alias("customer_id"),
                stg_orders.order_date.alias("order_date"),
                stg_orders.status.alias("status"),
                *[F.col(payment_method + "_amount") for payment_method in payment_methods],
                order_payments.total_amount.alias("amount")
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_orders": "jaffle_shop.bruno.stg_orders", "stg_payments": "jaffle_shop.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle_shop'
    schema = 'bruno'
    identifier = 'orders'
    def __repr__(self):
        return 'jaffle_shop.bruno.orders'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# To run this in snowsight, you need to select entry point to be main
# And you may have to modify the return type to text to get the result back
# def main(session):
#     dbt = dbtObj(session.table)
#     df = model(dbt, session)
#     return df.collect()

# to run this in local notebook, you need to create a session following examples https://github.com/Snowflake-Labs/sfguide-getting-started-snowpark-python
# then you can do the following to run model
# dbt = dbtObj(session.table)
# df = model(dbt, session)


def materialize(session, df, target_relation):
    # make sure pandas exists
    import importlib.util
    package_name = 'pandas'
    if importlib.util.find_spec(package_name):
        import pandas
        if isinstance(df, pandas.core.frame.DataFrame):
          # session.write_pandas does not have overwrite function
          df = session.createDataFrame(df)
    df.write.mode("overwrite").save_as_table("jaffle_shop.bruno.orders", create_temp_table=False)

def main(session):
    dbt = dbtObj(session.table)
    df = model(dbt, session)
    materialize(session, df, dbt.this)
    return "OK"

  
$$
CALL orders__dbt_sp();
[0m03:41:57.315452 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:42:03.799315 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 6.48 seconds
[0m03:42:03.808804 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:03.809274 [debug] [Thread-1  ]: On model.jaffle_shop.orders: Close
[0m03:42:04.170055 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f50e8d1b-f442-4902-ac19-2d430a329182', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1148048e0>]}
[0m03:42:04.171212 [info ] [Thread-1  ]: 16 of 28 OK created python table model bruno.orders ............................ [[32mSUCCESS 1[0m in 6.87s]
[0m03:42:04.172163 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.orders
[0m03:42:04.172553 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m03:42:04.173123 [info ] [Thread-1  ]: 17 of 28 START test not_null_customers_customer_id ............................. [RUN]
[0m03:42:04.174935 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m03:42:04.175289 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m03:42:04.175620 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m03:42:04.185750 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m03:42:04.187971 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:04.188321 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m03:42:04.193176 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m03:42:04.194708 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m03:42:04.194910 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from jaffle_shop.bruno.customers
where customer_id is null



      
    ) dbt_internal_test
[0m03:42:04.195089 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:42:05.145097 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.95 seconds
[0m03:42:05.149734 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:05.150136 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d: Close
[0m03:42:05.498609 [info ] [Thread-1  ]: 17 of 28 PASS not_null_customers_customer_id ................................... [[32mPASS[0m in 1.32s]
[0m03:42:05.499880 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m03:42:05.500281 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m03:42:05.500727 [info ] [Thread-1  ]: 18 of 28 START test unique_customers_customer_id ............................... [RUN]
[0m03:42:05.502096 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m03:42:05.502405 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m03:42:05.502683 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m03:42:05.512019 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m03:42:05.513721 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:05.514154 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m03:42:05.519124 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m03:42:05.520904 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m03:42:05.521137 [debug] [Thread-1  ]: On test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from jaffle_shop.bruno.customers
where customer_id is not null
group by customer_id
having count(*) > 1



      
    ) dbt_internal_test
[0m03:42:05.521361 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:42:06.367035 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.85 seconds
[0m03:42:06.369839 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:06.370116 [debug] [Thread-1  ]: On test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1: Close
[0m03:42:06.781703 [info ] [Thread-1  ]: 18 of 28 PASS unique_customers_customer_id ..................................... [[32mPASS[0m in 1.28s]
[0m03:42:06.782425 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m03:42:06.782696 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m03:42:06.783016 [info ] [Thread-1  ]: 19 of 28 START test accepted_values_orders_status__placed__shipped__completed__return_pending__returned  [RUN]
[0m03:42:06.783925 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3"
[0m03:42:06.784147 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m03:42:06.784357 [debug] [Thread-1  ]: Compiling test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m03:42:06.842733 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3"
[0m03:42:06.843197 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:06.843310 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m03:42:06.845310 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3"
[0m03:42:06.846033 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3"
[0m03:42:06.846130 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from jaffle_shop.bruno.orders
    group by status

)

select *
from all_values
where value_field not in (
    'placed','shipped','completed','return_pending','returned'
)



      
    ) dbt_internal_test
[0m03:42:06.846221 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:42:07.603900 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.76 seconds
[0m03:42:07.607546 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:07.607882 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3: Close
[0m03:42:07.939711 [info ] [Thread-1  ]: 19 of 28 PASS accepted_values_orders_status__placed__shipped__completed__return_pending__returned  [[32mPASS[0m in 1.16s]
[0m03:42:07.940760 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m03:42:07.941150 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m03:42:07.941592 [info ] [Thread-1  ]: 20 of 28 START test not_null_orders_amount ..................................... [RUN]
[0m03:42:07.942853 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.not_null_orders_amount.106140f9fd"
[0m03:42:07.943153 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m03:42:07.943429 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m03:42:07.952903 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_orders_amount.106140f9fd"
[0m03:42:07.953899 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:07.954179 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m03:42:07.958610 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_orders_amount.106140f9fd"
[0m03:42:07.959812 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.not_null_orders_amount.106140f9fd"
[0m03:42:07.960004 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_amount.106140f9fd: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_orders_amount.106140f9fd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select amount
from jaffle_shop.bruno.orders
where amount is null



      
    ) dbt_internal_test
[0m03:42:07.960185 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:42:08.827527 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.87 seconds
[0m03:42:08.835144 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:08.835676 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_amount.106140f9fd: Close
[0m03:42:09.218514 [info ] [Thread-1  ]: 20 of 28 PASS not_null_orders_amount ........................................... [[32mPASS[0m in 1.28s]
[0m03:42:09.219781 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m03:42:09.220165 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m03:42:09.220657 [info ] [Thread-1  ]: 21 of 28 START test not_null_orders_bank_transfer_amount ....................... [RUN]
[0m03:42:09.221936 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49"
[0m03:42:09.222259 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m03:42:09.222590 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m03:42:09.232531 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49"
[0m03:42:09.233533 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:09.233804 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m03:42:09.238344 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49"
[0m03:42:09.239669 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49"
[0m03:42:09.239867 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select bank_transfer_amount
from jaffle_shop.bruno.orders
where bank_transfer_amount is null



      
    ) dbt_internal_test
[0m03:42:09.240046 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:42:09.912785 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.67 seconds
[0m03:42:09.917636 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:09.918043 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49: Close
[0m03:42:10.259502 [info ] [Thread-1  ]: 21 of 28 PASS not_null_orders_bank_transfer_amount ............................. [[32mPASS[0m in 1.04s]
[0m03:42:10.260481 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m03:42:10.260862 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m03:42:10.261280 [info ] [Thread-1  ]: 22 of 28 START test not_null_orders_coupon_amount .............................. [RUN]
[0m03:42:10.262483 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625"
[0m03:42:10.262816 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m03:42:10.263069 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m03:42:10.272439 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625"
[0m03:42:10.273255 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:10.273480 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m03:42:10.277551 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625"
[0m03:42:10.278821 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625"
[0m03:42:10.279067 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select coupon_amount
from jaffle_shop.bruno.orders
where coupon_amount is null



      
    ) dbt_internal_test
[0m03:42:10.279255 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:42:10.862556 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.58 seconds
[0m03:42:10.866052 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:10.866362 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625: Close
[0m03:42:11.206534 [info ] [Thread-1  ]: 22 of 28 PASS not_null_orders_coupon_amount .................................... [[32mPASS[0m in 0.94s]
[0m03:42:11.208639 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m03:42:11.209519 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m03:42:11.210359 [info ] [Thread-1  ]: 23 of 28 START test not_null_orders_credit_card_amount ......................... [RUN]
[0m03:42:11.212397 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59"
[0m03:42:11.213005 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m03:42:11.214090 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m03:42:11.226823 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59"
[0m03:42:11.228112 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:11.228607 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m03:42:11.234200 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59"
[0m03:42:11.235919 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59"
[0m03:42:11.236243 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select credit_card_amount
from jaffle_shop.bruno.orders
where credit_card_amount is null



      
    ) dbt_internal_test
[0m03:42:11.236494 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:42:12.070723 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.83 seconds
[0m03:42:12.078047 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:12.078954 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59: Close
[0m03:42:12.488603 [info ] [Thread-1  ]: 23 of 28 PASS not_null_orders_credit_card_amount ............................... [[32mPASS[0m in 1.28s]
[0m03:42:12.489929 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m03:42:12.490385 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m03:42:12.490894 [info ] [Thread-1  ]: 24 of 28 START test not_null_orders_customer_id ................................ [RUN]
[0m03:42:12.492518 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.not_null_orders_customer_id.c5f02694af"
[0m03:42:12.493078 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m03:42:12.493431 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m03:42:12.507535 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_orders_customer_id.c5f02694af"
[0m03:42:12.508545 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:12.508834 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m03:42:12.513290 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_orders_customer_id.c5f02694af"
[0m03:42:12.514685 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.not_null_orders_customer_id.c5f02694af"
[0m03:42:12.514889 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_customer_id.c5f02694af: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_orders_customer_id.c5f02694af"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from jaffle_shop.bruno.orders
where customer_id is null



      
    ) dbt_internal_test
[0m03:42:12.515070 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:42:13.231346 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.72 seconds
[0m03:42:13.239526 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:13.240295 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_customer_id.c5f02694af: Close
[0m03:42:13.643235 [info ] [Thread-1  ]: 24 of 28 PASS not_null_orders_customer_id ...................................... [[32mPASS[0m in 1.15s]
[0m03:42:13.645067 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m03:42:13.645839 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m03:42:13.646503 [info ] [Thread-1  ]: 25 of 28 START test not_null_orders_gift_card_amount ........................... [RUN]
[0m03:42:13.648247 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a"
[0m03:42:13.648802 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m03:42:13.649173 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m03:42:13.664746 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a"
[0m03:42:13.665875 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:13.666202 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m03:42:13.672277 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a"
[0m03:42:13.674204 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a"
[0m03:42:13.674460 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select gift_card_amount
from jaffle_shop.bruno.orders
where gift_card_amount is null



      
    ) dbt_internal_test
[0m03:42:13.674674 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:42:14.460581 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.79 seconds
[0m03:42:14.469859 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:14.470608 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a: Close
[0m03:42:14.801523 [info ] [Thread-1  ]: 25 of 28 PASS not_null_orders_gift_card_amount ................................. [[32mPASS[0m in 1.15s]
[0m03:42:14.803512 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m03:42:14.804421 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m03:42:14.805166 [info ] [Thread-1  ]: 26 of 28 START test not_null_orders_order_id ................................... [RUN]
[0m03:42:14.807415 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.not_null_orders_order_id.cf6c17daed"
[0m03:42:14.808085 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m03:42:14.808483 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m03:42:14.822457 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_orders_order_id.cf6c17daed"
[0m03:42:14.823488 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:14.823812 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m03:42:14.828969 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_orders_order_id.cf6c17daed"
[0m03:42:14.830315 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.not_null_orders_order_id.cf6c17daed"
[0m03:42:14.830569 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_order_id.cf6c17daed: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_orders_order_id.cf6c17daed"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from jaffle_shop.bruno.orders
where order_id is null



      
    ) dbt_internal_test
[0m03:42:14.830779 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:42:15.439277 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.61 seconds
[0m03:42:15.445642 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:15.446536 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_order_id.cf6c17daed: Close
[0m03:42:15.793687 [info ] [Thread-1  ]: 26 of 28 PASS not_null_orders_order_id ......................................... [[32mPASS[0m in 0.99s]
[0m03:42:15.795018 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m03:42:15.795684 [debug] [Thread-1  ]: Began running node test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m03:42:15.796307 [info ] [Thread-1  ]: 27 of 28 START test relationships_orders_customer_id__customer_id__ref_customers_  [RUN]
[0m03:42:15.799297 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2"
[0m03:42:15.801409 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m03:42:15.802268 [debug] [Thread-1  ]: Compiling test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m03:42:15.815738 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2"
[0m03:42:15.816475 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:15.816684 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m03:42:15.821394 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2"
[0m03:42:15.823212 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2"
[0m03:42:15.824010 [debug] [Thread-1  ]: On test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select customer_id as from_field
    from jaffle_shop.bruno.orders
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from jaffle_shop.bruno.customers
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m03:42:15.824338 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:42:16.602960 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.78 seconds
[0m03:42:16.613197 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:16.614179 [debug] [Thread-1  ]: On test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2: Close
[0m03:42:16.960697 [info ] [Thread-1  ]: 27 of 28 PASS relationships_orders_customer_id__customer_id__ref_customers_ .... [[32mPASS[0m in 1.16s]
[0m03:42:16.962754 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m03:42:16.963525 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m03:42:16.964439 [info ] [Thread-1  ]: 28 of 28 START test unique_orders_order_id ..................................... [RUN]
[0m03:42:16.966578 [debug] [Thread-1  ]: Acquiring new snowflake connection "test.jaffle_shop.unique_orders_order_id.fed79b3a6e"
[0m03:42:16.967453 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m03:42:16.968757 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m03:42:16.984984 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_orders_order_id.fed79b3a6e"
[0m03:42:16.986398 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:16.986806 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m03:42:16.993116 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_orders_order_id.fed79b3a6e"
[0m03:42:16.995292 [debug] [Thread-1  ]: Using snowflake connection "test.jaffle_shop.unique_orders_order_id.fed79b3a6e"
[0m03:42:16.995661 [debug] [Thread-1  ]: On test.jaffle_shop.unique_orders_order_id.fed79b3a6e: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_orders_order_id.fed79b3a6e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    order_id as unique_field,
    count(*) as n_records

from jaffle_shop.bruno.orders
where order_id is not null
group by order_id
having count(*) > 1



      
    ) dbt_internal_test
[0m03:42:16.996272 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:42:17.602323 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.61 seconds
[0m03:42:17.609151 [debug] [Thread-1  ]: finished collecting timing info
[0m03:42:17.609927 [debug] [Thread-1  ]: On test.jaffle_shop.unique_orders_order_id.fed79b3a6e: Close
[0m03:42:18.038453 [info ] [Thread-1  ]: 28 of 28 PASS unique_orders_order_id ........................................... [[32mPASS[0m in 1.07s]
[0m03:42:18.039764 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m03:42:18.043156 [debug] [MainThread]: Acquiring new snowflake connection "master"
[0m03:42:18.045203 [info ] [MainThread]: 
[0m03:42:18.046186 [info ] [MainThread]: Finished running 3 seeds, 3 view models, 20 tests, 2 table models in 0 hours 0 minutes and 56.66 seconds (56.66s).
[0m03:42:18.047078 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:42:18.047488 [debug] [MainThread]: Connection 'test.jaffle_shop.unique_orders_order_id.fed79b3a6e' was properly closed.
[0m03:42:18.074611 [info ] [MainThread]: 
[0m03:42:18.075145 [info ] [MainThread]: [32mCompleted successfully[0m
[0m03:42:18.075829 [info ] [MainThread]: 
[0m03:42:18.076287 [info ] [MainThread]: Done. PASS=28 WARN=0 ERROR=0 SKIP=0 TOTAL=28
[0m03:42:18.077085 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a094c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112f41eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11469b6d0>]}
[0m03:42:18.077533 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 22:16:11.090453 | e282ab52-f96e-4bf6-a135-f3ed211d563a ==============================
[0m22:16:11.090466 [info ] [MainThread]: Running with dbt=1.3.2
[0m22:16:11.093486 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m22:16:11.093591 [debug] [MainThread]: Tracking: tracking
[0m22:16:11.121078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e8422e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e841160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e86f340>]}
[0m22:16:11.800531 [debug] [MainThread]: Executing "git --help"
[0m22:16:11.825538 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m22:16:11.826489 [debug] [MainThread]: STDERR: "b''"
[0m22:16:11.836292 [debug] [MainThread]: Acquiring new databricks connection "debug"
[0m22:16:11.838019 [debug] [MainThread]: Using databricks connection "debug"
[0m22:16:11.838203 [debug] [MainThread]: On debug: select 1 as id
[0m22:16:11.838343 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:16:31.411139 [debug] [MainThread]: SQL status: OK in 19.57 seconds
[0m22:16:32.920459 [debug] [MainThread]: On debug: Close
[0m22:16:34.174517 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12cd58550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12cd58910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12cd58310>]}
[0m22:16:34.175909 [debug] [MainThread]: Flushing usage events
[0m22:16:35.207347 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-01-24 22:17:38.199118 | e8b5baee-d921-4b43-9657-e6abe20d7958 ==============================
[0m22:17:38.199182 [info ] [MainThread]: Running with dbt=1.3.2
[0m22:17:38.199955 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'indirect_selection': 'eager', 'resource_types': [], 'exclude': ['orders.py', 'customers.py'], 'which': 'build', 'rpc_method': 'build'}
[0m22:17:38.200106 [debug] [MainThread]: Tracking: tracking
[0m22:17:38.228287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c69a8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c69abb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c6ad640>]}
[0m22:17:38.268420 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m22:17:38.268687 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
[0m22:17:38.268853 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m22:17:38.269022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'e8b5baee-d921-4b43-9657-e6abe20d7958', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e232370>]}
[0m22:17:38.295692 [debug] [MainThread]: Parsing macros/statement.sql
[0m22:17:38.298541 [debug] [MainThread]: Parsing macros/copy_into.sql
[0m22:17:38.304546 [debug] [MainThread]: Parsing macros/catalog.sql
[0m22:17:38.306255 [debug] [MainThread]: Parsing macros/adapters.sql
[0m22:17:38.324367 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m22:17:38.330988 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m22:17:38.331397 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m22:17:38.334411 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m22:17:38.350738 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m22:17:38.351877 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m22:17:38.357918 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m22:17:38.358686 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m22:17:38.359549 [debug] [MainThread]: Parsing macros/adapters.sql
[0m22:17:38.394301 [debug] [MainThread]: Parsing macros/apply_grants.sql
[0m22:17:38.396620 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m22:17:38.403873 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m22:17:38.404264 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m22:17:38.409137 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m22:17:38.430948 [debug] [MainThread]: Parsing macros/materializations/incremental/column_helpers.sql
[0m22:17:38.432516 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
[0m22:17:38.436310 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m22:17:38.442905 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m22:17:38.481132 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m22:17:38.481615 [debug] [MainThread]: Parsing macros/utils/assert_not_null.sql
[0m22:17:38.482537 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m22:17:38.486884 [debug] [MainThread]: Parsing macros/utils/timestamps.sql
[0m22:17:38.487141 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m22:17:38.488385 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m22:17:38.499185 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m22:17:38.499512 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m22:17:38.499869 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m22:17:38.500187 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m22:17:38.501117 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m22:17:38.501477 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m22:17:38.501889 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m22:17:38.504589 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m22:17:38.506214 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m22:17:38.507391 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m22:17:38.519450 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m22:17:38.529884 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m22:17:38.539427 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m22:17:38.542711 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m22:17:38.543976 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m22:17:38.545243 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m22:17:38.551160 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m22:17:38.563589 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m22:17:38.564667 [debug] [MainThread]: Parsing macros/materializations/models/incremental/strategies.sql
[0m22:17:38.569562 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m22:17:38.577362 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m22:17:38.590890 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m22:17:38.594948 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m22:17:38.597431 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m22:17:38.601513 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m22:17:38.602402 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m22:17:38.604827 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m22:17:38.606407 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m22:17:38.611595 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m22:17:38.625962 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m22:17:38.626998 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m22:17:38.628744 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m22:17:38.629838 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m22:17:38.630462 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m22:17:38.631013 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m22:17:38.631480 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m22:17:38.632456 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m22:17:38.636269 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m22:17:38.642543 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m22:17:38.643114 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m22:17:38.643974 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m22:17:38.644635 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m22:17:38.645285 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m22:17:38.646166 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m22:17:38.646726 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m22:17:38.647441 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m22:17:38.648292 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m22:17:38.649999 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m22:17:38.650868 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m22:17:38.651614 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m22:17:38.652360 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m22:17:38.653078 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m22:17:38.653711 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m22:17:38.654479 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m22:17:38.655103 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m22:17:38.660997 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m22:17:38.661734 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m22:17:38.662381 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m22:17:38.663631 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m22:17:38.665351 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m22:17:38.666065 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m22:17:38.667092 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m22:17:38.667810 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m22:17:38.669260 [debug] [MainThread]: Parsing macros/adapters/timestamps.sql
[0m22:17:38.671543 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m22:17:38.673479 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m22:17:38.684328 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m22:17:38.685708 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m22:17:38.695399 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m22:17:38.698490 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m22:17:38.703635 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m22:17:38.710723 [debug] [MainThread]: Parsing macros/python_model/python.sql
[0m22:17:38.715119 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m22:17:38.975182 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_customers.sql
[0m22:17:38.984318 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_payments.sql
[0m22:17:38.986475 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_orders.sql
[0m22:17:39.128173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e8b5baee-d921-4b43-9657-e6abe20d7958', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c9350d0>]}
[0m22:17:39.136284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e8b5baee-d921-4b43-9657-e6abe20d7958', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c69a8b0>]}
[0m22:17:39.136499 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m22:17:39.136686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e8b5baee-d921-4b43-9657-e6abe20d7958', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c89a280>]}
[0m22:17:39.138537 [info ] [MainThread]: 
[0m22:17:39.139101 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:17:39.140135 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m22:17:39.149208 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m22:17:39.149384 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m22:17:39.149472 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:17:42.781453 [debug] [ThreadPool]: SQL status: OK in 3.63 seconds
[0m22:17:42.819264 [debug] [ThreadPool]: On list_schemas: Close
[0m22:17:43.996773 [debug] [ThreadPool]: Acquiring new databricks connection "create__bruno"
[0m22:17:43.998757 [debug] [ThreadPool]: Acquiring new databricks connection "create__bruno"
[0m22:17:43.999517 [debug] [ThreadPool]: Creating schema "_ReferenceKey(database=None, schema='bruno', identifier=None)"
[0m22:17:44.011195 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:17:44.011509 [debug] [ThreadPool]: Using databricks connection "create__bruno"
[0m22:17:44.011736 [debug] [ThreadPool]: On create__bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "create__bruno"} */
create schema if not exists bruno
  
[0m22:17:44.011939 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:17:47.376498 [debug] [ThreadPool]: SQL status: OK in 3.36 seconds
[0m22:17:47.380354 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m22:17:47.380962 [debug] [ThreadPool]: On create__bruno: ROLLBACK
[0m22:17:47.381242 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:17:47.381495 [debug] [ThreadPool]: On create__bruno: Close
[0m22:17:48.594603 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_bruno"
[0m22:17:48.610698 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:17:48.610985 [debug] [ThreadPool]: Using databricks connection "list_None_bruno"
[0m22:17:48.611222 [debug] [ThreadPool]: On list_None_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_bruno"} */
show table extended in bruno like '*'
  
[0m22:17:48.611421 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:17:51.267493 [debug] [ThreadPool]: SQL status: OK in 2.66 seconds
[0m22:17:51.275207 [debug] [ThreadPool]: On list_None_bruno: ROLLBACK
[0m22:17:51.275683 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:17:51.275962 [debug] [ThreadPool]: On list_None_bruno: Close
[0m22:17:52.504570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e8b5baee-d921-4b43-9657-e6abe20d7958', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c935370>]}
[0m22:17:52.506093 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:17:52.506407 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:17:52.507720 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:17:52.508702 [info ] [MainThread]: 
[0m22:17:52.517661 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_customers
[0m22:17:52.518275 [info ] [Thread-1  ]: 1 of 14 START seed file bruno.raw_customers .................................... [RUN]
[0m22:17:52.519550 [debug] [Thread-1  ]: Acquiring new databricks connection "seed.jaffle_shop.raw_customers"
[0m22:17:52.519796 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_customers
[0m22:17:52.520057 [debug] [Thread-1  ]: finished collecting timing info
[0m22:17:52.520275 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_customers
[0m22:17:52.592249 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m22:17:52.592428 [debug] [Thread-1  ]: Using databricks connection "seed.jaffle_shop.raw_customers"
[0m22:17:52.592559 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_customers"} */

    create table bruno.raw_customers (`id` bigint,`first_name` string,`last_name` string)
    
    using delta
    
    
    
    
    
  
[0m22:17:52.592656 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:18:15.315479 [debug] [Thread-1  ]: SQL status: OK in 22.72 seconds
[0m22:18:16.743591 [debug] [Thread-1  ]: Using databricks connection "seed.jaffle_shop.raw_customers"
[0m22:18:16.743916 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: 
          insert overwrite bruno.raw_customers values
          (cast(%s as bigint),cast(%s as string),cast(%s as string)),(cast(%s as bigint),cast(%s as string),cast(%s as string)),(cast(%s as bigint),cast(%s as string),cast(%s as string)),(cast(%s as bigint),cast(%s as string),cast(%s as string)),(cast(%s as bigint),cast(%s as string),cast(%s as string)),(cast(%s as bigint),cast(%s as string),cast(%s as string)),(cast(%s as bigint),cast(%s as string),cast(%s as string)),(cast(%s as bigint),cast(%s as str...
[0m22:18:42.263823 [debug] [Thread-1  ]: SQL status: OK in 25.52 seconds
[0m22:18:43.417165 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_customers"
[0m22:18:43.452256 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m22:18:43.453935 [debug] [Thread-1  ]: finished collecting timing info
[0m22:18:43.454175 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: ROLLBACK
[0m22:18:43.454344 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m22:18:43.454481 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_customers: Close
[0m22:18:44.621185 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8b5baee-d921-4b43-9657-e6abe20d7958', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ca10d30>]}
[0m22:18:44.623540 [info ] [Thread-1  ]: 1 of 14 OK loaded seed file bruno.raw_customers ................................ [[32mINSERT 100[0m in 52.10s]
[0m22:18:44.627565 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_customers
[0m22:18:44.629070 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_orders
[0m22:18:44.631970 [info ] [Thread-1  ]: 2 of 14 START seed file bruno.raw_orders ....................................... [RUN]
[0m22:18:44.635389 [debug] [Thread-1  ]: Acquiring new databricks connection "seed.jaffle_shop.raw_orders"
[0m22:18:44.636203 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_orders
[0m22:18:44.636798 [debug] [Thread-1  ]: finished collecting timing info
[0m22:18:44.637400 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_orders
[0m22:18:44.667217 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m22:18:44.667531 [debug] [Thread-1  ]: Using databricks connection "seed.jaffle_shop.raw_orders"
[0m22:18:44.667762 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_orders"} */

    create table bruno.raw_orders (`id` bigint,`user_id` bigint,`order_date` date,`status` string)
    
    using delta
    
    
    
    
    
  
[0m22:18:44.667965 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:18:57.107343 [debug] [Thread-1  ]: SQL status: OK in 12.44 seconds
[0m22:18:58.432551 [debug] [Thread-1  ]: Using databricks connection "seed.jaffle_shop.raw_orders"
[0m22:18:58.432911 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: 
          insert overwrite bruno.raw_orders values
          (cast(%s as bigint),cast(%s as bigint),cast(%s as date),cast(%s as string)),(cast(%s as bigint),cast(%s as bigint),cast(%s as date),cast(%s as string)),(cast(%s as bigint),cast(%s as bigint),cast(%s as date),cast(%s as string)),(cast(%s as bigint),cast(%s as bigint),cast(%s as date),cast(%s as string)),(cast(%s as bigint),cast(%s as bigint),cast(%s as date),cast(%s as string)),(cast(%s as bigint),cast(%s as bigint),cast(%s as date),cast(%s as str...
[0m22:19:18.942544 [debug] [Thread-1  ]: SQL status: OK in 20.51 seconds
[0m22:19:19.983105 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_orders"
[0m22:19:19.999629 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m22:19:20.001345 [debug] [Thread-1  ]: finished collecting timing info
[0m22:19:20.001736 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: ROLLBACK
[0m22:19:20.002010 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m22:19:20.002281 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_orders: Close
[0m22:19:21.274581 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8b5baee-d921-4b43-9657-e6abe20d7958', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c9f7970>]}
[0m22:19:21.276347 [info ] [Thread-1  ]: 2 of 14 OK loaded seed file bruno.raw_orders ................................... [[32mINSERT 99[0m in 36.64s]
[0m22:19:21.278047 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_orders
[0m22:19:21.278577 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_payments
[0m22:19:21.279735 [info ] [Thread-1  ]: 3 of 14 START seed file bruno.raw_payments ..................................... [RUN]
[0m22:19:21.281714 [debug] [Thread-1  ]: Acquiring new databricks connection "seed.jaffle_shop.raw_payments"
[0m22:19:21.282288 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_payments
[0m22:19:21.282618 [debug] [Thread-1  ]: finished collecting timing info
[0m22:19:21.282918 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_payments
[0m22:19:21.313656 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m22:19:21.314032 [debug] [Thread-1  ]: Using databricks connection "seed.jaffle_shop.raw_payments"
[0m22:19:21.314284 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "seed.jaffle_shop.raw_payments"} */

    create table bruno.raw_payments (`id` bigint,`order_id` bigint,`payment_method` string,`amount` bigint)
    
    using delta
    
    
    
    
    
  
[0m22:19:21.314493 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:19:26.192381 [debug] [Thread-1  ]: SQL status: OK in 4.88 seconds
[0m22:19:26.421855 [debug] [Thread-1  ]: Using databricks connection "seed.jaffle_shop.raw_payments"
[0m22:19:26.422249 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: 
          insert overwrite bruno.raw_payments values
          (cast(%s as bigint),cast(%s as bigint),cast(%s as string),cast(%s as bigint)),(cast(%s as bigint),cast(%s as bigint),cast(%s as string),cast(%s as bigint)),(cast(%s as bigint),cast(%s as bigint),cast(%s as string),cast(%s as bigint)),(cast(%s as bigint),cast(%s as bigint),cast(%s as string),cast(%s as bigint)),(cast(%s as bigint),cast(%s as bigint),cast(%s as string),cast(%s as bigint)),(cast(%s as bigint),cast(%s as bigint),cast(%s as string),...
[0m22:19:47.021156 [debug] [Thread-1  ]: SQL status: OK in 20.6 seconds
[0m22:19:48.152383 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_payments"
[0m22:19:48.164054 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m22:19:48.167305 [debug] [Thread-1  ]: finished collecting timing info
[0m22:19:48.167869 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: ROLLBACK
[0m22:19:48.168210 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m22:19:48.168526 [debug] [Thread-1  ]: On seed.jaffle_shop.raw_payments: Close
[0m22:19:49.268918 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8b5baee-d921-4b43-9657-e6abe20d7958', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c9f7ac0>]}
[0m22:19:49.270035 [info ] [Thread-1  ]: 3 of 14 OK loaded seed file bruno.raw_payments ................................. [[32mINSERT 113[0m in 27.99s]
[0m22:19:49.271084 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_payments
[0m22:19:49.271655 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m22:19:49.272958 [info ] [Thread-1  ]: 4 of 14 START sql view model bruno.stg_customers ............................... [RUN]
[0m22:19:49.275567 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m22:19:49.276110 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m22:19:49.276723 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m22:19:49.288313 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m22:19:49.290100 [debug] [Thread-1  ]: finished collecting timing info
[0m22:19:49.290669 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m22:19:49.313137 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_customers"
[0m22:19:49.315188 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m22:19:49.315356 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m22:19:49.315523 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view bruno.stg_customers
  
  
  as
    with source as (
    select * from bruno.raw_customers

),

renamed as (

    select
        id as customer_id,
        first_name,
        last_name

    from source

)

select * from renamed

[0m22:19:49.315660 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:19:52.146325 [debug] [Thread-1  ]: SQL status: OK in 2.83 seconds
[0m22:19:52.155571 [debug] [Thread-1  ]: finished collecting timing info
[0m22:19:52.156339 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m22:19:52.156725 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m22:19:52.157042 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m22:19:53.219947 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8b5baee-d921-4b43-9657-e6abe20d7958', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c9f7bb0>]}
[0m22:19:53.221088 [info ] [Thread-1  ]: 4 of 14 OK created sql view model bruno.stg_customers .......................... [[32mOK[0m in 3.95s]
[0m22:19:53.222239 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m22:19:53.222878 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m22:19:53.223951 [info ] [Thread-1  ]: 5 of 14 START sql view model bruno.stg_orders .................................. [RUN]
[0m22:19:53.225957 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m22:19:53.226466 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m22:19:53.226882 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m22:19:53.243600 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m22:19:53.245648 [debug] [Thread-1  ]: finished collecting timing info
[0m22:19:53.245994 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m22:19:53.254550 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_orders"
[0m22:19:53.255651 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m22:19:53.255914 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m22:19:53.256165 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view bruno.stg_orders
  
  
  as
    with source as (
    select * from bruno.raw_orders

),

renamed as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from source

)

select * from renamed

[0m22:19:53.256375 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:19:56.160879 [debug] [Thread-1  ]: SQL status: OK in 2.9 seconds
[0m22:19:56.166003 [debug] [Thread-1  ]: finished collecting timing info
[0m22:19:56.166523 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m22:19:56.166938 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m22:19:56.167303 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m22:19:57.224227 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8b5baee-d921-4b43-9657-e6abe20d7958', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c9e1eb0>]}
[0m22:19:57.225467 [info ] [Thread-1  ]: 5 of 14 OK created sql view model bruno.stg_orders ............................. [[32mOK[0m in 4.00s]
[0m22:19:57.226847 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m22:19:57.227376 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m22:19:57.229307 [info ] [Thread-1  ]: 6 of 14 START sql view model bruno.stg_payments ................................ [RUN]
[0m22:19:57.230854 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m22:19:57.231270 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m22:19:57.231599 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m22:19:57.240303 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m22:19:57.242023 [debug] [Thread-1  ]: finished collecting timing info
[0m22:19:57.242592 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m22:19:57.253283 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_payments"
[0m22:19:57.254584 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m22:19:57.254909 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m22:19:57.255222 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view bruno.stg_payments
  
  
  as
    with source as (
    select * from bruno.raw_payments

),

renamed as (

    select
        id as payment_id,
        order_id,
        payment_method,

        -- `amount` is currently stored in cents, so we convert it to dollars
        amount / 100 as amount

    from source

)

select * from renamed

[0m22:19:57.255473 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:20:00.481913 [debug] [Thread-1  ]: SQL status: OK in 3.23 seconds
[0m22:20:00.486025 [debug] [Thread-1  ]: finished collecting timing info
[0m22:20:00.486486 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m22:20:00.486757 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m22:20:00.487016 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m22:20:01.515981 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8b5baee-d921-4b43-9657-e6abe20d7958', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12cb6fc40>]}
[0m22:20:01.516834 [info ] [Thread-1  ]: 6 of 14 OK created sql view model bruno.stg_payments ........................... [[32mOK[0m in 4.29s]
[0m22:20:01.517619 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m22:20:01.518042 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m22:20:01.518760 [info ] [Thread-1  ]: 7 of 14 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m22:20:01.520839 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m22:20:01.521565 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m22:20:01.522063 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m22:20:01.544927 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m22:20:01.545747 [debug] [Thread-1  ]: finished collecting timing info
[0m22:20:01.545946 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m22:20:01.564661 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m22:20:01.565329 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m22:20:01.565490 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m22:20:01.565654 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from bruno.stg_customers
where customer_id is null



      
    ) dbt_internal_test
[0m22:20:01.565790 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:20:07.791308 [debug] [Thread-1  ]: SQL status: OK in 6.23 seconds
[0m22:20:07.817246 [debug] [Thread-1  ]: finished collecting timing info
[0m22:20:07.817785 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: ROLLBACK
[0m22:20:07.818049 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m22:20:07.818237 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m22:20:08.836685 [info ] [Thread-1  ]: 7 of 14 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 7.32s]
[0m22:20:08.839564 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m22:20:08.840389 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m22:20:08.841293 [info ] [Thread-1  ]: 8 of 14 START test unique_stg_customers_customer_id ............................ [RUN]
[0m22:20:08.843950 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m22:20:08.844522 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m22:20:08.845195 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m22:20:08.865841 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m22:20:08.867181 [debug] [Thread-1  ]: finished collecting timing info
[0m22:20:08.867326 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m22:20:08.874164 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m22:20:08.874672 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m22:20:08.874782 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m22:20:08.874898 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from bruno.stg_customers
where customer_id is not null
group by customer_id
having count(*) > 1



      
    ) dbt_internal_test
[0m22:20:08.875017 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:20:16.041606 [debug] [Thread-1  ]: SQL status: OK in 7.17 seconds
[0m22:20:16.047530 [debug] [Thread-1  ]: finished collecting timing info
[0m22:20:16.048063 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: ROLLBACK
[0m22:20:16.048336 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m22:20:16.048593 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: Close
[0m22:20:17.287928 [info ] [Thread-1  ]: 8 of 14 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 8.45s]
[0m22:20:17.290167 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m22:20:17.291134 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m22:20:17.291974 [info ] [Thread-1  ]: 9 of 14 START test accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned  [RUN]
[0m22:20:17.294307 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m22:20:17.294916 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m22:20:17.295288 [debug] [Thread-1  ]: Compiling test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m22:20:17.321162 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m22:20:17.322180 [debug] [Thread-1  ]: finished collecting timing info
[0m22:20:17.322451 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m22:20:17.327062 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m22:20:17.327717 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m22:20:17.327943 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m22:20:17.328197 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from bruno.stg_orders
    group by status

)

select *
from all_values
where value_field not in (
    'placed','shipped','completed','return_pending','returned'
)



      
    ) dbt_internal_test
[0m22:20:17.328400 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:20:24.097312 [debug] [Thread-1  ]: SQL status: OK in 6.77 seconds
[0m22:20:24.109835 [debug] [Thread-1  ]: finished collecting timing info
[0m22:20:24.110518 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad: ROLLBACK
[0m22:20:24.110872 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m22:20:24.111206 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad: Close
[0m22:20:25.024326 [info ] [Thread-1  ]: 9 of 14 PASS accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned  [[32mPASS[0m in 7.73s]
[0m22:20:25.026343 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m22:20:25.027213 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m22:20:25.028185 [info ] [Thread-1  ]: 10 of 14 START test not_null_stg_orders_order_id ............................... [RUN]
[0m22:20:25.030678 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m22:20:25.031282 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m22:20:25.031750 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m22:20:25.050932 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m22:20:25.053300 [debug] [Thread-1  ]: finished collecting timing info
[0m22:20:25.053835 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m22:20:25.059637 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m22:20:25.060547 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m22:20:25.060831 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m22:20:25.061158 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from bruno.stg_orders
where order_id is null



      
    ) dbt_internal_test
[0m22:20:25.061410 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:20:27.733348 [debug] [Thread-1  ]: SQL status: OK in 2.67 seconds
[0m22:20:27.744593 [debug] [Thread-1  ]: finished collecting timing info
[0m22:20:27.745378 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: ROLLBACK
[0m22:20:27.745680 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m22:20:27.745944 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: Close
[0m22:20:28.671788 [info ] [Thread-1  ]: 10 of 14 PASS not_null_stg_orders_order_id ..................................... [[32mPASS[0m in 3.64s]
[0m22:20:28.672743 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m22:20:28.673140 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m22:20:28.673556 [info ] [Thread-1  ]: 11 of 14 START test unique_stg_orders_order_id ................................. [RUN]
[0m22:20:28.674838 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m22:20:28.675214 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m22:20:28.675501 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m22:20:28.692080 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m22:20:28.694169 [debug] [Thread-1  ]: finished collecting timing info
[0m22:20:28.694436 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m22:20:28.697348 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m22:20:28.698062 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m22:20:28.698224 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m22:20:28.698347 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    order_id as unique_field,
    count(*) as n_records

from bruno.stg_orders
where order_id is not null
group by order_id
having count(*) > 1



      
    ) dbt_internal_test
[0m22:20:28.698451 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:20:32.369753 [debug] [Thread-1  ]: SQL status: OK in 3.67 seconds
[0m22:20:32.385453 [debug] [Thread-1  ]: finished collecting timing info
[0m22:20:32.386249 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: ROLLBACK
[0m22:20:32.386857 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m22:20:32.387699 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: Close
[0m22:20:33.617194 [info ] [Thread-1  ]: 11 of 14 PASS unique_stg_orders_order_id ....................................... [[32mPASS[0m in 4.94s]
[0m22:20:33.619940 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m22:20:33.620868 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m22:20:33.622171 [info ] [Thread-1  ]: 12 of 14 START test accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card  [RUN]
[0m22:20:33.624991 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m22:20:33.625862 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m22:20:33.626581 [debug] [Thread-1  ]: Compiling test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m22:20:33.654768 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m22:20:33.656251 [debug] [Thread-1  ]: finished collecting timing info
[0m22:20:33.656521 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m22:20:33.663357 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m22:20:33.664193 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m22:20:33.664394 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m22:20:33.664616 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        payment_method as value_field,
        count(*) as n_records

    from bruno.stg_payments
    group by payment_method

)

select *
from all_values
where value_field not in (
    'credit_card','coupon','bank_transfer','gift_card'
)



      
    ) dbt_internal_test
[0m22:20:33.664808 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:20:36.942707 [debug] [Thread-1  ]: SQL status: OK in 3.28 seconds
[0m22:20:36.953691 [debug] [Thread-1  ]: finished collecting timing info
[0m22:20:36.954393 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278: ROLLBACK
[0m22:20:36.954761 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m22:20:36.955026 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278: Close
[0m22:20:37.913898 [info ] [Thread-1  ]: 12 of 14 PASS accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card  [[32mPASS[0m in 4.29s]
[0m22:20:37.916294 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m22:20:37.917133 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m22:20:37.918167 [info ] [Thread-1  ]: 13 of 14 START test not_null_stg_payments_payment_id ........................... [RUN]
[0m22:20:37.920747 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m22:20:37.921491 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m22:20:37.922097 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m22:20:37.942753 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m22:20:37.943793 [debug] [Thread-1  ]: finished collecting timing info
[0m22:20:37.944121 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m22:20:37.949617 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m22:20:37.950387 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m22:20:37.950661 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m22:20:37.950951 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select payment_id
from bruno.stg_payments
where payment_id is null



      
    ) dbt_internal_test
[0m22:20:37.951177 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:20:40.635655 [debug] [Thread-1  ]: SQL status: OK in 2.68 seconds
[0m22:20:40.645978 [debug] [Thread-1  ]: finished collecting timing info
[0m22:20:40.646595 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075: ROLLBACK
[0m22:20:40.647009 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m22:20:40.647429 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075: Close
[0m22:20:41.866968 [info ] [Thread-1  ]: 13 of 14 PASS not_null_stg_payments_payment_id ................................. [[32mPASS[0m in 3.95s]
[0m22:20:41.868535 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m22:20:41.869059 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m22:20:41.869622 [info ] [Thread-1  ]: 14 of 14 START test unique_stg_payments_payment_id ............................. [RUN]
[0m22:20:41.871424 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m22:20:41.872078 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m22:20:41.872407 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m22:20:41.895960 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m22:20:41.897468 [debug] [Thread-1  ]: finished collecting timing info
[0m22:20:41.897852 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m22:20:41.903880 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m22:20:41.905109 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m22:20:41.905412 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m22:20:41.905721 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_payments_payment_id.3744510712: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    payment_id as unique_field,
    count(*) as n_records

from bruno.stg_payments
where payment_id is not null
group by payment_id
having count(*) > 1



      
    ) dbt_internal_test
[0m22:20:41.905974 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:20:45.138858 [debug] [Thread-1  ]: SQL status: OK in 3.23 seconds
[0m22:20:45.146502 [debug] [Thread-1  ]: finished collecting timing info
[0m22:20:45.147245 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_payments_payment_id.3744510712: ROLLBACK
[0m22:20:45.147542 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m22:20:45.147916 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_payments_payment_id.3744510712: Close
[0m22:20:46.367558 [info ] [Thread-1  ]: 14 of 14 PASS unique_stg_payments_payment_id ................................... [[32mPASS[0m in 4.50s]
[0m22:20:46.369960 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m22:20:46.376946 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:20:46.378067 [debug] [MainThread]: On master: ROLLBACK
[0m22:20:46.378488 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:20:47.525537 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:20:47.528157 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:20:47.528719 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:20:47.529946 [debug] [MainThread]: On master: ROLLBACK
[0m22:20:47.530367 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:20:47.530665 [debug] [MainThread]: On master: Close
[0m22:20:48.531708 [info ] [MainThread]: 
[0m22:20:48.533386 [info ] [MainThread]: Finished running 3 seeds, 3 view models, 8 tests in 0 hours 3 minutes and 9.39 seconds (189.39s).
[0m22:20:48.535064 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:20:48.535469 [debug] [MainThread]: Connection 'test.jaffle_shop.unique_stg_payments_payment_id.3744510712' was properly closed.
[0m22:20:48.572726 [info ] [MainThread]: 
[0m22:20:48.573358 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:20:48.573929 [info ] [MainThread]: 
[0m22:20:48.574320 [info ] [MainThread]: Done. PASS=14 WARN=0 ERROR=0 SKIP=0 TOTAL=14
[0m22:20:48.575643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c935520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c9354f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c9b9220>]}
[0m22:20:48.576324 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 22:30:02.162638 | ee637751-a501-41cd-b356-9584aa51b44c ==============================
[0m22:30:02.162734 [info ] [MainThread]: Running with dbt=1.3.2
[0m22:30:02.163685 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:30:02.163843 [debug] [MainThread]: Tracking: tracking
[0m22:30:02.192492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ee9b8e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ee9bbe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12eeac670>]}
[0m22:30:02.252722 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:30:02.253115 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m22:30:02.343090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ee637751-a501-41cd-b356-9584aa51b44c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fc9d160>]}
[0m22:30:02.351048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ee637751-a501-41cd-b356-9584aa51b44c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f334580>]}
[0m22:30:02.351251 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m22:30:02.351436 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ee637751-a501-41cd-b356-9584aa51b44c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1109ff3a0>]}
[0m22:30:02.352552 [info ] [MainThread]: 
[0m22:30:02.353026 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:30:02.353786 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m22:30:02.362407 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m22:30:02.362594 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m22:30:02.362685 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:30:05.201279 [debug] [ThreadPool]: SQL status: OK in 2.84 seconds
[0m22:30:05.227355 [debug] [ThreadPool]: On list_schemas: Close
[0m22:30:06.520561 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_bruno"
[0m22:30:06.535267 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:30:06.535583 [debug] [ThreadPool]: Using databricks connection "list_None_bruno"
[0m22:30:06.535824 [debug] [ThreadPool]: On list_None_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_bruno"} */
show table extended in bruno like '*'
  
[0m22:30:06.536029 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:30:09.604007 [debug] [ThreadPool]: SQL status: OK in 3.07 seconds
[0m22:30:09.614474 [debug] [ThreadPool]: On list_None_bruno: ROLLBACK
[0m22:30:09.614839 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:30:09.615104 [debug] [ThreadPool]: On list_None_bruno: Close
[0m22:30:10.825415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ee637751-a501-41cd-b356-9584aa51b44c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f069e80>]}
[0m22:30:10.826636 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:30:10.826963 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:30:10.828587 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:30:10.829448 [info ] [MainThread]: 
[0m22:30:10.841856 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m22:30:10.842468 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m22:30:10.843641 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.customers"
[0m22:30:10.843893 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m22:30:10.844144 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m22:30:10.873611 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m22:30:10.875353 [debug] [Thread-1  ]: finished collecting timing info
[0m22:30:10.875557 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m22:30:10.900063 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m22:30:10.900872 [debug] [Thread-1  ]: On model.jaffle_shop.customers: 
  
    
import pyspark.sql.functions as F

def model(dbt, session):

    stg_customers_df = dbt.ref('stg_customers')
    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    customer_orders_df = (
        stg_orders_df
        .groupby("customer_id")
        .agg(
            F.min(F.col("order_date")).alias('first_order'),
            F.max(F.col("order_date")).alias('most_recent_order'),
            F.count(F.col("order_id")).alias('number_of_orders')
        )
    )

    customer_payments_df = (
        stg_payments_df
        .join(stg_orders_df, stg_payments_df.order_id == stg_orders_df.order_id, "left")
        .group_by(stg_orders_df.customer_id)
        .agg(
            F.sum(F.col("amount")).alias('total_amount')
        )
    )

    final_df = (
        stg_customers_df
        .join(customer_orders_df, stg_customers_df.customer_id == customer_orders_df.customer_id, "left")
        .join(customer_payments_df, stg_customers_df.customer_id == customer_payments_df.customer_id, "left")
        .select(stg_customers_df.customer_id.alias("customer_id"),
                stg_customers_df.first_name.alias("first_name"),
                stg_customers_df.last_name.alias("last_name"),
                customer_orders_df.first_order.alias("first_order"),
                customer_orders_df.most_recent_order.alias("most_recent_order"),
                customer_orders_df.number_of_orders.alias("number_of_orders"),
                customer_payments_df.total_amount.alias("customer_lifetime_value")
        )
    )

    return final_df



# stg_customers_df = spark.table("stg_customers")
# stg_orders_df = spark.table("stg_orders")
# stg_payments_df = spark.table("stg_payments")

# # customer_orders
# customer_orders_df = stg_orders_df.groupBy("customer_id") \
#                                   .agg(min("order_date").alias("first_order"), 
#                                        max("order_date").alias("most_recent_order"), 
#                                        count("order_id").alias("number_of_orders"))

# # customer_payments
# customer_payments_df = stg_payments_df.join(stg_orders_df, stg_payments_df["order_id"] == stg_orders_df["order_id"], "left") \
#                                       .groupBy("customer_id") \
#                                       .agg(sum("amount").alias("total_amount"))

# # final
# final_df = stg_customers_df.join(customer_orders_df, stg_customers_df["customer_id"] == customer_orders_df["customer_id"], "left") \
#                            .join(customer_payments_df, stg_customers_df["customer_id"] == customer_payments_df["customer_id"], "left") \
#                            .selectExpr("customer_id", "first_name", "last_name", "first_order", "most_recent_order", "number_of_orders", "total_amount as customer_lifetime_value")

# final_df.show()


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "bruno.stg_customers", "stg_orders": "bruno.stg_orders", "stg_payments": "bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'None'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist
import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write.mode("overwrite").format("delta").option("overwriteSchema", "true").saveAsTable("bruno.customers")
  
[0m22:30:10.901124 [debug] [Thread-1  ]: finished collecting timing info
[0m22:30:10.901591 [error] [Thread-1  ]: [31mUnhandled error while executing model.jaffle_shop.customers[0m
Databricks `http_path` or `cluster_id` of an all-purpose cluster is required for the `all_purpose_cluster` submission method.
[0m22:30:10.901828 [debug] [Thread-1  ]: 
[0m22:30:10.902020 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee637751-a501-41cd-b356-9584aa51b44c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f15b310>]}
[0m22:30:10.902312 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 0.06s]
[0m22:30:10.902689 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m22:30:10.903628 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:30:10.903790 [debug] [MainThread]: On master: ROLLBACK
[0m22:30:10.903899 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:30:12.145126 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:30:12.146238 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:30:12.146762 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:30:12.147151 [debug] [MainThread]: On master: ROLLBACK
[0m22:30:12.147473 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:30:12.147787 [debug] [MainThread]: On master: Close
[0m22:30:13.377467 [info ] [MainThread]: 
[0m22:30:13.378873 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 11.02 seconds (11.02s).
[0m22:30:13.379493 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:30:13.379810 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m22:30:13.403653 [info ] [MainThread]: 
[0m22:30:13.404217 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:30:13.404659 [info ] [MainThread]: 
[0m22:30:13.405032 [error] [MainThread]: [33mDatabricks `http_path` or `cluster_id` of an all-purpose cluster is required for the `all_purpose_cluster` submission method.[0m
[0m22:30:13.405403 [info ] [MainThread]: 
[0m22:30:13.405752 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:30:13.406222 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f069d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f16c1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f15bee0>]}
[0m22:30:13.406567 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 22:36:25.674044 | 783dd619-9029-4b8d-b84d-b708346b4340 ==============================
[0m22:36:25.674112 [info ] [MainThread]: Running with dbt=1.3.2
[0m22:36:25.675050 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:36:25.675226 [debug] [MainThread]: Tracking: tracking
[0m22:36:25.714314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e459880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e459b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e46c610>]}
[0m22:36:25.773383 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:36:25.773749 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m22:36:25.863309 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '783dd619-9029-4b8d-b84d-b708346b4340', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e6a35b0>]}
[0m22:36:25.870820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '783dd619-9029-4b8d-b84d-b708346b4340', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e5d13d0>]}
[0m22:36:25.871029 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m22:36:25.871216 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '783dd619-9029-4b8d-b84d-b708346b4340', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e612040>]}
[0m22:36:25.872310 [info ] [MainThread]: 
[0m22:36:25.872801 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:36:25.873587 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m22:36:25.882184 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m22:36:25.882372 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m22:36:25.882463 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:36:28.669253 [debug] [ThreadPool]: SQL status: OK in 2.79 seconds
[0m22:36:28.691431 [debug] [ThreadPool]: On list_schemas: Close
[0m22:36:30.038434 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_bruno"
[0m22:36:30.052572 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:36:30.052855 [debug] [ThreadPool]: Using databricks connection "list_None_bruno"
[0m22:36:30.053098 [debug] [ThreadPool]: On list_None_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_bruno"} */
show table extended in bruno like '*'
  
[0m22:36:30.053301 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:36:33.109159 [debug] [ThreadPool]: SQL status: OK in 3.06 seconds
[0m22:36:33.118521 [debug] [ThreadPool]: On list_None_bruno: ROLLBACK
[0m22:36:33.118863 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:36:33.119126 [debug] [ThreadPool]: On list_None_bruno: Close
[0m22:36:34.362837 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '783dd619-9029-4b8d-b84d-b708346b4340', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e5d8d60>]}
[0m22:36:34.364123 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:36:34.364466 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:36:34.365646 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:36:34.366250 [info ] [MainThread]: 
[0m22:36:34.379453 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m22:36:34.379993 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m22:36:34.381142 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.customers"
[0m22:36:34.381390 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m22:36:34.381637 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m22:36:34.412065 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m22:36:34.413844 [debug] [Thread-1  ]: finished collecting timing info
[0m22:36:34.414028 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m22:36:34.438326 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m22:36:34.439290 [debug] [Thread-1  ]: On model.jaffle_shop.customers: 
  
    
import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method="all_purpose_cluster",
        create_notebook=True,
        cluster_id="0124-220040-qfvx2ysq"
    )

    stg_customers_df = dbt.ref('stg_customers')
    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    customer_orders_df = (
        stg_orders_df
        .groupby("customer_id")
        .agg(
            F.min(F.col("order_date")).alias('first_order'),
            F.max(F.col("order_date")).alias('most_recent_order'),
            F.count(F.col("order_id")).alias('number_of_orders')
        )
    )

    customer_payments_df = (
        stg_payments_df
        .join(stg_orders_df, stg_payments_df.order_id == stg_orders_df.order_id, "left")
        .group_by(stg_orders_df.customer_id)
        .agg(
            F.sum(F.col("amount")).alias('total_amount')
        )
    )

    final_df = (
        stg_customers_df
        .join(customer_orders_df, stg_customers_df.customer_id == customer_orders_df.customer_id, "left")
        .join(customer_payments_df, stg_customers_df.customer_id == customer_payments_df.customer_id, "left")
        .select(stg_customers_df.customer_id.alias("customer_id"),
                stg_customers_df.first_name.alias("first_name"),
                stg_customers_df.last_name.alias("last_name"),
                customer_orders_df.first_order.alias("first_order"),
                customer_orders_df.most_recent_order.alias("most_recent_order"),
                customer_orders_df.number_of_orders.alias("number_of_orders"),
                customer_payments_df.total_amount.alias("customer_lifetime_value")
        )
    )

    return final_df



# stg_customers_df = spark.table("stg_customers")
# stg_orders_df = spark.table("stg_orders")
# stg_payments_df = spark.table("stg_payments")

# # customer_orders
# customer_orders_df = stg_orders_df.groupBy("customer_id") \
#                                   .agg(min("order_date").alias("first_order"), 
#                                        max("order_date").alias("most_recent_order"), 
#                                        count("order_id").alias("number_of_orders"))

# # customer_payments
# customer_payments_df = stg_payments_df.join(stg_orders_df, stg_payments_df["order_id"] == stg_orders_df["order_id"], "left") \
#                                       .groupBy("customer_id") \
#                                       .agg(sum("amount").alias("total_amount"))

# # final
# final_df = stg_customers_df.join(customer_orders_df, stg_customers_df["customer_id"] == customer_orders_df["customer_id"], "left") \
#                            .join(customer_payments_df, stg_customers_df["customer_id"] == customer_payments_df["customer_id"], "left") \
#                            .selectExpr("customer_id", "first_name", "last_name", "first_order", "most_recent_order", "number_of_orders", "total_amount as customer_lifetime_value")

# final_df.show()


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "bruno.stg_customers", "stg_orders": "bruno.stg_orders", "stg_payments": "bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'None'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist
import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write.mode("overwrite").format("delta").option("overwriteSchema", "true").saveAsTable("bruno.customers")
  
[0m22:37:03.322645 [debug] [Thread-1  ]: finished collecting timing info
[0m22:37:03.325833 [debug] [Thread-1  ]: Runtime Error in model customers (models/customers.py)
  Python model failed with traceback as:
  (Note that the line number here does not match the line number in your code due to dbt templating)
  [0;31m---------------------------------------------------------------------------[0m
  [0;31mAttributeError[0m                            Traceback (most recent call last)
  [0;32m<command-1437986966279486>[0m in [0;36m<cell line: 8>[0;34m()[0m
  [1;32m      6[0m [0;31m# --- Autogenerated dbt materialization code. --- #[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  [1;32m      7[0m [0mdbt[0m [0;34m=[0m [0mdbtObj[0m[0;34m([0m[0mspark[0m[0;34m.[0m[0mtable[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0;32m----> 8[0;31m [0mdf[0m [0;34m=[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0mspark[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m      9[0m [0;34m[0m[0m
  [1;32m     10[0m [0;31m# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  
  [0;32m<command-1437986966279484>[0m in [0;36mmodel[0;34m(dbt, session)[0m
  [1;32m     24[0m [0;34m[0m[0m
  [1;32m     25[0m     customer_payments_df = (
  [0;32m---> 26[0;31m         [0mstg_payments_df[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m     27[0m         [0;34m.[0m[0mjoin[0m[0;34m([0m[0mstg_orders_df[0m[0;34m,[0m [0mstg_payments_df[0m[0;34m.[0m[0morder_id[0m [0;34m==[0m [0mstg_orders_df[0m[0;34m.[0m[0morder_id[0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     28[0m         [0;34m.[0m[0mgroup_by[0m[0;34m([0m[0mstg_orders_df[0m[0;34m.[0m[0mcustomer_id[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  
  [0;32m/databricks/spark/python/pyspark/sql/dataframe.py[0m in [0;36m__getattr__[0;34m(self, name)[0m
  [1;32m   2010[0m         """
  [1;32m   2011[0m         [0;32mif[0m [0mname[0m [0;32mnot[0m [0;32min[0m [0mself[0m[0;34m.[0m[0mcolumns[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
  [0;32m-> 2012[0;31m             raise AttributeError(
  [0m[1;32m   2013[0m                 [0;34m"'%s' object has no attribute '%s'"[0m [0;34m%[0m [0;34m([0m[0mself[0m[0;34m.[0m[0m__class__[0m[0;34m.[0m[0m__name__[0m[0;34m,[0m [0mname[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [1;32m   2014[0m             )
  
  [0;31mAttributeError[0m: 'DataFrame' object has no attribute 'group_by'
[0m22:37:03.326575 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '783dd619-9029-4b8d-b84d-b708346b4340', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e71cd30>]}
[0m22:37:03.327351 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 28.95s]
[0m22:37:03.328392 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m22:37:03.330628 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:37:03.330984 [debug] [MainThread]: On master: ROLLBACK
[0m22:37:03.331244 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:37:04.540639 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:37:04.541197 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:37:04.541499 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:37:04.541814 [debug] [MainThread]: On master: ROLLBACK
[0m22:37:04.542079 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:37:04.542338 [debug] [MainThread]: On master: Close
[0m22:37:05.624819 [info ] [MainThread]: 
[0m22:37:05.626121 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 39.75 seconds (39.75s).
[0m22:37:05.626632 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:37:05.626934 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m22:37:05.651183 [info ] [MainThread]: 
[0m22:37:05.651678 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:37:05.652063 [info ] [MainThread]: 
[0m22:37:05.652384 [error] [MainThread]: [33mRuntime Error in model customers (models/customers.py)[0m
[0m22:37:05.652687 [error] [MainThread]:   Python model failed with traceback as:
[0m22:37:05.652978 [error] [MainThread]:   (Note that the line number here does not match the line number in your code due to dbt templating)
[0m22:37:05.653261 [error] [MainThread]:   [0;31m---------------------------------------------------------------------------[0m
[0m22:37:05.653541 [error] [MainThread]:   [0;31mAttributeError[0m                            Traceback (most recent call last)
[0m22:37:05.653820 [error] [MainThread]:   [0;32m<command-1437986966279486>[0m in [0;36m<cell line: 8>[0;34m()[0m
[0m22:37:05.654100 [error] [MainThread]:   [1;32m      6[0m [0;31m# --- Autogenerated dbt materialization code. --- #[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:05.654382 [error] [MainThread]:   [1;32m      7[0m [0mdbt[0m [0;34m=[0m [0mdbtObj[0m[0;34m([0m[0mspark[0m[0;34m.[0m[0mtable[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:05.654670 [error] [MainThread]:   [0;32m----> 8[0;31m [0mdf[0m [0;34m=[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0mspark[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:05.654947 [error] [MainThread]:   [0m[1;32m      9[0m [0;34m[0m[0m
[0m22:37:05.655224 [error] [MainThread]:   [1;32m     10[0m [0;31m# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:05.655499 [error] [MainThread]:   
[0m22:37:05.655777 [error] [MainThread]:   [0;32m<command-1437986966279484>[0m in [0;36mmodel[0;34m(dbt, session)[0m
[0m22:37:05.656051 [error] [MainThread]:   [1;32m     24[0m [0;34m[0m[0m
[0m22:37:05.656339 [error] [MainThread]:   [1;32m     25[0m     customer_payments_df = (
[0m22:37:05.656616 [error] [MainThread]:   [0;32m---> 26[0;31m         [0mstg_payments_df[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:05.656890 [error] [MainThread]:   [0m[1;32m     27[0m         [0;34m.[0m[0mjoin[0m[0;34m([0m[0mstg_orders_df[0m[0;34m,[0m [0mstg_payments_df[0m[0;34m.[0m[0morder_id[0m [0;34m==[0m [0mstg_orders_df[0m[0;34m.[0m[0morder_id[0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:05.657174 [error] [MainThread]:   [1;32m     28[0m         [0;34m.[0m[0mgroup_by[0m[0;34m([0m[0mstg_orders_df[0m[0;34m.[0m[0mcustomer_id[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:05.657450 [error] [MainThread]:   
[0m22:37:05.657751 [error] [MainThread]:   [0;32m/databricks/spark/python/pyspark/sql/dataframe.py[0m in [0;36m__getattr__[0;34m(self, name)[0m
[0m22:37:05.658026 [error] [MainThread]:   [1;32m   2010[0m         """
[0m22:37:05.658302 [error] [MainThread]:   [1;32m   2011[0m         [0;32mif[0m [0mname[0m [0;32mnot[0m [0;32min[0m [0mself[0m[0;34m.[0m[0mcolumns[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:05.658576 [error] [MainThread]:   [0;32m-> 2012[0;31m             raise AttributeError(
[0m22:37:05.658849 [error] [MainThread]:   [0m[1;32m   2013[0m                 [0;34m"'%s' object has no attribute '%s'"[0m [0;34m%[0m [0;34m([0m[0mself[0m[0;34m.[0m[0m__class__[0m[0;34m.[0m[0m__name__[0m[0;34m,[0m [0mname[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:05.659128 [error] [MainThread]:   [1;32m   2014[0m             )
[0m22:37:05.659406 [error] [MainThread]:   
[0m22:37:05.659881 [error] [MainThread]:   [0;31mAttributeError[0m: 'DataFrame' object has no attribute 'group_by'
[0m22:37:05.660306 [info ] [MainThread]: 
[0m22:37:05.660835 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:37:05.661365 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e5d10d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e72aaf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e6c6640>]}
[0m22:37:05.661705 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 22:37:21.315730 | 3287716a-08a3-478f-bc3b-00f7a8d61c10 ==============================
[0m22:37:21.315770 [info ] [MainThread]: Running with dbt=1.3.2
[0m22:37:21.316623 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:37:21.316777 [debug] [MainThread]: Tracking: tracking
[0m22:37:21.344261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12aae5850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12aae5b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12aaf95e0>]}
[0m22:37:21.405095 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:37:21.405540 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m22:37:21.497937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3287716a-08a3-478f-bc3b-00f7a8d61c10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ad2e610>]}
[0m22:37:21.505827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3287716a-08a3-478f-bc3b-00f7a8d61c10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b022910>]}
[0m22:37:21.506042 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m22:37:21.506225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3287716a-08a3-478f-bc3b-00f7a8d61c10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba252b0>]}
[0m22:37:21.507427 [info ] [MainThread]: 
[0m22:37:21.507946 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:37:21.508726 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m22:37:21.516878 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m22:37:21.517005 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m22:37:21.517095 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:37:24.021711 [debug] [ThreadPool]: SQL status: OK in 2.5 seconds
[0m22:37:24.046618 [debug] [ThreadPool]: On list_schemas: Close
[0m22:37:25.237793 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_bruno"
[0m22:37:25.252847 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:37:25.253241 [debug] [ThreadPool]: Using databricks connection "list_None_bruno"
[0m22:37:25.253488 [debug] [ThreadPool]: On list_None_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_bruno"} */
show table extended in bruno like '*'
  
[0m22:37:25.253708 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:37:28.411576 [debug] [ThreadPool]: SQL status: OK in 3.16 seconds
[0m22:37:28.422921 [debug] [ThreadPool]: On list_None_bruno: ROLLBACK
[0m22:37:28.423240 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:37:28.423463 [debug] [ThreadPool]: On list_None_bruno: Close
[0m22:37:29.635174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3287716a-08a3-478f-bc3b-00f7a8d61c10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ac63eb0>]}
[0m22:37:29.636601 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:37:29.637033 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:37:29.638665 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:37:29.639613 [info ] [MainThread]: 
[0m22:37:29.652294 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m22:37:29.652863 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m22:37:29.654030 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.customers"
[0m22:37:29.654283 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m22:37:29.654529 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m22:37:29.685114 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m22:37:29.686253 [debug] [Thread-1  ]: finished collecting timing info
[0m22:37:29.686448 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m22:37:29.711369 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m22:37:29.712300 [debug] [Thread-1  ]: On model.jaffle_shop.customers: 
  
    
import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method="all_purpose_cluster",
        create_notebook=True,
        cluster_id="0124-220040-qfvx2ysq"
    )

    stg_customers_df = dbt.ref('stg_customers')
    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    customer_orders_df = (
        stg_orders_df
        .groupby("customer_id")
        .agg(
            F.min(F.col("order_date")).alias('first_order'),
            F.max(F.col("order_date")).alias('most_recent_order'),
            F.count(F.col("order_id")).alias('number_of_orders')
        )
    )

    customer_payments_df = (
        stg_payments_df
        .join(stg_orders_df, stg_payments_df.order_id == stg_orders_df.order_id, "left")
        .groupby(stg_orders_df.customer_id)
        .agg(
            F.sum(F.col("amount")).alias('total_amount')
        )
    )

    final_df = (
        stg_customers_df
        .join(customer_orders_df, stg_customers_df.customer_id == customer_orders_df.customer_id, "left")
        .join(customer_payments_df, stg_customers_df.customer_id == customer_payments_df.customer_id, "left")
        .select(stg_customers_df.customer_id.alias("customer_id"),
                stg_customers_df.first_name.alias("first_name"),
                stg_customers_df.last_name.alias("last_name"),
                customer_orders_df.first_order.alias("first_order"),
                customer_orders_df.most_recent_order.alias("most_recent_order"),
                customer_orders_df.number_of_orders.alias("number_of_orders"),
                customer_payments_df.total_amount.alias("customer_lifetime_value")
        )
    )

    return final_df



# stg_customers_df = spark.table("stg_customers")
# stg_orders_df = spark.table("stg_orders")
# stg_payments_df = spark.table("stg_payments")

# # customer_orders
# customer_orders_df = stg_orders_df.groupBy("customer_id") \
#                                   .agg(min("order_date").alias("first_order"), 
#                                        max("order_date").alias("most_recent_order"), 
#                                        count("order_id").alias("number_of_orders"))

# # customer_payments
# customer_payments_df = stg_payments_df.join(stg_orders_df, stg_payments_df["order_id"] == stg_orders_df["order_id"], "left") \
#                                       .groupBy("customer_id") \
#                                       .agg(sum("amount").alias("total_amount"))

# # final
# final_df = stg_customers_df.join(customer_orders_df, stg_customers_df["customer_id"] == customer_orders_df["customer_id"], "left") \
#                            .join(customer_payments_df, stg_customers_df["customer_id"] == customer_payments_df["customer_id"], "left") \
#                            .selectExpr("customer_id", "first_name", "last_name", "first_order", "most_recent_order", "number_of_orders", "total_amount as customer_lifetime_value")

# final_df.show()


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "bruno.stg_customers", "stg_orders": "bruno.stg_orders", "stg_payments": "bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'None'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist
import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write.mode("overwrite").format("delta").option("overwriteSchema", "true").saveAsTable("bruno.customers")
  
[0m22:37:46.690519 [debug] [Thread-1  ]: finished collecting timing info
[0m22:37:46.692435 [debug] [Thread-1  ]: Runtime Error in model customers (models/customers.py)
  Python model failed with traceback as:
  (Note that the line number here does not match the line number in your code due to dbt templating)
  [0;31m---------------------------------------------------------------------------[0m
  [0;31mAnalysisException[0m                         Traceback (most recent call last)
  [0;32m<command-1437986966279489>[0m in [0;36m<cell line: 8>[0;34m()[0m
  [1;32m      6[0m [0;31m# --- Autogenerated dbt materialization code. --- #[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  [1;32m      7[0m [0mdbt[0m [0;34m=[0m [0mdbtObj[0m[0;34m([0m[0mspark[0m[0;34m.[0m[0mtable[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0;32m----> 8[0;31m [0mdf[0m [0;34m=[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0mspark[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m      9[0m [0;34m[0m[0m
  [1;32m     10[0m [0;31m# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  
  [0;32m<command-1437986966279487>[0m in [0;36mmodel[0;34m(dbt, session)[0m
  [1;32m     33[0m [0;34m[0m[0m
  [1;32m     34[0m     final_df = (
  [0;32m---> 35[0;31m         [0mstg_customers_df[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m     36[0m         [0;34m.[0m[0mjoin[0m[0;34m([0m[0mcustomer_orders_df[0m[0;34m,[0m [0mstg_customers_df[0m[0;34m.[0m[0mcustomer_id[0m [0;34m==[0m [0mcustomer_orders_df[0m[0;34m.[0m[0mcustomer_id[0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     37[0m         [0;34m.[0m[0mjoin[0m[0;34m([0m[0mcustomer_payments_df[0m[0;34m,[0m [0mstg_customers_df[0m[0;34m.[0m[0mcustomer_id[0m [0;34m==[0m [0mcustomer_payments_df[0m[0;34m.[0m[0mcustomer_id[0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  
  [0;32m/databricks/spark/python/pyspark/sql/dataframe.py[0m in [0;36mjoin[0;34m(self, other, on, how)[0m
  [1;32m   1561[0m                 [0mon[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jseq[0m[0;34m([0m[0;34m[[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [1;32m   1562[0m             [0;32massert[0m [0misinstance[0m[0;34m([0m[0mhow[0m[0;34m,[0m [0mstr[0m[0;34m)[0m[0;34m,[0m [0;34m"how should be a string"[0m[0;34m[0m[0;34m[0m[0m
  [0;32m-> 1563[0;31m             [0mjdf[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mjoin[0m[0;34m([0m[0mother[0m[0;34m.[0m[0m_jdf[0m[0;34m,[0m [0mon[0m[0;34m,[0m [0mhow[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m   1564[0m         [0;32mreturn[0m [0mDataFrame[0m[0;34m([0m[0mjdf[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0msparkSession[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [1;32m   1565[0m [0;34m[0m[0m
  
  [0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
  [1;32m   1319[0m [0;34m[0m[0m
  [1;32m   1320[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0;32m-> 1321[0;31m         return_value = get_return_value(
  [0m[1;32m   1322[0m             answer, self.gateway_client, self.target_id, self.name)
  [1;32m   1323[0m [0;34m[0m[0m
  
  [0;32m/databricks/spark/python/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
  [1;32m    200[0m                 [0;31m# Hide where the exception came from that shows a non-Pythonic[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  [1;32m    201[0m                 [0;31m# JVM exception message.[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  [0;32m--> 202[0;31m                 [0;32mraise[0m [0mconverted[0m [0;32mfrom[0m [0;32mNone[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m    203[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
  [1;32m    204[0m                 [0;32mraise[0m[0;34m[0m[0;34m[0m[0m
  
  [0;31mAnalysisException[0m:  Column customer_id#101L are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as("a").join(df.as("b"), $"a.id" > $"b.id")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.        
[0m22:37:46.693086 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3287716a-08a3-478f-bc3b-00f7a8d61c10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ada7d60>]}
[0m22:37:46.693852 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 17.04s]
[0m22:37:46.694869 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m22:37:46.697196 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:37:46.697552 [debug] [MainThread]: On master: ROLLBACK
[0m22:37:46.697804 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:37:47.964025 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:37:47.965435 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:37:47.965757 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:37:47.966130 [debug] [MainThread]: On master: ROLLBACK
[0m22:37:47.966437 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:37:47.966745 [debug] [MainThread]: On master: Close
[0m22:37:49.220860 [info ] [MainThread]: 
[0m22:37:49.222419 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 27.71 seconds (27.71s).
[0m22:37:49.222959 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:37:49.223233 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m22:37:49.245279 [info ] [MainThread]: 
[0m22:37:49.245906 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:37:49.246358 [info ] [MainThread]: 
[0m22:37:49.246744 [error] [MainThread]: [33mRuntime Error in model customers (models/customers.py)[0m
[0m22:37:49.247098 [error] [MainThread]:   Python model failed with traceback as:
[0m22:37:49.247429 [error] [MainThread]:   (Note that the line number here does not match the line number in your code due to dbt templating)
[0m22:37:49.247738 [error] [MainThread]:   [0;31m---------------------------------------------------------------------------[0m
[0m22:37:49.248015 [error] [MainThread]:   [0;31mAnalysisException[0m                         Traceback (most recent call last)
[0m22:37:49.248290 [error] [MainThread]:   [0;32m<command-1437986966279489>[0m in [0;36m<cell line: 8>[0;34m()[0m
[0m22:37:49.248569 [error] [MainThread]:   [1;32m      6[0m [0;31m# --- Autogenerated dbt materialization code. --- #[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:49.248845 [error] [MainThread]:   [1;32m      7[0m [0mdbt[0m [0;34m=[0m [0mdbtObj[0m[0;34m([0m[0mspark[0m[0;34m.[0m[0mtable[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:49.249124 [error] [MainThread]:   [0;32m----> 8[0;31m [0mdf[0m [0;34m=[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0mspark[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:49.249399 [error] [MainThread]:   [0m[1;32m      9[0m [0;34m[0m[0m
[0m22:37:49.249671 [error] [MainThread]:   [1;32m     10[0m [0;31m# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:49.249942 [error] [MainThread]:   
[0m22:37:49.250213 [error] [MainThread]:   [0;32m<command-1437986966279487>[0m in [0;36mmodel[0;34m(dbt, session)[0m
[0m22:37:49.250483 [error] [MainThread]:   [1;32m     33[0m [0;34m[0m[0m
[0m22:37:49.250755 [error] [MainThread]:   [1;32m     34[0m     final_df = (
[0m22:37:49.251025 [error] [MainThread]:   [0;32m---> 35[0;31m         [0mstg_customers_df[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:49.251295 [error] [MainThread]:   [0m[1;32m     36[0m         [0;34m.[0m[0mjoin[0m[0;34m([0m[0mcustomer_orders_df[0m[0;34m,[0m [0mstg_customers_df[0m[0;34m.[0m[0mcustomer_id[0m [0;34m==[0m [0mcustomer_orders_df[0m[0;34m.[0m[0mcustomer_id[0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:49.251570 [error] [MainThread]:   [1;32m     37[0m         [0;34m.[0m[0mjoin[0m[0;34m([0m[0mcustomer_payments_df[0m[0;34m,[0m [0mstg_customers_df[0m[0;34m.[0m[0mcustomer_id[0m [0;34m==[0m [0mcustomer_payments_df[0m[0;34m.[0m[0mcustomer_id[0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:49.251842 [error] [MainThread]:   
[0m22:37:49.252110 [error] [MainThread]:   [0;32m/databricks/spark/python/pyspark/sql/dataframe.py[0m in [0;36mjoin[0;34m(self, other, on, how)[0m
[0m22:37:49.252389 [error] [MainThread]:   [1;32m   1561[0m                 [0mon[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jseq[0m[0;34m([0m[0;34m[[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:49.252662 [error] [MainThread]:   [1;32m   1562[0m             [0;32massert[0m [0misinstance[0m[0;34m([0m[0mhow[0m[0;34m,[0m [0mstr[0m[0;34m)[0m[0;34m,[0m [0;34m"how should be a string"[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:49.252939 [error] [MainThread]:   [0;32m-> 1563[0;31m             [0mjdf[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mjoin[0m[0;34m([0m[0mother[0m[0;34m.[0m[0m_jdf[0m[0;34m,[0m [0mon[0m[0;34m,[0m [0mhow[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:49.253217 [error] [MainThread]:   [0m[1;32m   1564[0m         [0;32mreturn[0m [0mDataFrame[0m[0;34m([0m[0mjdf[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0msparkSession[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:49.253489 [error] [MainThread]:   [1;32m   1565[0m [0;34m[0m[0m
[0m22:37:49.253760 [error] [MainThread]:   
[0m22:37:49.254030 [error] [MainThread]:   [0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[0m22:37:49.254303 [error] [MainThread]:   [1;32m   1319[0m [0;34m[0m[0m
[0m22:37:49.254576 [error] [MainThread]:   [1;32m   1320[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:49.254851 [error] [MainThread]:   [0;32m-> 1321[0;31m         return_value = get_return_value(
[0m22:37:49.255124 [error] [MainThread]:   [0m[1;32m   1322[0m             answer, self.gateway_client, self.target_id, self.name)
[0m22:37:49.255401 [error] [MainThread]:   [1;32m   1323[0m [0;34m[0m[0m
[0m22:37:49.255702 [error] [MainThread]:   
[0m22:37:49.255979 [error] [MainThread]:   [0;32m/databricks/spark/python/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[0m22:37:49.256251 [error] [MainThread]:   [1;32m    200[0m                 [0;31m# Hide where the exception came from that shows a non-Pythonic[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:49.256526 [error] [MainThread]:   [1;32m    201[0m                 [0;31m# JVM exception message.[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:49.256799 [error] [MainThread]:   [0;32m--> 202[0;31m                 [0;32mraise[0m [0mconverted[0m [0;32mfrom[0m [0;32mNone[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:49.257071 [error] [MainThread]:   [0m[1;32m    203[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:49.257342 [error] [MainThread]:   [1;32m    204[0m                 [0;32mraise[0m[0;34m[0m[0;34m[0m[0m
[0m22:37:49.257601 [error] [MainThread]:   
[0m22:37:49.257834 [error] [MainThread]:   [0;31mAnalysisException[0m:  Column customer_id#101L are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as("a").join(df.as("b"), $"a.id" > $"b.id")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.        
[0m22:37:49.258101 [info ] [MainThread]: 
[0m22:37:49.258352 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:37:49.258730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ac63e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12adb2550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ae25430>]}
[0m22:37:49.259025 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 22:41:04.079423 | b97d1961-6fe7-4f26-9cfc-7e519fe3588a ==============================
[0m22:41:04.079467 [info ] [MainThread]: Running with dbt=1.3.2
[0m22:41:04.080506 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:41:04.080683 [debug] [MainThread]: Tracking: tracking
[0m22:41:04.113881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f2eb880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f2ebb80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f2fe610>]}
[0m22:41:04.173908 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:41:04.174287 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m22:41:04.263709 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b97d1961-6fe7-4f26-9cfc-7e519fe3588a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f5344f0>]}
[0m22:41:04.271322 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b97d1961-6fe7-4f26-9cfc-7e519fe3588a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f4623d0>]}
[0m22:41:04.271524 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m22:41:04.271712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b97d1961-6fe7-4f26-9cfc-7e519fe3588a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f4a3040>]}
[0m22:41:04.272807 [info ] [MainThread]: 
[0m22:41:04.273301 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:41:04.274114 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m22:41:04.282778 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m22:41:04.282964 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m22:41:04.283056 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:41:07.257772 [debug] [ThreadPool]: SQL status: OK in 2.97 seconds
[0m22:41:07.283580 [debug] [ThreadPool]: On list_schemas: Close
[0m22:41:08.500238 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_bruno"
[0m22:41:08.514233 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:41:08.514608 [debug] [ThreadPool]: Using databricks connection "list_None_bruno"
[0m22:41:08.514849 [debug] [ThreadPool]: On list_None_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_bruno"} */
show table extended in bruno like '*'
  
[0m22:41:08.515078 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:41:11.598166 [debug] [ThreadPool]: SQL status: OK in 3.08 seconds
[0m22:41:11.608356 [debug] [ThreadPool]: On list_None_bruno: ROLLBACK
[0m22:41:11.608720 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:41:11.609000 [debug] [ThreadPool]: On list_None_bruno: Close
[0m22:41:12.884620 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b97d1961-6fe7-4f26-9cfc-7e519fe3588a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f469d60>]}
[0m22:41:12.886216 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:41:12.886532 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:41:12.887611 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:41:12.888180 [info ] [MainThread]: 
[0m22:41:12.901526 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m22:41:12.902107 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m22:41:12.903242 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.customers"
[0m22:41:12.903485 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m22:41:12.903731 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m22:41:12.933701 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m22:41:12.935292 [debug] [Thread-1  ]: finished collecting timing info
[0m22:41:12.935485 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m22:41:12.960008 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m22:41:12.960975 [debug] [Thread-1  ]: On model.jaffle_shop.customers: 
  
    
import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method="all_purpose_cluster",
        create_notebook=True,
        cluster_id="0124-220040-qfvx2ysq"
    )

    stg_customers_df = dbt.ref('stg_customers')
    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    customer_orders_df = (
        stg_orders_df
        .groupby("customer_id")
        .agg(
            F.min(F.col("order_date")).alias('first_order'),
            F.max(F.col("order_date")).alias('most_recent_order'),
            F.count(F.col("order_id")).alias('number_of_orders')
        )
    )

    customer_payments_df = (
        stg_payments_df
        .join(stg_orders_df, stg_payments_df.order_id == stg_orders_df.order_id, "left")
        .groupby(stg_orders_df.customer_id)
        .agg(
            F.sum(F.col("amount")).alias('total_amount')
        )
    )

    final_df = (
        stg_customers_df.join(customer_orders_df, stg_customers_df["customer_id"] == customer_orders_df["customer_id"], "left") \
                        .join(customer_payments_df, stg_customers_df["customer_id"] == customer_payments_df["customer_id"], "left") \
                        .selectExpr("customer_id", "first_name", "last_name", "first_order", "most_recent_order", "number_of_orders", "total_amount as customer_lifetime_value")

    )

    return final_df



# stg_customers_df = spark.table("stg_customers")
# stg_orders_df = spark.table("stg_orders")
# stg_payments_df = spark.table("stg_payments")

# # customer_orders
# customer_orders_df = stg_orders_df.groupBy("customer_id") \
#                                   .agg(min("order_date").alias("first_order"), 
#                                        max("order_date").alias("most_recent_order"), 
#                                        count("order_id").alias("number_of_orders"))

# # customer_payments
# customer_payments_df = stg_payments_df.join(stg_orders_df, stg_payments_df["order_id"] == stg_orders_df["order_id"], "left") \
#                                       .groupBy("customer_id") \
#                                       .agg(sum("amount").alias("total_amount"))

# # final
# final_df = stg_customers_df.join(customer_orders_df, stg_customers_df["customer_id"] == customer_orders_df["customer_id"], "left") \
#                            .join(customer_payments_df, stg_customers_df["customer_id"] == customer_payments_df["customer_id"], "left") \
#                            .selectExpr("customer_id", "first_name", "last_name", "first_order", "most_recent_order", "number_of_orders", "total_amount as customer_lifetime_value")

# final_df.show()


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "bruno.stg_customers", "stg_orders": "bruno.stg_orders", "stg_payments": "bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'None'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist
import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write.mode("overwrite").format("delta").option("overwriteSchema", "true").saveAsTable("bruno.customers")
  
[0m22:41:30.136599 [debug] [Thread-1  ]: finished collecting timing info
[0m22:41:30.138806 [debug] [Thread-1  ]: Runtime Error in model customers (models/customers.py)
  Python model failed with traceback as:
  (Note that the line number here does not match the line number in your code due to dbt templating)
  [0;31m---------------------------------------------------------------------------[0m
  [0;31mAnalysisException[0m                         Traceback (most recent call last)
  [0;32m<command-1437986966279492>[0m in [0;36m<cell line: 8>[0;34m()[0m
  [1;32m      6[0m [0;31m# --- Autogenerated dbt materialization code. --- #[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  [1;32m      7[0m [0mdbt[0m [0;34m=[0m [0mdbtObj[0m[0;34m([0m[0mspark[0m[0;34m.[0m[0mtable[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0;32m----> 8[0;31m [0mdf[0m [0;34m=[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0mspark[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m      9[0m [0;34m[0m[0m
  [1;32m     10[0m [0;31m# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  
  [0;32m<command-1437986966279490>[0m in [0;36mmodel[0;34m(dbt, session)[0m
  [1;32m     33[0m [0;34m[0m[0m
  [1;32m     34[0m     final_df = (
  [0;32m---> 35[0;31m         [0mstg_customers_df[0m[0;34m.[0m[0mjoin[0m[0;34m([0m[0mcustomer_orders_df[0m[0;34m,[0m [0mstg_customers_df[0m[0;34m[[0m[0;34m"customer_id"[0m[0;34m][0m [0;34m==[0m [0mcustomer_orders_df[0m[0;34m[[0m[0;34m"customer_id"[0m[0;34m][0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;31m [0m[0;31m\[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m     36[0m                         [0;34m.[0m[0mjoin[0m[0;34m([0m[0mcustomer_payments_df[0m[0;34m,[0m [0mstg_customers_df[0m[0;34m[[0m[0;34m"customer_id"[0m[0;34m][0m [0;34m==[0m [0mcustomer_payments_df[0m[0;34m[[0m[0;34m"customer_id"[0m[0;34m][0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;31m [0m[0;31m\[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     37[0m                         [0;34m.[0m[0mselectExpr[0m[0;34m([0m[0;34m"customer_id"[0m[0;34m,[0m [0;34m"first_name"[0m[0;34m,[0m [0;34m"last_name"[0m[0;34m,[0m [0;34m"first_order"[0m[0;34m,[0m [0;34m"most_recent_order"[0m[0;34m,[0m [0;34m"number_of_orders"[0m[0;34m,[0m [0;34m"total_amount as customer_lifetime_value"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  
  [0;32m/databricks/spark/python/pyspark/sql/dataframe.py[0m in [0;36mjoin[0;34m(self, other, on, how)[0m
  [1;32m   1561[0m                 [0mon[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jseq[0m[0;34m([0m[0;34m[[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [1;32m   1562[0m             [0;32massert[0m [0misinstance[0m[0;34m([0m[0mhow[0m[0;34m,[0m [0mstr[0m[0;34m)[0m[0;34m,[0m [0;34m"how should be a string"[0m[0;34m[0m[0;34m[0m[0m
  [0;32m-> 1563[0;31m             [0mjdf[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mjoin[0m[0;34m([0m[0mother[0m[0;34m.[0m[0m_jdf[0m[0;34m,[0m [0mon[0m[0;34m,[0m [0mhow[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m   1564[0m         [0;32mreturn[0m [0mDataFrame[0m[0;34m([0m[0mjdf[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0msparkSession[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [1;32m   1565[0m [0;34m[0m[0m
  
  [0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
  [1;32m   1319[0m [0;34m[0m[0m
  [1;32m   1320[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0;32m-> 1321[0;31m         return_value = get_return_value(
  [0m[1;32m   1322[0m             answer, self.gateway_client, self.target_id, self.name)
  [1;32m   1323[0m [0;34m[0m[0m
  
  [0;32m/databricks/spark/python/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
  [1;32m    200[0m                 [0;31m# Hide where the exception came from that shows a non-Pythonic[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  [1;32m    201[0m                 [0;31m# JVM exception message.[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  [0;32m--> 202[0;31m                 [0;32mraise[0m [0mconverted[0m [0;32mfrom[0m [0;32mNone[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m    203[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
  [1;32m    204[0m                 [0;32mraise[0m[0;34m[0m[0;34m[0m[0m
  
  [0;31mAnalysisException[0m:  Column customer_id#226L are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as("a").join(df.as("b"), $"a.id" > $"b.id")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.        
[0m22:41:30.139484 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b97d1961-6fe7-4f26-9cfc-7e519fe3588a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f5afd30>]}
[0m22:41:30.140226 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 17.24s]
[0m22:41:30.141256 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m22:41:30.143563 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:41:30.143915 [debug] [MainThread]: On master: ROLLBACK
[0m22:41:30.144162 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:41:31.314948 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:41:31.316180 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:41:31.316581 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:41:31.317005 [debug] [MainThread]: On master: ROLLBACK
[0m22:41:31.317263 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:41:31.317516 [debug] [MainThread]: On master: Close
[0m22:41:32.366899 [info ] [MainThread]: 
[0m22:41:32.368000 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 28.09 seconds (28.09s).
[0m22:41:32.368988 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:41:32.369365 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m22:41:32.391038 [info ] [MainThread]: 
[0m22:41:32.391593 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:41:32.392018 [info ] [MainThread]: 
[0m22:41:32.392345 [error] [MainThread]: [33mRuntime Error in model customers (models/customers.py)[0m
[0m22:41:32.392640 [error] [MainThread]:   Python model failed with traceback as:
[0m22:41:32.392920 [error] [MainThread]:   (Note that the line number here does not match the line number in your code due to dbt templating)
[0m22:41:32.393196 [error] [MainThread]:   [0;31m---------------------------------------------------------------------------[0m
[0m22:41:32.393469 [error] [MainThread]:   [0;31mAnalysisException[0m                         Traceback (most recent call last)
[0m22:41:32.393738 [error] [MainThread]:   [0;32m<command-1437986966279492>[0m in [0;36m<cell line: 8>[0;34m()[0m
[0m22:41:32.394008 [error] [MainThread]:   [1;32m      6[0m [0;31m# --- Autogenerated dbt materialization code. --- #[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m22:41:32.394279 [error] [MainThread]:   [1;32m      7[0m [0mdbt[0m [0;34m=[0m [0mdbtObj[0m[0;34m([0m[0mspark[0m[0;34m.[0m[0mtable[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:41:32.394551 [error] [MainThread]:   [0;32m----> 8[0;31m [0mdf[0m [0;34m=[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0mspark[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:41:32.394824 [error] [MainThread]:   [0m[1;32m      9[0m [0;34m[0m[0m
[0m22:41:32.395102 [error] [MainThread]:   [1;32m     10[0m [0;31m# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m22:41:32.395377 [error] [MainThread]:   
[0m22:41:32.395643 [error] [MainThread]:   [0;32m<command-1437986966279490>[0m in [0;36mmodel[0;34m(dbt, session)[0m
[0m22:41:32.395912 [error] [MainThread]:   [1;32m     33[0m [0;34m[0m[0m
[0m22:41:32.396182 [error] [MainThread]:   [1;32m     34[0m     final_df = (
[0m22:41:32.396451 [error] [MainThread]:   [0;32m---> 35[0;31m         [0mstg_customers_df[0m[0;34m.[0m[0mjoin[0m[0;34m([0m[0mcustomer_orders_df[0m[0;34m,[0m [0mstg_customers_df[0m[0;34m[[0m[0;34m"customer_id"[0m[0;34m][0m [0;34m==[0m [0mcustomer_orders_df[0m[0;34m[[0m[0;34m"customer_id"[0m[0;34m][0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;31m [0m[0;31m\[0m[0;34m[0m[0;34m[0m[0m
[0m22:41:32.396726 [error] [MainThread]:   [0m[1;32m     36[0m                         [0;34m.[0m[0mjoin[0m[0;34m([0m[0mcustomer_payments_df[0m[0;34m,[0m [0mstg_customers_df[0m[0;34m[[0m[0;34m"customer_id"[0m[0;34m][0m [0;34m==[0m [0mcustomer_payments_df[0m[0;34m[[0m[0;34m"customer_id"[0m[0;34m][0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;31m [0m[0;31m\[0m[0;34m[0m[0;34m[0m[0m
[0m22:41:32.396997 [error] [MainThread]:   [1;32m     37[0m                         [0;34m.[0m[0mselectExpr[0m[0;34m([0m[0;34m"customer_id"[0m[0;34m,[0m [0;34m"first_name"[0m[0;34m,[0m [0;34m"last_name"[0m[0;34m,[0m [0;34m"first_order"[0m[0;34m,[0m [0;34m"most_recent_order"[0m[0;34m,[0m [0;34m"number_of_orders"[0m[0;34m,[0m [0;34m"total_amount as customer_lifetime_value"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:41:32.397272 [error] [MainThread]:   
[0m22:41:32.397539 [error] [MainThread]:   [0;32m/databricks/spark/python/pyspark/sql/dataframe.py[0m in [0;36mjoin[0;34m(self, other, on, how)[0m
[0m22:41:32.397805 [error] [MainThread]:   [1;32m   1561[0m                 [0mon[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jseq[0m[0;34m([0m[0;34m[[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:41:32.398072 [error] [MainThread]:   [1;32m   1562[0m             [0;32massert[0m [0misinstance[0m[0;34m([0m[0mhow[0m[0;34m,[0m [0mstr[0m[0;34m)[0m[0;34m,[0m [0;34m"how should be a string"[0m[0;34m[0m[0;34m[0m[0m
[0m22:41:32.398339 [error] [MainThread]:   [0;32m-> 1563[0;31m             [0mjdf[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mjoin[0m[0;34m([0m[0mother[0m[0;34m.[0m[0m_jdf[0m[0;34m,[0m [0mon[0m[0;34m,[0m [0mhow[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:41:32.398609 [error] [MainThread]:   [0m[1;32m   1564[0m         [0;32mreturn[0m [0mDataFrame[0m[0;34m([0m[0mjdf[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0msparkSession[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:41:32.398876 [error] [MainThread]:   [1;32m   1565[0m [0;34m[0m[0m
[0m22:41:32.399147 [error] [MainThread]:   
[0m22:41:32.399415 [error] [MainThread]:   [0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[0m22:41:32.399682 [error] [MainThread]:   [1;32m   1319[0m [0;34m[0m[0m
[0m22:41:32.399954 [error] [MainThread]:   [1;32m   1320[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:41:32.400228 [error] [MainThread]:   [0;32m-> 1321[0;31m         return_value = get_return_value(
[0m22:41:32.400495 [error] [MainThread]:   [0m[1;32m   1322[0m             answer, self.gateway_client, self.target_id, self.name)
[0m22:41:32.400767 [error] [MainThread]:   [1;32m   1323[0m [0;34m[0m[0m
[0m22:41:32.401033 [error] [MainThread]:   
[0m22:41:32.401299 [error] [MainThread]:   [0;32m/databricks/spark/python/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[0m22:41:32.401563 [error] [MainThread]:   [1;32m    200[0m                 [0;31m# Hide where the exception came from that shows a non-Pythonic[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m22:41:32.401830 [error] [MainThread]:   [1;32m    201[0m                 [0;31m# JVM exception message.[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m22:41:32.402063 [error] [MainThread]:   [0;32m--> 202[0;31m                 [0;32mraise[0m [0mconverted[0m [0;32mfrom[0m [0;32mNone[0m[0;34m[0m[0;34m[0m[0m
[0m22:41:32.402295 [error] [MainThread]:   [0m[1;32m    203[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m22:41:32.402561 [error] [MainThread]:   [1;32m    204[0m                 [0;32mraise[0m[0;34m[0m[0;34m[0m[0m
[0m22:41:32.402795 [error] [MainThread]:   
[0m22:41:32.403029 [error] [MainThread]:   [0;31mAnalysisException[0m:  Column customer_id#226L are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as("a").join(df.as("b"), $"a.id" > $"b.id")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.        
[0m22:41:32.403290 [info ] [MainThread]: 
[0m22:41:32.403537 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:41:32.403908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f4620d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f5bbaf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f62c730>]}
[0m22:41:32.404197 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 22:47:04.647805 | 161cb240-cf24-4f7c-a987-92ababff78a8 ==============================
[0m22:47:04.647849 [info ] [MainThread]: Running with dbt=1.3.2
[0m22:47:04.648579 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:47:04.648725 [debug] [MainThread]: Tracking: tracking
[0m22:47:04.675982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ed898e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ed89be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ed9c670>]}
[0m22:47:04.735719 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:47:04.736099 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m22:47:04.826371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '161cb240-cf24-4f7c-a987-92ababff78a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ef790d0>]}
[0m22:47:04.833907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '161cb240-cf24-4f7c-a987-92ababff78a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ef01430>]}
[0m22:47:04.834109 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m22:47:04.834297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '161cb240-cf24-4f7c-a987-92ababff78a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ef3cfa0>]}
[0m22:47:04.835422 [info ] [MainThread]: 
[0m22:47:04.835914 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:47:04.836709 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m22:47:04.845501 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m22:47:04.845703 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m22:47:04.845804 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:47:07.796058 [debug] [ThreadPool]: SQL status: OK in 2.95 seconds
[0m22:47:07.819468 [debug] [ThreadPool]: On list_schemas: Close
[0m22:47:09.048987 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_bruno"
[0m22:47:09.062802 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:47:09.063078 [debug] [ThreadPool]: Using databricks connection "list_None_bruno"
[0m22:47:09.063319 [debug] [ThreadPool]: On list_None_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_bruno"} */
show table extended in bruno like '*'
  
[0m22:47:09.063538 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:47:12.208247 [debug] [ThreadPool]: SQL status: OK in 3.14 seconds
[0m22:47:12.217191 [debug] [ThreadPool]: On list_None_bruno: ROLLBACK
[0m22:47:12.217488 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:47:12.217732 [debug] [ThreadPool]: On list_None_bruno: Close
[0m22:47:13.452909 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '161cb240-cf24-4f7c-a987-92ababff78a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ef06dc0>]}
[0m22:47:13.454275 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:47:13.454545 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:47:13.455631 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:47:13.456520 [info ] [MainThread]: 
[0m22:47:13.469280 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m22:47:13.469951 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m22:47:13.471104 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.customers"
[0m22:47:13.471364 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m22:47:13.471615 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m22:47:13.501678 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m22:47:13.503833 [debug] [Thread-1  ]: finished collecting timing info
[0m22:47:13.504013 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m22:47:13.528424 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m22:47:13.529296 [debug] [Thread-1  ]: On model.jaffle_shop.customers: 
  
    
import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method="all_purpose_cluster",
        create_notebook=True,
        cluster_id="0124-220040-qfvx2ysq"
    )

    stg_customers_df = dbt.ref('stg_customers')
    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    customer_orders_df = (
        stg_orders_df
        .groupby("customer_id")
        .agg(
            F.min(F.col("order_date")).alias('first_order'),
            F.max(F.col("order_date")).alias('most_recent_order'),
            F.count(F.col("order_id")).alias('number_of_orders')
        )
    )

    customer_payments_df = (
        stg_payments_df
        .join(stg_orders_df, stg_payments_df.order_id == stg_orders_df.order_id, "left")
        .groupby(stg_orders_df.customer_id)
        .agg(
            F.sum(F.col("amount")).alias('total_amount')
        )
    )

    final_df = (
        stg_customers_df.alias("customers").join(customer_orders_df.alias("customer_orders"), col("customers.customer_id") == col("customer_orders.customer_id"), "left") \
                        .join(customer_payments_df.alias("customer_payments"), col("customers.customer_id") == col("customer_payments.customer_id"), "left") \
                        .selectExpr("customer_id", "first_name", "last_name", "first_order", "most_recent_order", "number_of_orders", "total_amount as customer_lifetime_value")

    )

    return final_df



# stg_customers_df = spark.table("stg_customers")
# stg_orders_df = spark.table("stg_orders")
# stg_payments_df = spark.table("stg_payments")

# # customer_orders
# customer_orders_df = stg_orders_df.groupBy("customer_id") \
#                                   .agg(min("order_date").alias("first_order"), 
#                                        max("order_date").alias("most_recent_order"), 
#                                        count("order_id").alias("number_of_orders"))

# # customer_payments
# customer_payments_df = stg_payments_df.join(stg_orders_df, stg_payments_df["order_id"] == stg_orders_df["order_id"], "left") \
#                                       .groupBy("customer_id") \
#                                       .agg(sum("amount").alias("total_amount"))

# # final
# final_df = stg_customers_df.join(customer_orders_df, stg_customers_df["customer_id"] == customer_orders_df["customer_id"], "left") \
#                            .join(customer_payments_df, stg_customers_df["customer_id"] == customer_payments_df["customer_id"], "left") \
#                            .selectExpr("customer_id", "first_name", "last_name", "first_order", "most_recent_order", "number_of_orders", "total_amount as customer_lifetime_value")

# final_df.show()


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "bruno.stg_customers", "stg_orders": "bruno.stg_orders", "stg_payments": "bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'None'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist
import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write.mode("overwrite").format("delta").option("overwriteSchema", "true").saveAsTable("bruno.customers")
  
[0m22:47:32.074906 [debug] [Thread-1  ]: finished collecting timing info
[0m22:47:32.077780 [debug] [Thread-1  ]: Runtime Error in model customers (models/customers.py)
  Python model failed with traceback as:
  (Note that the line number here does not match the line number in your code due to dbt templating)
  [0;31m---------------------------------------------------------------------------[0m
  [0;31mNameError[0m                                 Traceback (most recent call last)
  [0;32m<command-2756326064963465>[0m in [0;36m<cell line: 8>[0;34m()[0m
  [1;32m      6[0m [0;31m# --- Autogenerated dbt materialization code. --- #[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  [1;32m      7[0m [0mdbt[0m [0;34m=[0m [0mdbtObj[0m[0;34m([0m[0mspark[0m[0;34m.[0m[0mtable[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0;32m----> 8[0;31m [0mdf[0m [0;34m=[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0mspark[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m      9[0m [0;34m[0m[0m
  [1;32m     10[0m [0;31m# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  
  [0;32m<command-2756326064963463>[0m in [0;36mmodel[0;34m(dbt, session)[0m
  [1;32m     33[0m [0;34m[0m[0m
  [1;32m     34[0m     final_df = (
  [0;32m---> 35[0;31m         [0mstg_customers_df[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"customers"[0m[0;34m)[0m[0;34m.[0m[0mjoin[0m[0;34m([0m[0mcustomer_orders_df[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"customer_orders"[0m[0;34m)[0m[0;34m,[0m [0mcol[0m[0;34m([0m[0;34m"customers.customer_id"[0m[0;34m)[0m [0;34m==[0m [0mcol[0m[0;34m([0m[0;34m"customer_orders.customer_id"[0m[0;34m)[0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;31m [0m[0;31m\[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m     36[0m                         [0;34m.[0m[0mjoin[0m[0;34m([0m[0mcustomer_payments_df[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"customer_payments"[0m[0;34m)[0m[0;34m,[0m [0mcol[0m[0;34m([0m[0;34m"customers.customer_id"[0m[0;34m)[0m [0;34m==[0m [0mcol[0m[0;34m([0m[0;34m"customer_payments.customer_id"[0m[0;34m)[0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;31m [0m[0;31m\[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     37[0m                         [0;34m.[0m[0mselectExpr[0m[0;34m([0m[0;34m"customer_id"[0m[0;34m,[0m [0;34m"first_name"[0m[0;34m,[0m [0;34m"last_name"[0m[0;34m,[0m [0;34m"first_order"[0m[0;34m,[0m [0;34m"most_recent_order"[0m[0;34m,[0m [0;34m"number_of_orders"[0m[0;34m,[0m [0;34m"total_amount as customer_lifetime_value"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  
  [0;31mNameError[0m: name 'col' is not defined
[0m22:47:32.078446 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '161cb240-cf24-4f7c-a987-92ababff78a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f04cd60>]}
[0m22:47:32.079244 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 18.61s]
[0m22:47:32.080319 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m22:47:32.082859 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:47:32.083280 [debug] [MainThread]: On master: ROLLBACK
[0m22:47:32.083557 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:47:33.296260 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:47:33.298622 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:47:33.299222 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:47:33.299853 [debug] [MainThread]: On master: ROLLBACK
[0m22:47:33.300448 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:47:33.300997 [debug] [MainThread]: On master: Close
[0m22:47:34.480013 [info ] [MainThread]: 
[0m22:47:34.481386 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 29.64 seconds (29.64s).
[0m22:47:34.482042 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:47:34.482412 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m22:47:34.506729 [info ] [MainThread]: 
[0m22:47:34.507222 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:47:34.507586 [info ] [MainThread]: 
[0m22:47:34.507910 [error] [MainThread]: [33mRuntime Error in model customers (models/customers.py)[0m
[0m22:47:34.508212 [error] [MainThread]:   Python model failed with traceback as:
[0m22:47:34.508499 [error] [MainThread]:   (Note that the line number here does not match the line number in your code due to dbt templating)
[0m22:47:34.508782 [error] [MainThread]:   [0;31m---------------------------------------------------------------------------[0m
[0m22:47:34.509062 [error] [MainThread]:   [0;31mNameError[0m                                 Traceback (most recent call last)
[0m22:47:34.509340 [error] [MainThread]:   [0;32m<command-2756326064963465>[0m in [0;36m<cell line: 8>[0;34m()[0m
[0m22:47:34.509617 [error] [MainThread]:   [1;32m      6[0m [0;31m# --- Autogenerated dbt materialization code. --- #[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m22:47:34.509897 [error] [MainThread]:   [1;32m      7[0m [0mdbt[0m [0;34m=[0m [0mdbtObj[0m[0;34m([0m[0mspark[0m[0;34m.[0m[0mtable[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:47:34.510175 [error] [MainThread]:   [0;32m----> 8[0;31m [0mdf[0m [0;34m=[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0mspark[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:47:34.510453 [error] [MainThread]:   [0m[1;32m      9[0m [0;34m[0m[0m
[0m22:47:34.510730 [error] [MainThread]:   [1;32m     10[0m [0;31m# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m22:47:34.511008 [error] [MainThread]:   
[0m22:47:34.511286 [error] [MainThread]:   [0;32m<command-2756326064963463>[0m in [0;36mmodel[0;34m(dbt, session)[0m
[0m22:47:34.511561 [error] [MainThread]:   [1;32m     33[0m [0;34m[0m[0m
[0m22:47:34.511837 [error] [MainThread]:   [1;32m     34[0m     final_df = (
[0m22:47:34.512112 [error] [MainThread]:   [0;32m---> 35[0;31m         [0mstg_customers_df[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"customers"[0m[0;34m)[0m[0;34m.[0m[0mjoin[0m[0;34m([0m[0mcustomer_orders_df[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"customer_orders"[0m[0;34m)[0m[0;34m,[0m [0mcol[0m[0;34m([0m[0;34m"customers.customer_id"[0m[0;34m)[0m [0;34m==[0m [0mcol[0m[0;34m([0m[0;34m"customer_orders.customer_id"[0m[0;34m)[0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;31m [0m[0;31m\[0m[0;34m[0m[0;34m[0m[0m
[0m22:47:34.512405 [error] [MainThread]:   [0m[1;32m     36[0m                         [0;34m.[0m[0mjoin[0m[0;34m([0m[0mcustomer_payments_df[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"customer_payments"[0m[0;34m)[0m[0;34m,[0m [0mcol[0m[0;34m([0m[0;34m"customers.customer_id"[0m[0;34m)[0m [0;34m==[0m [0mcol[0m[0;34m([0m[0;34m"customer_payments.customer_id"[0m[0;34m)[0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;31m [0m[0;31m\[0m[0;34m[0m[0;34m[0m[0m
[0m22:47:34.512691 [error] [MainThread]:   [1;32m     37[0m                         [0;34m.[0m[0mselectExpr[0m[0;34m([0m[0;34m"customer_id"[0m[0;34m,[0m [0;34m"first_name"[0m[0;34m,[0m [0;34m"last_name"[0m[0;34m,[0m [0;34m"first_order"[0m[0;34m,[0m [0;34m"most_recent_order"[0m[0;34m,[0m [0;34m"number_of_orders"[0m[0;34m,[0m [0;34m"total_amount as customer_lifetime_value"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:47:34.512973 [error] [MainThread]:   
[0m22:47:34.513251 [error] [MainThread]:   [0;31mNameError[0m: name 'col' is not defined
[0m22:47:34.513547 [info ] [MainThread]: 
[0m22:47:34.513837 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:47:34.514335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ef01160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f0584c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f04ca30>]}
[0m22:47:34.514750 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 22:48:06.380897 | 39a77bf4-42d6-4dcc-95b8-8216ee320715 ==============================
[0m22:48:06.380950 [info ] [MainThread]: Running with dbt=1.3.2
[0m22:48:06.381802 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:48:06.381964 [debug] [MainThread]: Tracking: tracking
[0m22:48:06.409558 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e89a8e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e89abe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e8ad670>]}
[0m22:48:06.470228 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:48:06.470636 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m22:48:06.562931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '39a77bf4-42d6-4dcc-95b8-8216ee320715', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ea8d0d0>]}
[0m22:48:06.570664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '39a77bf4-42d6-4dcc-95b8-8216ee320715', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ea11430>]}
[0m22:48:06.570882 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m22:48:06.571072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '39a77bf4-42d6-4dcc-95b8-8216ee320715', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ea4dfa0>]}
[0m22:48:06.572177 [info ] [MainThread]: 
[0m22:48:06.572655 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:48:06.573442 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m22:48:06.581587 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m22:48:06.581723 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m22:48:06.581815 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:48:09.054798 [debug] [ThreadPool]: SQL status: OK in 2.47 seconds
[0m22:48:09.077751 [debug] [ThreadPool]: On list_schemas: Close
[0m22:48:10.270408 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_bruno"
[0m22:48:10.284512 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:48:10.284814 [debug] [ThreadPool]: Using databricks connection "list_None_bruno"
[0m22:48:10.285058 [debug] [ThreadPool]: On list_None_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_bruno"} */
show table extended in bruno like '*'
  
[0m22:48:10.285262 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:48:13.129116 [debug] [ThreadPool]: SQL status: OK in 2.84 seconds
[0m22:48:13.140574 [debug] [ThreadPool]: On list_None_bruno: ROLLBACK
[0m22:48:13.141052 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:48:13.141333 [debug] [ThreadPool]: On list_None_bruno: Close
[0m22:48:14.363877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '39a77bf4-42d6-4dcc-95b8-8216ee320715', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ea18dc0>]}
[0m22:48:14.365075 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:48:14.365558 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:48:14.367467 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:48:14.368356 [info ] [MainThread]: 
[0m22:48:14.380749 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m22:48:14.381291 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m22:48:14.382417 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.customers"
[0m22:48:14.382675 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m22:48:14.382927 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m22:48:14.412461 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m22:48:14.413477 [debug] [Thread-1  ]: finished collecting timing info
[0m22:48:14.413658 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m22:48:14.438205 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m22:48:14.438738 [debug] [Thread-1  ]: On model.jaffle_shop.customers: 
  
    
import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method="all_purpose_cluster",
        create_notebook=True,
        cluster_id="0124-220040-qfvx2ysq"
    )

    stg_customers_df = dbt.ref('stg_customers')
    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    customer_orders_df = (
        stg_orders_df
        .groupby("customer_id")
        .agg(
            F.min(F.col("order_date")).alias('first_order'),
            F.max(F.col("order_date")).alias('most_recent_order'),
            F.count(F.col("order_id")).alias('number_of_orders')
        )
    )

    customer_payments_df = (
        stg_payments_df
        .join(stg_orders_df, stg_payments_df.order_id == stg_orders_df.order_id, "left")
        .groupby(stg_orders_df.customer_id)
        .agg(
            F.sum(F.col("amount")).alias('total_amount')
        )
    )

    final_df = (
        stg_customers_df.alias("customers").join(customer_orders_df.alias("customer_orders"), F.col("customers.customer_id") == F.col("customer_orders.customer_id"), "left") \
                        .join(customer_payments_df.alias("customer_payments"), F.col("customers.customer_id") == F.col("customer_payments.customer_id"), "left") \
                        .selectExpr("customer_id", "first_name", "last_name", "first_order", "most_recent_order", "number_of_orders", "total_amount as customer_lifetime_value")

    )

    return final_df



# stg_customers_df = spark.table("stg_customers")
# stg_orders_df = spark.table("stg_orders")
# stg_payments_df = spark.table("stg_payments")

# # customer_orders
# customer_orders_df = stg_orders_df.groupBy("customer_id") \
#                                   .agg(min("order_date").alias("first_order"), 
#                                        max("order_date").alias("most_recent_order"), 
#                                        count("order_id").alias("number_of_orders"))

# # customer_payments
# customer_payments_df = stg_payments_df.join(stg_orders_df, stg_payments_df["order_id"] == stg_orders_df["order_id"], "left") \
#                                       .groupBy("customer_id") \
#                                       .agg(sum("amount").alias("total_amount"))

# # final
# final_df = stg_customers_df.join(customer_orders_df, stg_customers_df["customer_id"] == customer_orders_df["customer_id"], "left") \
#                            .join(customer_payments_df, stg_customers_df["customer_id"] == customer_payments_df["customer_id"], "left") \
#                            .selectExpr("customer_id", "first_name", "last_name", "first_order", "most_recent_order", "number_of_orders", "total_amount as customer_lifetime_value")

# final_df.show()


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "bruno.stg_customers", "stg_orders": "bruno.stg_orders", "stg_payments": "bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'None'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist
import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write.mode("overwrite").format("delta").option("overwriteSchema", "true").saveAsTable("bruno.customers")
  
[0m22:48:31.366354 [debug] [Thread-1  ]: finished collecting timing info
[0m22:48:31.369066 [debug] [Thread-1  ]: Runtime Error in model customers (models/customers.py)
  Python model failed with traceback as:
  (Note that the line number here does not match the line number in your code due to dbt templating)
  [0;31m---------------------------------------------------------------------------[0m
  [0;31mAnalysisException[0m                         Traceback (most recent call last)
  [0;32m<command-2756326064963468>[0m in [0;36m<cell line: 8>[0;34m()[0m
  [1;32m      6[0m [0;31m# --- Autogenerated dbt materialization code. --- #[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  [1;32m      7[0m [0mdbt[0m [0;34m=[0m [0mdbtObj[0m[0;34m([0m[0mspark[0m[0;34m.[0m[0mtable[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0;32m----> 8[0;31m [0mdf[0m [0;34m=[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0mspark[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m      9[0m [0;34m[0m[0m
  [1;32m     10[0m [0;31m# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  
  [0;32m<command-2756326064963466>[0m in [0;36mmodel[0;34m(dbt, session)[0m
  [1;32m     33[0m [0;34m[0m[0m
  [1;32m     34[0m     final_df = (
  [0;32m---> 35[0;31m         [0mstg_customers_df[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"customers"[0m[0;34m)[0m[0;34m.[0m[0mjoin[0m[0;34m([0m[0mcustomer_orders_df[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"customer_orders"[0m[0;34m)[0m[0;34m,[0m [0mF[0m[0;34m.[0m[0mcol[0m[0;34m([0m[0;34m"customers.customer_id"[0m[0;34m)[0m [0;34m==[0m [0mF[0m[0;34m.[0m[0mcol[0m[0;34m([0m[0;34m"customer_orders.customer_id"[0m[0;34m)[0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;31m [0m[0;31m\[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m     36[0m                         [0;34m.[0m[0mjoin[0m[0;34m([0m[0mcustomer_payments_df[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"customer_payments"[0m[0;34m)[0m[0;34m,[0m [0mF[0m[0;34m.[0m[0mcol[0m[0;34m([0m[0;34m"customers.customer_id"[0m[0;34m)[0m [0;34m==[0m [0mF[0m[0;34m.[0m[0mcol[0m[0;34m([0m[0;34m"customer_payments.customer_id"[0m[0;34m)[0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;31m [0m[0;31m\[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     37[0m                         [0;34m.[0m[0mselectExpr[0m[0;34m([0m[0;34m"customer_id"[0m[0;34m,[0m [0;34m"first_name"[0m[0;34m,[0m [0;34m"last_name"[0m[0;34m,[0m [0;34m"first_order"[0m[0;34m,[0m [0;34m"most_recent_order"[0m[0;34m,[0m [0;34m"number_of_orders"[0m[0;34m,[0m [0;34m"total_amount as customer_lifetime_value"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  
  [0;32m/databricks/spark/python/pyspark/sql/dataframe.py[0m in [0;36mselectExpr[0;34m(self, *expr)[0m
  [1;32m   2070[0m         [0;32mif[0m [0mlen[0m[0;34m([0m[0mexpr[0m[0;34m)[0m [0;34m==[0m [0;36m1[0m [0;32mand[0m [0misinstance[0m[0;34m([0m[0mexpr[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m,[0m [0mlist[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
  [1;32m   2071[0m             [0mexpr[0m [0;34m=[0m [0mexpr[0m[0;34m[[0m[0;36m0[0m[0;34m][0m  [0;31m# type: ignore[assignment][0m[0;34m[0m[0;34m[0m[0m
  [0;32m-> 2072[0;31m         [0mjdf[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mselectExpr[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jseq[0m[0;34m([0m[0mexpr[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m   2073[0m         [0;32mreturn[0m [0mDataFrame[0m[0;34m([0m[0mjdf[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0msparkSession[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [1;32m   2074[0m [0;34m[0m[0m
  
  [0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
  [1;32m   1319[0m [0;34m[0m[0m
  [1;32m   1320[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0;32m-> 1321[0;31m         return_value = get_return_value(
  [0m[1;32m   1322[0m             answer, self.gateway_client, self.target_id, self.name)
  [1;32m   1323[0m [0;34m[0m[0m
  
  [0;32m/databricks/spark/python/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
  [1;32m    200[0m                 [0;31m# Hide where the exception came from that shows a non-Pythonic[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  [1;32m    201[0m                 [0;31m# JVM exception message.[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  [0;32m--> 202[0;31m                 [0;32mraise[0m [0mconverted[0m [0;32mfrom[0m [0;32mNone[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m    203[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
  [1;32m    204[0m                 [0;32mraise[0m[0;34m[0m[0;34m[0m[0m
  
  [0;31mAnalysisException[0m: Reference 'customer_id' is ambiguous, could be: customers.customer_id, customer_orders.customer_id, customer_payments.customer_id.; line 1 pos 0
[0m22:48:31.369746 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '39a77bf4-42d6-4dcc-95b8-8216ee320715', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12eb5dd60>]}
[0m22:48:31.370517 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 16.99s]
[0m22:48:31.371559 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m22:48:31.374031 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:48:31.374455 [debug] [MainThread]: On master: ROLLBACK
[0m22:48:31.374742 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:48:32.586279 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:48:32.586536 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:48:32.586657 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:48:32.586782 [debug] [MainThread]: On master: ROLLBACK
[0m22:48:32.586887 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:48:32.586990 [debug] [MainThread]: On master: Close
[0m22:48:33.820011 [info ] [MainThread]: 
[0m22:48:33.821556 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 27.25 seconds (27.25s).
[0m22:48:33.822145 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:48:33.822484 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m22:48:33.846023 [info ] [MainThread]: 
[0m22:48:33.846535 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:48:33.846957 [info ] [MainThread]: 
[0m22:48:33.847311 [error] [MainThread]: [33mRuntime Error in model customers (models/customers.py)[0m
[0m22:48:33.847611 [error] [MainThread]:   Python model failed with traceback as:
[0m22:48:33.847895 [error] [MainThread]:   (Note that the line number here does not match the line number in your code due to dbt templating)
[0m22:48:33.848182 [error] [MainThread]:   [0;31m---------------------------------------------------------------------------[0m
[0m22:48:33.848468 [error] [MainThread]:   [0;31mAnalysisException[0m                         Traceback (most recent call last)
[0m22:48:33.848752 [error] [MainThread]:   [0;32m<command-2756326064963468>[0m in [0;36m<cell line: 8>[0;34m()[0m
[0m22:48:33.849036 [error] [MainThread]:   [1;32m      6[0m [0;31m# --- Autogenerated dbt materialization code. --- #[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m22:48:33.849313 [error] [MainThread]:   [1;32m      7[0m [0mdbt[0m [0;34m=[0m [0mdbtObj[0m[0;34m([0m[0mspark[0m[0;34m.[0m[0mtable[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:48:33.849587 [error] [MainThread]:   [0;32m----> 8[0;31m [0mdf[0m [0;34m=[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0mspark[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:48:33.849862 [error] [MainThread]:   [0m[1;32m      9[0m [0;34m[0m[0m
[0m22:48:33.850137 [error] [MainThread]:   [1;32m     10[0m [0;31m# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m22:48:33.850408 [error] [MainThread]:   
[0m22:48:33.850681 [error] [MainThread]:   [0;32m<command-2756326064963466>[0m in [0;36mmodel[0;34m(dbt, session)[0m
[0m22:48:33.850954 [error] [MainThread]:   [1;32m     33[0m [0;34m[0m[0m
[0m22:48:33.851225 [error] [MainThread]:   [1;32m     34[0m     final_df = (
[0m22:48:33.851495 [error] [MainThread]:   [0;32m---> 35[0;31m         [0mstg_customers_df[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"customers"[0m[0;34m)[0m[0;34m.[0m[0mjoin[0m[0;34m([0m[0mcustomer_orders_df[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"customer_orders"[0m[0;34m)[0m[0;34m,[0m [0mF[0m[0;34m.[0m[0mcol[0m[0;34m([0m[0;34m"customers.customer_id"[0m[0;34m)[0m [0;34m==[0m [0mF[0m[0;34m.[0m[0mcol[0m[0;34m([0m[0;34m"customer_orders.customer_id"[0m[0;34m)[0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;31m [0m[0;31m\[0m[0;34m[0m[0;34m[0m[0m
[0m22:48:33.851780 [error] [MainThread]:   [0m[1;32m     36[0m                         [0;34m.[0m[0mjoin[0m[0;34m([0m[0mcustomer_payments_df[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"customer_payments"[0m[0;34m)[0m[0;34m,[0m [0mF[0m[0;34m.[0m[0mcol[0m[0;34m([0m[0;34m"customers.customer_id"[0m[0;34m)[0m [0;34m==[0m [0mF[0m[0;34m.[0m[0mcol[0m[0;34m([0m[0;34m"customer_payments.customer_id"[0m[0;34m)[0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;31m [0m[0;31m\[0m[0;34m[0m[0;34m[0m[0m
[0m22:48:33.852062 [error] [MainThread]:   [1;32m     37[0m                         [0;34m.[0m[0mselectExpr[0m[0;34m([0m[0;34m"customer_id"[0m[0;34m,[0m [0;34m"first_name"[0m[0;34m,[0m [0;34m"last_name"[0m[0;34m,[0m [0;34m"first_order"[0m[0;34m,[0m [0;34m"most_recent_order"[0m[0;34m,[0m [0;34m"number_of_orders"[0m[0;34m,[0m [0;34m"total_amount as customer_lifetime_value"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:48:33.852341 [error] [MainThread]:   
[0m22:48:33.852611 [error] [MainThread]:   [0;32m/databricks/spark/python/pyspark/sql/dataframe.py[0m in [0;36mselectExpr[0;34m(self, *expr)[0m
[0m22:48:33.852885 [error] [MainThread]:   [1;32m   2070[0m         [0;32mif[0m [0mlen[0m[0;34m([0m[0mexpr[0m[0;34m)[0m [0;34m==[0m [0;36m1[0m [0;32mand[0m [0misinstance[0m[0;34m([0m[0mexpr[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m,[0m [0mlist[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m22:48:33.853167 [error] [MainThread]:   [1;32m   2071[0m             [0mexpr[0m [0;34m=[0m [0mexpr[0m[0;34m[[0m[0;36m0[0m[0;34m][0m  [0;31m# type: ignore[assignment][0m[0;34m[0m[0;34m[0m[0m
[0m22:48:33.853447 [error] [MainThread]:   [0;32m-> 2072[0;31m         [0mjdf[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_jdf[0m[0;34m.[0m[0mselectExpr[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_jseq[0m[0;34m([0m[0mexpr[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:48:33.853726 [error] [MainThread]:   [0m[1;32m   2073[0m         [0;32mreturn[0m [0mDataFrame[0m[0;34m([0m[0mjdf[0m[0;34m,[0m [0mself[0m[0;34m.[0m[0msparkSession[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:48:33.854003 [error] [MainThread]:   [1;32m   2074[0m [0;34m[0m[0m
[0m22:48:33.854280 [error] [MainThread]:   
[0m22:48:33.854559 [error] [MainThread]:   [0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py[0m in [0;36m__call__[0;34m(self, *args)[0m
[0m22:48:33.854836 [error] [MainThread]:   [1;32m   1319[0m [0;34m[0m[0m
[0m22:48:33.855115 [error] [MainThread]:   [1;32m   1320[0m         [0manswer[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mgateway_client[0m[0;34m.[0m[0msend_command[0m[0;34m([0m[0mcommand[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:48:33.855394 [error] [MainThread]:   [0;32m-> 1321[0;31m         return_value = get_return_value(
[0m22:48:33.855674 [error] [MainThread]:   [0m[1;32m   1322[0m             answer, self.gateway_client, self.target_id, self.name)
[0m22:48:33.855950 [error] [MainThread]:   [1;32m   1323[0m [0;34m[0m[0m
[0m22:48:33.856222 [error] [MainThread]:   
[0m22:48:33.856495 [error] [MainThread]:   [0;32m/databricks/spark/python/pyspark/sql/utils.py[0m in [0;36mdeco[0;34m(*a, **kw)[0m
[0m22:48:33.856769 [error] [MainThread]:   [1;32m    200[0m                 [0;31m# Hide where the exception came from that shows a non-Pythonic[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m22:48:33.857043 [error] [MainThread]:   [1;32m    201[0m                 [0;31m# JVM exception message.[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m22:48:33.857289 [error] [MainThread]:   [0;32m--> 202[0;31m                 [0;32mraise[0m [0mconverted[0m [0;32mfrom[0m [0;32mNone[0m[0;34m[0m[0;34m[0m[0m
[0m22:48:33.857525 [error] [MainThread]:   [0m[1;32m    203[0m             [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m22:48:33.857762 [error] [MainThread]:   [1;32m    204[0m                 [0;32mraise[0m[0;34m[0m[0;34m[0m[0m
[0m22:48:33.858162 [error] [MainThread]:   
[0m22:48:33.858400 [error] [MainThread]:   [0;31mAnalysisException[0m: Reference 'customer_id' is ambiguous, could be: customers.customer_id, customer_orders.customer_id, customer_payments.customer_id.; line 1 pos 0
[0m22:48:33.858684 [info ] [MainThread]: 
[0m22:48:33.858943 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:48:33.859328 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ea11160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12eb674c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ebdb760>]}
[0m22:48:33.859620 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 22:51:12.296307 | 56ddec93-b2ce-43fa-bae7-da766f5b7de5 ==============================
[0m22:51:12.296348 [info ] [MainThread]: Running with dbt=1.3.2
[0m22:51:12.297103 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'indirect_selection': 'eager', 'resource_types': [], 'exclude': ['orders.py', 'customers.py'], 'which': 'build', 'rpc_method': 'build'}
[0m22:51:12.297266 [debug] [MainThread]: Tracking: tracking
[0m22:51:12.326753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12cdb68e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12cdb6be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12cdc9670>]}
[0m22:51:12.387059 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:51:12.387434 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m22:51:12.478652 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '56ddec93-b2ce-43fa-bae7-da766f5b7de5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12cefe550>]}
[0m22:51:12.486287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '56ddec93-b2ce-43fa-bae7-da766f5b7de5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12cf2d460>]}
[0m22:51:12.486485 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m22:51:12.486672 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '56ddec93-b2ce-43fa-bae7-da766f5b7de5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e9413a0>]}
[0m22:51:12.488424 [info ] [MainThread]: 
[0m22:51:12.488935 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:51:12.489964 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m22:51:12.498642 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m22:51:12.498825 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m22:51:12.498913 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:51:13.120081 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:51:13.120804 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m22:51:13.121637 [debug] [MainThread]: Flushing usage events
[0m22:51:13.269186 [info ] [MainThread]: ctrl-c


============================== 2023-01-24 22:51:16.832707 | fb26c7db-cc8f-4a38-91b3-aa6a2df42df1 ==============================
[0m22:51:16.832750 [info ] [MainThread]: Running with dbt=1.3.2
[0m22:51:16.833463 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:51:16.833692 [debug] [MainThread]: Tracking: tracking
[0m22:51:16.859197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a89b8e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a89bbe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a8ad670>]}
[0m22:51:16.911082 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:51:16.911256 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:51:16.916821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fb26c7db-cc8f-4a38-91b3-aa6a2df42df1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12aaa10d0>]}
[0m22:51:16.925105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fb26c7db-cc8f-4a38-91b3-aa6a2df42df1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a990970>]}
[0m22:51:16.925313 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m22:51:16.925506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fb26c7db-cc8f-4a38-91b3-aa6a2df42df1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a9909a0>]}
[0m22:51:16.926787 [info ] [MainThread]: 
[0m22:51:16.927301 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:51:16.928239 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m22:51:16.973853 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m22:51:16.974033 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m22:51:16.974123 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:51:20.034844 [debug] [ThreadPool]: SQL status: OK in 3.06 seconds
[0m22:51:20.059070 [debug] [ThreadPool]: On list_schemas: Close
[0m22:51:21.252104 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_bruno"
[0m22:51:21.266859 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:51:21.267186 [debug] [ThreadPool]: Using databricks connection "list_None_bruno"
[0m22:51:21.267429 [debug] [ThreadPool]: On list_None_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_bruno"} */
show table extended in bruno like '*'
  
[0m22:51:21.267644 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:51:24.330648 [debug] [ThreadPool]: SQL status: OK in 3.06 seconds
[0m22:51:24.342064 [debug] [ThreadPool]: On list_None_bruno: ROLLBACK
[0m22:51:24.342452 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:51:24.342725 [debug] [ThreadPool]: On list_None_bruno: Close
[0m22:51:25.556577 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fb26c7db-cc8f-4a38-91b3-aa6a2df42df1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c4b05e0>]}
[0m22:51:25.557782 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:51:25.558107 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:51:25.559738 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:51:25.560561 [info ] [MainThread]: 
[0m22:51:25.568989 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m22:51:25.569796 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m22:51:25.571290 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.customers"
[0m22:51:25.571619 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m22:51:25.571923 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m22:51:25.603379 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m22:51:25.604685 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:25.604885 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m22:51:25.630173 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m22:51:25.631593 [debug] [Thread-1  ]: On model.jaffle_shop.customers: 
  
    
import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method="all_purpose_cluster",
        create_notebook=True,
        cluster_id="0124-220040-qfvx2ysq"
    )

    stg_customers_df = dbt.ref('stg_customers')
    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    customer_orders_df = (
        stg_orders_df
        .groupby("customer_id")
        .agg(
            F.min(F.col("order_date")).alias('first_order'),
            F.max(F.col("order_date")).alias('most_recent_order'),
            F.count(F.col("order_id")).alias('number_of_orders')
        )
    )

    customer_payments_df = (
        stg_payments_df
        .join(stg_orders_df, stg_payments_df.order_id == stg_orders_df.order_id, "left")
        .groupby(stg_orders_df.customer_id)
        .agg(
            F.sum(F.col("amount")).alias('total_amount')
        )
    )

    final_df = (
        stg_customers_df.alias("customers") \
            .join(customer_orders_df.alias("customer_orders"), customers.customer_id == customer_orders.customer_id, "left") \
            .join(customer_payments_df.alias("customer_payments"), customers.customer_id == customer_payments.customer_id, "left") \
            .select(customers.customer_id.alias("customer_id"),
                    customers.first_name.alias("first_name"),
                    customers.last_name.alias("last_name"),
                    customer_orders.first_order.alias("first_order"),
                    customer_orders.most_recent_order.alias("most_recent_order"),
                    customer_orders.number_of_orders.alias("number_of_orders"),
                    customer_payments.total_amount.alias("customer_lifetime_value")
            )

    )

    return final_df



# stg_customers_df = spark.table("stg_customers")
# stg_orders_df = spark.table("stg_orders")
# stg_payments_df = spark.table("stg_payments")

# # customer_orders
# customer_orders_df = stg_orders_df.groupBy("customer_id") \
#                                   .agg(min("order_date").alias("first_order"), 
#                                        max("order_date").alias("most_recent_order"), 
#                                        count("order_id").alias("number_of_orders"))

# # customer_payments
# customer_payments_df = stg_payments_df.join(stg_orders_df, stg_payments_df["order_id"] == stg_orders_df["order_id"], "left") \
#                                       .groupBy("customer_id") \
#                                       .agg(sum("amount").alias("total_amount"))

# # final
# final_df = stg_customers_df.join(customer_orders_df, stg_customers_df["customer_id"] == customer_orders_df["customer_id"], "left") \
#                            .join(customer_payments_df, stg_customers_df["customer_id"] == customer_payments_df["customer_id"], "left") \
#                            .selectExpr("customer_id", "first_name", "last_name", "first_order", "most_recent_order", "number_of_orders", "total_amount as customer_lifetime_value")

# final_df.show()


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "bruno.stg_customers", "stg_orders": "bruno.stg_orders", "stg_payments": "bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'None'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist
import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write.mode("overwrite").format("delta").option("overwriteSchema", "true").saveAsTable("bruno.customers")
  
[0m22:51:43.022710 [debug] [Thread-1  ]: finished collecting timing info
[0m22:51:43.024286 [debug] [Thread-1  ]: Runtime Error in model customers (models/customers.py)
  Python model failed with traceback as:
  (Note that the line number here does not match the line number in your code due to dbt templating)
  [0;31m---------------------------------------------------------------------------[0m
  [0;31mNameError[0m                                 Traceback (most recent call last)
  [0;32m<command-2756326064963471>[0m in [0;36m<cell line: 8>[0;34m()[0m
  [1;32m      6[0m [0;31m# --- Autogenerated dbt materialization code. --- #[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  [1;32m      7[0m [0mdbt[0m [0;34m=[0m [0mdbtObj[0m[0;34m([0m[0mspark[0m[0;34m.[0m[0mtable[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0;32m----> 8[0;31m [0mdf[0m [0;34m=[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0mspark[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m      9[0m [0;34m[0m[0m
  [1;32m     10[0m [0;31m# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  
  [0;32m<command-2756326064963469>[0m in [0;36mmodel[0;34m(dbt, session)[0m
  [1;32m     34[0m     final_df = (
  [1;32m     35[0m         [0mstg_customers_df[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"customers"[0m[0;34m)[0m[0;31m [0m[0;31m\[0m[0;34m[0m[0;34m[0m[0m
  [0;32m---> 36[0;31m             [0;34m.[0m[0mjoin[0m[0;34m([0m[0mcustomer_orders_df[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"customer_orders"[0m[0;34m)[0m[0;34m,[0m [0mcustomers[0m[0;34m.[0m[0mcustomer_id[0m [0;34m==[0m [0mcustomer_orders[0m[0;34m.[0m[0mcustomer_id[0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;31m [0m[0;31m\[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m     37[0m             [0;34m.[0m[0mjoin[0m[0;34m([0m[0mcustomer_payments_df[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"customer_payments"[0m[0;34m)[0m[0;34m,[0m [0mcustomers[0m[0;34m.[0m[0mcustomer_id[0m [0;34m==[0m [0mcustomer_payments[0m[0;34m.[0m[0mcustomer_id[0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;31m [0m[0;31m\[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     38[0m             .select(customers.customer_id.alias("customer_id"),
  
  [0;31mNameError[0m: name 'customers' is not defined
[0m22:51:43.024983 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fb26c7db-cc8f-4a38-91b3-aa6a2df42df1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ab68220>]}
[0m22:51:43.025691 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 17.45s]
[0m22:51:43.026601 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m22:51:43.028918 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:51:43.029316 [debug] [MainThread]: On master: ROLLBACK
[0m22:51:43.029582 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:51:44.293414 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:51:44.294926 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:51:44.295471 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:51:44.296063 [debug] [MainThread]: On master: ROLLBACK
[0m22:51:44.296593 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:51:44.297117 [debug] [MainThread]: On master: Close
[0m22:51:45.523793 [info ] [MainThread]: 
[0m22:51:45.525297 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 28.60 seconds (28.60s).
[0m22:51:45.525816 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:51:45.526078 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m22:51:45.549622 [info ] [MainThread]: 
[0m22:51:45.550141 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:51:45.550569 [info ] [MainThread]: 
[0m22:51:45.550954 [error] [MainThread]: [33mRuntime Error in model customers (models/customers.py)[0m
[0m22:51:45.551265 [error] [MainThread]:   Python model failed with traceback as:
[0m22:51:45.551550 [error] [MainThread]:   (Note that the line number here does not match the line number in your code due to dbt templating)
[0m22:51:45.551836 [error] [MainThread]:   [0;31m---------------------------------------------------------------------------[0m
[0m22:51:45.552114 [error] [MainThread]:   [0;31mNameError[0m                                 Traceback (most recent call last)
[0m22:51:45.552389 [error] [MainThread]:   [0;32m<command-2756326064963471>[0m in [0;36m<cell line: 8>[0;34m()[0m
[0m22:51:45.552664 [error] [MainThread]:   [1;32m      6[0m [0;31m# --- Autogenerated dbt materialization code. --- #[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m22:51:45.552939 [error] [MainThread]:   [1;32m      7[0m [0mdbt[0m [0;34m=[0m [0mdbtObj[0m[0;34m([0m[0mspark[0m[0;34m.[0m[0mtable[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:51:45.553216 [error] [MainThread]:   [0;32m----> 8[0;31m [0mdf[0m [0;34m=[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0mspark[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:51:45.553492 [error] [MainThread]:   [0m[1;32m      9[0m [0;34m[0m[0m
[0m22:51:45.553768 [error] [MainThread]:   [1;32m     10[0m [0;31m# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m22:51:45.554044 [error] [MainThread]:   
[0m22:51:45.554318 [error] [MainThread]:   [0;32m<command-2756326064963469>[0m in [0;36mmodel[0;34m(dbt, session)[0m
[0m22:51:45.554591 [error] [MainThread]:   [1;32m     34[0m     final_df = (
[0m22:51:45.554863 [error] [MainThread]:   [1;32m     35[0m         [0mstg_customers_df[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"customers"[0m[0;34m)[0m[0;31m [0m[0;31m\[0m[0;34m[0m[0;34m[0m[0m
[0m22:51:45.555141 [error] [MainThread]:   [0;32m---> 36[0;31m             [0;34m.[0m[0mjoin[0m[0;34m([0m[0mcustomer_orders_df[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"customer_orders"[0m[0;34m)[0m[0;34m,[0m [0mcustomers[0m[0;34m.[0m[0mcustomer_id[0m [0;34m==[0m [0mcustomer_orders[0m[0;34m.[0m[0mcustomer_id[0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;31m [0m[0;31m\[0m[0;34m[0m[0;34m[0m[0m
[0m22:51:45.555425 [error] [MainThread]:   [0m[1;32m     37[0m             [0;34m.[0m[0mjoin[0m[0;34m([0m[0mcustomer_payments_df[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m"customer_payments"[0m[0;34m)[0m[0;34m,[0m [0mcustomers[0m[0;34m.[0m[0mcustomer_id[0m [0;34m==[0m [0mcustomer_payments[0m[0;34m.[0m[0mcustomer_id[0m[0;34m,[0m [0;34m"left"[0m[0;34m)[0m[0;31m [0m[0;31m\[0m[0;34m[0m[0;34m[0m[0m
[0m22:51:45.555702 [error] [MainThread]:   [1;32m     38[0m             .select(customers.customer_id.alias("customer_id"),
[0m22:51:45.555984 [error] [MainThread]:   
[0m22:51:45.556258 [error] [MainThread]:   [0;31mNameError[0m: name 'customers' is not defined
[0m22:51:45.556551 [info ] [MainThread]: 
[0m22:51:45.556839 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:51:45.557256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a831c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a8adaf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12abf46a0>]}
[0m22:51:45.557572 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 22:53:09.291751 | 41bc0d5a-7302-48a5-af81-175f2454fdd6 ==============================
[0m22:53:09.291806 [info ] [MainThread]: Running with dbt=1.3.2
[0m22:53:09.292875 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:53:09.293034 [debug] [MainThread]: Tracking: tracking
[0m22:53:09.324264 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12aaab8e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12aaabbe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12aabe670>]}
[0m22:53:09.385317 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:53:09.385700 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m22:53:09.477065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '41bc0d5a-7302-48a5-af81-175f2454fdd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12aca70d0>]}
[0m22:53:09.484625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '41bc0d5a-7302-48a5-af81-175f2454fdd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ac21430>]}
[0m22:53:09.484819 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m22:53:09.485001 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '41bc0d5a-7302-48a5-af81-175f2454fdd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ac5ffa0>]}
[0m22:53:09.486112 [info ] [MainThread]: 
[0m22:53:09.486581 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:53:09.487426 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m22:53:09.496485 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m22:53:09.496685 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m22:53:09.496783 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:53:11.814817 [debug] [ThreadPool]: SQL status: OK in 2.32 seconds
[0m22:53:11.840994 [debug] [ThreadPool]: On list_schemas: Close
[0m22:53:12.836717 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_bruno"
[0m22:53:12.847559 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:53:12.847887 [debug] [ThreadPool]: Using databricks connection "list_None_bruno"
[0m22:53:12.848130 [debug] [ThreadPool]: On list_None_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_bruno"} */
show table extended in bruno like '*'
  
[0m22:53:12.848331 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:53:15.792156 [debug] [ThreadPool]: SQL status: OK in 2.94 seconds
[0m22:53:15.802578 [debug] [ThreadPool]: On list_None_bruno: ROLLBACK
[0m22:53:15.802884 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:53:15.803105 [debug] [ThreadPool]: On list_None_bruno: Close
[0m22:53:17.075361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '41bc0d5a-7302-48a5-af81-175f2454fdd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ac29dc0>]}
[0m22:53:17.076990 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:53:17.077408 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:53:17.079203 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:53:17.080101 [info ] [MainThread]: 
[0m22:53:17.092698 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m22:53:17.093367 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m22:53:17.094512 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.customers"
[0m22:53:17.094767 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m22:53:17.095008 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m22:53:17.126863 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m22:53:17.128429 [debug] [Thread-1  ]: finished collecting timing info
[0m22:53:17.128641 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m22:53:17.156922 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m22:53:17.158345 [debug] [Thread-1  ]: On model.jaffle_shop.customers: 
  
    
import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method="all_purpose_cluster",
        create_notebook=True,
        cluster_id="0124-220040-qfvx2ysq"
    )

    stg_customers_df = dbt.ref('stg_customers')
    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    customer_orders_df = (
        stg_orders_df
        .groupby("customer_id")
        .agg(
            F.min(F.col("order_date")).alias('first_order'),
            F.max(F.col("order_date")).alias('most_recent_order'),
            F.count(F.col("order_id")).alias('number_of_orders')
        )
    )

    customer_payments_df = (
        stg_payments_df
        .join(stg_orders_df, stg_payments_df.order_id == stg_orders_df.order_id, "left")
        .groupby(stg_orders_df.customer_id)
        .agg(
            F.sum(F.col("amount")).alias('total_amount')
        )
    )

    final_df = (
        stg_customers_df.alias("customers") \
            .join(customer_orders_df.alias("customer_orders"), F.col("customers.customer_id") == F.col("customer_orders.customer_id"), "left") \
            .join(customer_payments_df.alias("customer_payments"), F.col("customers.customer_id") == F.col("customer_payments.customer_id"), "left") \
            .select(F.col("customers.customer_id").alias("customer_id"),
                    F.col("customers.first_name").alias("first_name"),
                    F.col("customers.last_name").alias("last_name"),
                    F.col("customer_orders.first_order").alias("first_order"),
                    F.col("customer_orders.most_recent_order").alias("most_recent_order"),
                    F.col("customer_orders.number_of_orders").alias("number_of_orders"),
                    F.col("customer_payments.total_amount").alias("customer_lifetime_value")
            )

    )

    return final_df



# stg_customers_df = spark.table("stg_customers")
# stg_orders_df = spark.table("stg_orders")
# stg_payments_df = spark.table("stg_payments")

# # customer_orders
# customer_orders_df = stg_orders_df.groupBy("customer_id") \
#                                   .agg(min("order_date").alias("first_order"), 
#                                        max("order_date").alias("most_recent_order"), 
#                                        count("order_id").alias("number_of_orders"))

# # customer_payments
# customer_payments_df = stg_payments_df.join(stg_orders_df, stg_payments_df["order_id"] == stg_orders_df["order_id"], "left") \
#                                       .groupBy("customer_id") \
#                                       .agg(sum("amount").alias("total_amount"))

# # final
# final_df = stg_customers_df.join(customer_orders_df, stg_customers_df["customer_id"] == customer_orders_df["customer_id"], "left") \
#                            .join(customer_payments_df, stg_customers_df["customer_id"] == customer_payments_df["customer_id"], "left") \
#                            .selectExpr("customer_id", "first_name", "last_name", "first_order", "most_recent_order", "number_of_orders", "total_amount as customer_lifetime_value")

# final_df.show()


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "bruno.stg_customers", "stg_orders": "bruno.stg_orders", "stg_payments": "bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'None'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist
import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write.mode("overwrite").format("delta").option("overwriteSchema", "true").saveAsTable("bruno.customers")
  
[0m22:54:46.331085 [debug] [Thread-1  ]: finished collecting timing info
[0m22:54:46.335745 [error] [Thread-1  ]: [31mUnhandled error while executing model.jaffle_shop.customers[0m
HTTPSConnectionPool(host='dbc-dc304b9d-f5ac.cloud.databricks.com', port=443): Max retries exceeded with url: /api/2.1/jobs/runs/get?run_id=2318 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12adca5e0>: Failed to establish a new connection: [Errno 60] Operation timed out'))
[0m22:54:46.336590 [debug] [Thread-1  ]: 
[0m22:54:46.337228 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '41bc0d5a-7302-48a5-af81-175f2454fdd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12adca580>]}
[0m22:54:46.338103 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.customers ....................... [[31mERROR[0m in 89.24s]
[0m22:54:46.339090 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m22:54:46.341443 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:54:46.341810 [debug] [MainThread]: On master: ROLLBACK
[0m22:54:46.342048 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:54:47.600538 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:54:47.603216 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:54:47.603668 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:54:47.604201 [debug] [MainThread]: On master: ROLLBACK
[0m22:54:47.604459 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:54:47.604740 [debug] [MainThread]: On master: Close
[0m22:54:48.814320 [info ] [MainThread]: 
[0m22:54:48.815891 [info ] [MainThread]: Finished running 1 table model in 0 hours 1 minutes and 39.33 seconds (99.33s).
[0m22:54:48.816687 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:54:48.816998 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m22:54:48.844791 [info ] [MainThread]: 
[0m22:54:48.845307 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:54:48.845696 [info ] [MainThread]: 
[0m22:54:48.846026 [error] [MainThread]: [33mHTTPSConnectionPool(host='dbc-dc304b9d-f5ac.cloud.databricks.com', port=443): Max retries exceeded with url: /api/2.1/jobs/runs/get?run_id=2318 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12adca5e0>: Failed to establish a new connection: [Errno 60] Operation timed out'))[0m
[0m22:54:48.846368 [info ] [MainThread]: 
[0m22:54:48.846696 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:54:48.847561 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ac29e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ad6e160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ad45bb0>]}
[0m22:54:48.847923 [debug] [MainThread]: Flushing usage events


============================== 2023-01-24 22:55:13.745452 | f6207fe4-96af-43fc-985c-f746a4735be7 ==============================
[0m22:55:13.745499 [info ] [MainThread]: Running with dbt=1.3.2
[0m22:55:13.746322 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['customers.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:55:13.746484 [debug] [MainThread]: Tracking: tracking
[0m22:55:13.773411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12eaeb880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12eaebb80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12eafe610>]}
[0m22:55:13.833537 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:55:13.833947 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m22:55:13.924394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f6207fe4-96af-43fc-985c-f746a4735be7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ec322e0>]}
[0m22:55:13.932283 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f6207fe4-96af-43fc-985c-f746a4735be7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ec643d0>]}
[0m22:55:13.932515 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m22:55:13.932698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f6207fe4-96af-43fc-985c-f746a4735be7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12eca3040>]}
[0m22:55:13.933868 [info ] [MainThread]: 
[0m22:55:13.934365 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:55:13.935118 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m22:55:13.943059 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m22:55:13.943196 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m22:55:13.943290 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:55:16.698229 [debug] [ThreadPool]: SQL status: OK in 2.75 seconds
[0m22:55:16.722429 [debug] [ThreadPool]: On list_schemas: Close
[0m22:55:17.921980 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_bruno"
[0m22:55:17.936129 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:55:17.936443 [debug] [ThreadPool]: Using databricks connection "list_None_bruno"
[0m22:55:17.936684 [debug] [ThreadPool]: On list_None_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_bruno"} */
show table extended in bruno like '*'
  
[0m22:55:17.936884 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:55:21.434981 [debug] [ThreadPool]: SQL status: OK in 3.5 seconds
[0m22:55:21.442932 [debug] [ThreadPool]: On list_None_bruno: ROLLBACK
[0m22:55:21.443223 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:55:21.443466 [debug] [ThreadPool]: On list_None_bruno: Close
[0m22:55:22.609530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f6207fe4-96af-43fc-985c-f746a4735be7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ec662b0>]}
[0m22:55:22.611230 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:55:22.611642 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:55:22.613309 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:55:22.614047 [info ] [MainThread]: 
[0m22:55:22.625745 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m22:55:22.626253 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.customers ................................ [RUN]
[0m22:55:22.627351 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.customers"
[0m22:55:22.627589 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m22:55:22.627838 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m22:55:22.656523 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m22:55:22.657836 [debug] [Thread-1  ]: finished collecting timing info
[0m22:55:22.658019 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m22:55:22.683133 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m22:55:22.684120 [debug] [Thread-1  ]: On model.jaffle_shop.customers: 
  
    
import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method="all_purpose_cluster",
        create_notebook=False,
        cluster_id="0124-220040-qfvx2ysq"
    )

    stg_customers_df = dbt.ref('stg_customers')
    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    customer_orders_df = (
        stg_orders_df
        .groupby("customer_id")
        .agg(
            F.min(F.col("order_date")).alias('first_order'),
            F.max(F.col("order_date")).alias('most_recent_order'),
            F.count(F.col("order_id")).alias('number_of_orders')
        )
    )

    customer_payments_df = (
        stg_payments_df
        .join(stg_orders_df, stg_payments_df.order_id == stg_orders_df.order_id, "left")
        .groupby(stg_orders_df.customer_id)
        .agg(
            F.sum(F.col("amount")).alias('total_amount')
        )
    )

    final_df = (
        stg_customers_df.alias("customers") \
            .join(customer_orders_df.alias("customer_orders"), F.col("customers.customer_id") == F.col("customer_orders.customer_id"), "left") \
            .join(customer_payments_df.alias("customer_payments"), F.col("customers.customer_id") == F.col("customer_payments.customer_id"), "left") \
            .select(F.col("customers.customer_id").alias("customer_id"),
                    F.col("customers.first_name").alias("first_name"),
                    F.col("customers.last_name").alias("last_name"),
                    F.col("customer_orders.first_order").alias("first_order"),
                    F.col("customer_orders.most_recent_order").alias("most_recent_order"),
                    F.col("customer_orders.number_of_orders").alias("number_of_orders"),
                    F.col("customer_payments.total_amount").alias("customer_lifetime_value")
            )
    )

    return final_df



# stg_customers_df = spark.table("stg_customers")
# stg_orders_df = spark.table("stg_orders")
# stg_payments_df = spark.table("stg_payments")

# # customer_orders
# customer_orders_df = stg_orders_df.groupBy("customer_id") \
#                                   .agg(min("order_date").alias("first_order"), 
#                                        max("order_date").alias("most_recent_order"), 
#                                        count("order_id").alias("number_of_orders"))

# # customer_payments
# customer_payments_df = stg_payments_df.join(stg_orders_df, stg_payments_df["order_id"] == stg_orders_df["order_id"], "left") \
#                                       .groupBy("customer_id") \
#                                       .agg(sum("amount").alias("total_amount"))

# # final
# final_df = stg_customers_df.join(customer_orders_df, stg_customers_df["customer_id"] == customer_orders_df["customer_id"], "left") \
#                            .join(customer_payments_df, stg_customers_df["customer_id"] == customer_payments_df["customer_id"], "left") \
#                            .selectExpr("customer_id", "first_name", "last_name", "first_order", "most_recent_order", "number_of_orders", "total_amount as customer_lifetime_value")

# final_df.show()


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "bruno.stg_customers", "stg_orders": "bruno.stg_orders", "stg_payments": "bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'None'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist
import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write.mode("overwrite").format("delta").option("overwriteSchema", "true").saveAsTable("bruno.customers")
  
[0m22:55:37.736075 [debug] [Thread-1  ]: Execution status: OK in 15.05 seconds
[0m22:55:37.786541 [debug] [Thread-1  ]: finished collecting timing info
[0m22:55:37.787469 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f6207fe4-96af-43fc-985c-f746a4735be7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12edae1f0>]}
[0m22:55:37.787967 [info ] [Thread-1  ]: 1 of 1 OK created python table model bruno.customers ........................... [[32mOK[0m in 15.16s]
[0m22:55:37.788623 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m22:55:37.790231 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:55:37.790458 [debug] [MainThread]: On master: ROLLBACK
[0m22:55:37.790611 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:55:39.007330 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:55:39.008607 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:55:39.009005 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:55:39.009696 [debug] [MainThread]: On master: ROLLBACK
[0m22:55:39.010220 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:55:39.010801 [debug] [MainThread]: On master: Close
[0m22:55:40.183067 [info ] [MainThread]: 
[0m22:55:40.184809 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 26.25 seconds (26.25s).
[0m22:55:40.185910 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:55:40.186482 [debug] [MainThread]: Connection 'model.jaffle_shop.customers' was properly closed.
[0m22:55:40.225042 [info ] [MainThread]: 
[0m22:55:40.225564 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:55:40.225999 [info ] [MainThread]: 
[0m22:55:40.226368 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:55:40.226975 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ec66220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12eb176d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ee04640>]}
[0m22:55:40.227353 [debug] [MainThread]: Flushing usage events


============================== 2023-01-25 01:09:26.580325 | 8ff300a4-118b-484f-b52f-3d57658eac2d ==============================
[0m01:09:26.580388 [info ] [MainThread]: Running with dbt=1.3.2
[0m01:09:26.581611 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['orders.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m01:09:26.581777 [debug] [MainThread]: Tracking: tracking
[0m01:09:26.613117 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e89a910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e89ac10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e8ac6a0>]}
[0m01:09:26.649925 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m01:09:26.650207 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8ff300a4-118b-484f-b52f-3d57658eac2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11042bd60>]}
[0m01:09:26.675727 [debug] [MainThread]: Parsing macros/statement.sql
[0m01:09:26.678515 [debug] [MainThread]: Parsing macros/copy_into.sql
[0m01:09:26.684969 [debug] [MainThread]: Parsing macros/catalog.sql
[0m01:09:26.686666 [debug] [MainThread]: Parsing macros/adapters.sql
[0m01:09:26.704627 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m01:09:26.711347 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m01:09:26.711747 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m01:09:26.714850 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m01:09:26.732288 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m01:09:26.733444 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m01:09:26.739408 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m01:09:26.740152 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m01:09:26.740999 [debug] [MainThread]: Parsing macros/adapters.sql
[0m01:09:26.776766 [debug] [MainThread]: Parsing macros/apply_grants.sql
[0m01:09:26.779085 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m01:09:26.819370 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m01:09:26.819887 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m01:09:26.824899 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m01:09:26.847805 [debug] [MainThread]: Parsing macros/materializations/incremental/column_helpers.sql
[0m01:09:26.849382 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
[0m01:09:26.853283 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m01:09:26.860167 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m01:09:26.865575 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m01:09:26.865933 [debug] [MainThread]: Parsing macros/utils/assert_not_null.sql
[0m01:09:26.866823 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m01:09:26.871337 [debug] [MainThread]: Parsing macros/utils/timestamps.sql
[0m01:09:26.871590 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m01:09:26.872846 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m01:09:26.884204 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m01:09:26.884528 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m01:09:26.884882 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m01:09:26.885202 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m01:09:26.886158 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m01:09:26.886522 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m01:09:26.886932 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m01:09:26.889673 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m01:09:26.891290 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m01:09:26.892469 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m01:09:26.904658 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m01:09:26.915487 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m01:09:26.925383 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m01:09:26.928675 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m01:09:26.929944 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m01:09:26.931211 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m01:09:26.937204 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m01:09:26.950179 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m01:09:26.951261 [debug] [MainThread]: Parsing macros/materializations/models/incremental/strategies.sql
[0m01:09:26.956209 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m01:09:26.964168 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m01:09:26.978403 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m01:09:26.982559 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m01:09:26.985064 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m01:09:26.989128 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m01:09:26.990042 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m01:09:26.992458 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m01:09:26.994037 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m01:09:26.999328 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m01:09:27.014280 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m01:09:27.015325 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m01:09:27.017106 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m01:09:27.018220 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m01:09:27.018853 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m01:09:27.019413 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m01:09:27.019889 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m01:09:27.020879 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m01:09:27.024694 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m01:09:27.031128 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m01:09:27.031688 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m01:09:27.032550 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m01:09:27.033212 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m01:09:27.033851 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m01:09:27.034728 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m01:09:27.035276 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m01:09:27.035986 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m01:09:27.036952 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m01:09:27.038669 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m01:09:27.039531 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m01:09:27.040271 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m01:09:27.040999 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m01:09:27.041712 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m01:09:27.042342 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m01:09:27.043094 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m01:09:27.043713 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m01:09:27.048691 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m01:09:27.049419 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m01:09:27.050051 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m01:09:27.051318 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m01:09:27.052917 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m01:09:27.053624 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m01:09:27.054664 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m01:09:27.055412 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m01:09:27.056899 [debug] [MainThread]: Parsing macros/adapters/timestamps.sql
[0m01:09:27.059229 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m01:09:27.061188 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m01:09:27.072567 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m01:09:27.073960 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m01:09:27.083972 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m01:09:27.087187 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m01:09:27.092538 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m01:09:27.099861 [debug] [MainThread]: Parsing macros/python_model/python.sql
[0m01:09:27.104377 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m01:09:27.366090 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_customers.sql
[0m01:09:27.375259 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_payments.sql
[0m01:09:27.377389 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_orders.sql
[0m01:09:27.513923 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8ff300a4-118b-484f-b52f-3d57658eac2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12eb7a0d0>]}
[0m01:09:27.521665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8ff300a4-118b-484f-b52f-3d57658eac2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e8acb20>]}
[0m01:09:27.521873 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m01:09:27.522055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8ff300a4-118b-484f-b52f-3d57658eac2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e925070>]}
[0m01:09:27.523184 [info ] [MainThread]: 
[0m01:09:27.523672 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m01:09:27.524373 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m01:09:27.533371 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m01:09:27.533558 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:09:27.533650 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:13:05.761097 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:13:05.762890 [debug] [MainThread]: Connection 'list_schemas' was left open.
[0m01:13:05.763272 [debug] [MainThread]: On list_schemas: Close
[0m01:13:05.965191 [debug] [MainThread]: Flushing usage events
[0m01:13:06.128067 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:13:06.129131 [debug] [MainThread]: Connection 'list_schemas' was left open.
[0m01:13:06.129473 [debug] [MainThread]: On list_schemas: Close
[0m01:13:06.321538 [error] [MainThread]: Encountered an error:
'NoneType' object has no attribute 'close'
[0m01:13:06.388965 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/task/runnable.py", line 556, in create_schemas
    for ls_future in as_completed(list_futures):
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py", line 245, in as_completed
    waiter.event.wait(wait_timeout)
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py", line 574, in wait
    signaled = self._cond.wait(timeout)
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py", line 312, in wait
    waiter.acquire()
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/task/runnable.py", line 434, in execute_with_hooks
    self.before_run(adapter, selected_uids)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/task/run.py", line 425, in before_run
    self.create_schemas(adapter, required_schemas)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/task/runnable.py", line 578, in create_schemas
    create_future.result()
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py", line 636, in __exit__
    self.shutdown(wait=True)
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/thread.py", line 229, in shutdown
    t.join()
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py", line 1053, in join
    self._wait_for_tstate_lock()
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py", line 1069, in _wait_for_tstate_lock
    elif lock.acquire(block, timeout):
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/main.py", line 208, in track_run
    yield
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/main.py", line 245, in run_from_args
    results = task.run()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/task/runnable.py", line 472, in run
    result = self.execute_with_hooks(selected_uids)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/task/runnable.py", line 441, in execute_with_hooks
    adapter.cleanup_connections()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 228, in cleanup_connections
    self.connections.cleanup_all()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/base/connections.py", line 310, in cleanup_all
    self.close(connection)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/base/connections.py", line 372, in close
    cls._close_handle(connection)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/base/connections.py", line 343, in _close_handle
    connection.handle.close()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 266, in close
    self._conn.close()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/databricks/sql/client.py", line 241, in close
    self._close()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/databricks/sql/client.py", line 247, in _close
    self.thrift_backend.close_session(self._session_handle)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 475, in close_session
    self.make_request(self._client.CloseSession, req)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 380, in make_request
    with self._request_lock:
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/main.py", line 169, in adapter_management
    yield
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/main.py", line 198, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/main.py", line 245, in run_from_args
    results = task.run()
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py", line 135, in __exit__
    self.gen.throw(type, value, traceback)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/main.py", line 217, in track_run
    dbt.tracking.flush()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/tracking.py", line 431, in flush
    tracker.flush()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/snowplow_tracker/tracker.py", line 218, in flush
    emitter.sync_flush()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/snowplow_tracker/emitters.py", line 182, in sync_flush
    Emitter.flush(self)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/snowplow_tracker/emitters.py", line 149, in flush
    self.send_events(self.buffer)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/snowplow_tracker/emitters.py", line 206, in send_events
    status_code = self.http_post(data).status_code
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/tracking.py", line 84, in http_post
    r = requests.post(
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/requests/sessions.py", line 587, in request
    resp = self.send(prep, **send_kwargs)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/requests/sessions.py", line 701, in send
    r = adapter.send(request, **kwargs)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/requests/adapters.py", line 489, in send
    resp = conn.urlopen(
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/urllib3/connectionpool.py", line 386, in _make_request
    self._validate_conn(conn)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/urllib3/connectionpool.py", line 1042, in _validate_conn
    conn.connect()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/urllib3/connection.py", line 358, in connect
    self.sock = conn = self._new_conn()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 475, in close_session
    self.make_request(self._client.CloseSession, req)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 380, in make_request
    with self._request_lock:
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/main.py", line 135, in main
    results, succeeded = handle_and_check(args)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/main.py", line 199, in handle_and_check
    success = task.interpret_results(res)
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py", line 135, in __exit__
    self.gen.throw(type, value, traceback)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/main.py", line 171, in adapter_management
    cleanup_connections()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/factory.py", line 185, in cleanup_connections
    FACTORY.cleanup_connections()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/factory.py", line 116, in cleanup_connections
    adapter.cleanup_connections()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 228, in cleanup_connections
    self.connections.cleanup_all()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/base/connections.py", line 310, in cleanup_all
    self.close(connection)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/base/connections.py", line 372, in close
    cls._close_handle(connection)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/base/connections.py", line 343, in _close_handle
    connection.handle.close()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 266, in close
    self._conn.close()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/databricks/sql/client.py", line 241, in close
    self._close()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/databricks/sql/client.py", line 247, in _close
    self.thrift_backend.close_session(self._session_handle)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 477, in close_session
    self._transport.close()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/thrift/transport/THttpClient.py", line 122, in close
    self.__http.close()
AttributeError: 'NoneType' object has no attribute 'close'



============================== 2023-01-25 01:17:05.041724 | ee4c3a4d-89ec-411c-ae9f-854625b1eaad ==============================
[0m01:17:05.041852 [info ] [MainThread]: Running with dbt=1.3.2
[0m01:17:05.043012 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['orders.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m01:17:05.043196 [debug] [MainThread]: Tracking: tracking
[0m01:17:05.070806 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a4ab8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a4abbb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a4be640>]}
[0m01:17:05.131334 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m01:17:05.131743 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/orders.py
[0m01:17:05.131939 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m01:17:05.260617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ee4c3a4d-89ec-411c-ae9f-854625b1eaad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a78e0d0>]}
[0m01:17:05.268395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ee4c3a4d-89ec-411c-ae9f-854625b1eaad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a65ef40>]}
[0m01:17:05.268602 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m01:17:05.268794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ee4c3a4d-89ec-411c-ae9f-854625b1eaad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a663040>]}
[0m01:17:05.269926 [info ] [MainThread]: 
[0m01:17:05.270400 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m01:17:05.271209 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m01:17:05.280306 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m01:17:05.280496 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:17:05.280594 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:17:43.577089 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:17:43.578795 [debug] [MainThread]: Connection 'list_schemas' was left open.
[0m01:17:43.579468 [debug] [MainThread]: On list_schemas: Close
[0m01:17:43.739105 [debug] [MainThread]: Flushing usage events
[0m01:17:43.891069 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:17:43.891828 [debug] [MainThread]: Connection 'list_schemas' was left open.
[0m01:17:43.892254 [debug] [MainThread]: On list_schemas: Close
[0m01:17:44.025099 [error] [MainThread]: Encountered an error:
'NoneType' object has no attribute 'close'
[0m01:17:44.092402 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/task/runnable.py", line 556, in create_schemas
    for ls_future in as_completed(list_futures):
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py", line 245, in as_completed
    waiter.event.wait(wait_timeout)
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py", line 574, in wait
    signaled = self._cond.wait(timeout)
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py", line 312, in wait
    waiter.acquire()
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/task/runnable.py", line 434, in execute_with_hooks
    self.before_run(adapter, selected_uids)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/task/run.py", line 425, in before_run
    self.create_schemas(adapter, required_schemas)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/task/runnable.py", line 578, in create_schemas
    create_future.result()
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py", line 636, in __exit__
    self.shutdown(wait=True)
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/thread.py", line 229, in shutdown
    t.join()
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py", line 1053, in join
    self._wait_for_tstate_lock()
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py", line 1069, in _wait_for_tstate_lock
    elif lock.acquire(block, timeout):
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/main.py", line 208, in track_run
    yield
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/main.py", line 245, in run_from_args
    results = task.run()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/task/runnable.py", line 472, in run
    result = self.execute_with_hooks(selected_uids)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/task/runnable.py", line 441, in execute_with_hooks
    adapter.cleanup_connections()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 228, in cleanup_connections
    self.connections.cleanup_all()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/base/connections.py", line 310, in cleanup_all
    self.close(connection)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/base/connections.py", line 372, in close
    cls._close_handle(connection)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/base/connections.py", line 343, in _close_handle
    connection.handle.close()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 266, in close
    self._conn.close()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/databricks/sql/client.py", line 241, in close
    self._close()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/databricks/sql/client.py", line 247, in _close
    self.thrift_backend.close_session(self._session_handle)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 475, in close_session
    self.make_request(self._client.CloseSession, req)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 380, in make_request
    with self._request_lock:
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/main.py", line 169, in adapter_management
    yield
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/main.py", line 198, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/main.py", line 245, in run_from_args
    results = task.run()
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py", line 135, in __exit__
    self.gen.throw(type, value, traceback)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/main.py", line 217, in track_run
    dbt.tracking.flush()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/tracking.py", line 431, in flush
    tracker.flush()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/snowplow_tracker/tracker.py", line 218, in flush
    emitter.sync_flush()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/snowplow_tracker/emitters.py", line 182, in sync_flush
    Emitter.flush(self)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/snowplow_tracker/emitters.py", line 149, in flush
    self.send_events(self.buffer)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/snowplow_tracker/emitters.py", line 206, in send_events
    status_code = self.http_post(data).status_code
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/tracking.py", line 84, in http_post
    r = requests.post(
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/requests/sessions.py", line 587, in request
    resp = self.send(prep, **send_kwargs)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/requests/sessions.py", line 701, in send
    r = adapter.send(request, **kwargs)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/requests/adapters.py", line 489, in send
    resp = conn.urlopen(
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/urllib3/connectionpool.py", line 386, in _make_request
    self._validate_conn(conn)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/urllib3/connectionpool.py", line 1042, in _validate_conn
    conn.connect()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/urllib3/connection.py", line 358, in connect
    self.sock = conn = self._new_conn()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 475, in close_session
    self.make_request(self._client.CloseSession, req)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 380, in make_request
    with self._request_lock:
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/main.py", line 135, in main
    results, succeeded = handle_and_check(args)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/main.py", line 199, in handle_and_check
    success = task.interpret_results(res)
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py", line 135, in __exit__
    self.gen.throw(type, value, traceback)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/main.py", line 171, in adapter_management
    cleanup_connections()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/factory.py", line 185, in cleanup_connections
    FACTORY.cleanup_connections()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/factory.py", line 116, in cleanup_connections
    adapter.cleanup_connections()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 228, in cleanup_connections
    self.connections.cleanup_all()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/base/connections.py", line 310, in cleanup_all
    self.close(connection)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/base/connections.py", line 372, in close
    cls._close_handle(connection)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/base/connections.py", line 343, in _close_handle
    connection.handle.close()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 266, in close
    self._conn.close()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/databricks/sql/client.py", line 241, in close
    self._close()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/databricks/sql/client.py", line 247, in _close
    self.thrift_backend.close_session(self._session_handle)
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 477, in close_session
    self._transport.close()
  File "/Users/bruno/Datafold/dbt_python/databricks/venv/lib/python3.9/site-packages/thrift/transport/THttpClient.py", line 122, in close
    self.__http.close()
AttributeError: 'NoneType' object has no attribute 'close'



============================== 2023-01-25 01:27:16.364336 | dfbf61a3-9cee-45fd-a6e9-5a8f4f56f586 ==============================
[0m01:27:16.364383 [info ] [MainThread]: Running with dbt=1.3.2
[0m01:27:16.365202 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['orders.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m01:27:16.365361 [debug] [MainThread]: Tracking: tracking
[0m01:27:16.400132 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12cf258e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12cf25be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12cf37670>]}
[0m01:27:16.460681 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:27:16.460858 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:27:16.466700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dfbf61a3-9cee-45fd-a6e9-5a8f4f56f586', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12d12c0d0>]}
[0m01:27:16.474995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dfbf61a3-9cee-45fd-a6e9-5a8f4f56f586', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12d03d970>]}
[0m01:27:16.475208 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m01:27:16.475400 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dfbf61a3-9cee-45fd-a6e9-5a8f4f56f586', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12d03d9a0>]}
[0m01:27:16.476709 [info ] [MainThread]: 
[0m01:27:16.477183 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m01:27:16.478057 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m01:27:16.523337 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m01:27:16.523537 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:27:16.523627 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:27:19.649542 [debug] [ThreadPool]: SQL status: OK in 3.13 seconds
[0m01:27:19.675400 [debug] [ThreadPool]: On list_schemas: Close
[0m01:27:20.970512 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_bruno"
[0m01:27:20.984440 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:27:20.984731 [debug] [ThreadPool]: Using databricks connection "list_None_bruno"
[0m01:27:20.984965 [debug] [ThreadPool]: On list_None_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_bruno"} */
show table extended in bruno like '*'
  
[0m01:27:20.985166 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:27:27.967319 [debug] [ThreadPool]: SQL status: OK in 6.98 seconds
[0m01:27:27.983141 [debug] [ThreadPool]: On list_None_bruno: ROLLBACK
[0m01:27:27.983866 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m01:27:27.984185 [debug] [ThreadPool]: On list_None_bruno: Close
[0m01:27:29.159607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dfbf61a3-9cee-45fd-a6e9-5a8f4f56f586', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12d14d610>]}
[0m01:27:29.160871 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:27:29.161439 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:27:29.164056 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:27:29.165167 [info ] [MainThread]: 
[0m01:27:29.179547 [debug] [Thread-1  ]: Began running node model.jaffle_shop.orders
[0m01:27:29.180638 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.orders ................................... [RUN]
[0m01:27:29.182518 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.orders"
[0m01:27:29.182928 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.orders
[0m01:27:29.183296 [debug] [Thread-1  ]: Compiling model.jaffle_shop.orders
[0m01:27:29.217806 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.orders"
[0m01:27:29.220150 [debug] [Thread-1  ]: finished collecting timing info
[0m01:27:29.220353 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.orders
[0m01:27:29.244477 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.orders"
[0m01:27:29.244984 [debug] [Thread-1  ]: On model.jaffle_shop.orders: 
  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    dbt.config(
        submission_method='all_purpose_cluster',
        create_notebook=False,
        cluster_id='0124-220040-qfvx2ysq'
    )

    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    payment_methods = ['credit_card', 'coupon', 'bank_transfer', 'gift_card']

    agg_list = [F.sum(F.when(stg_payments_df.payment_method == payment_method, stg_payments_df.amount).otherwise(0)).alias(payment_method + '_amount') for payment_method in payment_methods]

    agg_list.append(F.sum(F.col('amount')).alias('total_amount'))

    order_payments_df = (
        stg_payments_df
        .group_by('order_id')
        .agg(*agg_list)
    )

    final_df = (
        stg_orders_df
        .join(order_payments, stg_orders_df.order_id == order_payments.order_id, 'left')
        .select(stg_orders_df.order_id.alias('order_id'),
                stg_orders_df.customer_id.alias('customer_id'),
                stg_orders_df.order_date.alias('order_date'),
                stg_orders_df.status.alias('status'),
                *[F.col(payment_method + '_amount') for payment_method in payment_methods],
                order_payments_df.total_amount.alias('amount')
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_orders": "bruno.stg_orders", "stg_payments": "bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'None'
    schema = 'bruno'
    identifier = 'orders'
    def __repr__(self):
        return 'bruno.orders'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist
import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write.mode("overwrite").format("delta").option("overwriteSchema", "true").saveAsTable("bruno.orders")
  
[0m01:27:30.588622 [debug] [Thread-1  ]: finished collecting timing info
[0m01:27:30.590383 [debug] [Thread-1  ]: Runtime Error in model orders (models/orders.py)
  Error creating an execution context.
   b'{"error":"ClusterNotReadyException: Cluster 0124-220040-qfvx2ysq not currently ready for driver client (currently Terminated)"}\n'
[0m01:27:30.591036 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dfbf61a3-9cee-45fd-a6e9-5a8f4f56f586', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12d1fecd0>]}
[0m01:27:30.591699 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.orders .......................... [[31mERROR[0m in 1.41s]
[0m01:27:30.592667 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.orders
[0m01:27:30.594891 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m01:27:30.595231 [debug] [MainThread]: On master: ROLLBACK
[0m01:27:30.595480 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:27:31.819389 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:27:31.820826 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:27:31.821378 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:27:31.821974 [debug] [MainThread]: On master: ROLLBACK
[0m01:27:31.822527 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:27:31.823092 [debug] [MainThread]: On master: Close
[0m01:27:33.049870 [info ] [MainThread]: 
[0m01:27:33.051164 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 16.57 seconds (16.57s).
[0m01:27:33.051930 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:27:33.052252 [debug] [MainThread]: Connection 'model.jaffle_shop.orders' was properly closed.
[0m01:27:33.074801 [info ] [MainThread]: 
[0m01:27:33.075350 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m01:27:33.075789 [info ] [MainThread]: 
[0m01:27:33.076151 [error] [MainThread]: [33mRuntime Error in model orders (models/orders.py)[0m
[0m01:27:33.076496 [error] [MainThread]:   Error creating an execution context.
[0m01:27:33.076821 [error] [MainThread]:    b'{"error":"ClusterNotReadyException: Cluster 0124-220040-qfvx2ysq not currently ready for driver client (currently Terminated)"}\n'
[0m01:27:33.077120 [info ] [MainThread]: 
[0m01:27:33.077407 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:27:33.077835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12cf37df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12d1f8fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12d1cba00>]}
[0m01:27:33.078172 [debug] [MainThread]: Flushing usage events


============================== 2023-01-25 01:28:04.931063 | 0f84a39d-49fa-49b7-80d9-ef75095c9165 ==============================
[0m01:28:04.931113 [info ] [MainThread]: Running with dbt=1.3.2
[0m01:28:04.931937 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['orders.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m01:28:04.932084 [debug] [MainThread]: Tracking: tracking
[0m01:28:04.959732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a69a850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a69ab50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a6ae5e0>]}
[0m01:28:05.018055 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:28:05.018228 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:28:05.024234 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0f84a39d-49fa-49b7-80d9-ef75095c9165', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a8a10d0>]}
[0m01:28:05.032324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0f84a39d-49fa-49b7-80d9-ef75095c9165', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a7b28e0>]}
[0m01:28:05.032533 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m01:28:05.032727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0f84a39d-49fa-49b7-80d9-ef75095c9165', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a7b2910>]}
[0m01:28:05.034093 [info ] [MainThread]: 
[0m01:28:05.034607 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m01:28:05.035523 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m01:28:05.080612 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m01:28:05.080794 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:28:05.080888 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:28:07.675506 [debug] [ThreadPool]: SQL status: OK in 2.59 seconds
[0m01:28:07.701927 [debug] [ThreadPool]: On list_schemas: Close
[0m01:28:08.793534 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_bruno"
[0m01:28:08.807627 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:28:08.807916 [debug] [ThreadPool]: Using databricks connection "list_None_bruno"
[0m01:28:08.808158 [debug] [ThreadPool]: On list_None_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_bruno"} */
show table extended in bruno like '*'
  
[0m01:28:08.808375 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:28:12.067846 [debug] [ThreadPool]: SQL status: OK in 3.26 seconds
[0m01:28:12.079764 [debug] [ThreadPool]: On list_None_bruno: ROLLBACK
[0m01:28:12.080287 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m01:28:12.080561 [debug] [ThreadPool]: On list_None_bruno: Close
[0m01:28:13.189013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0f84a39d-49fa-49b7-80d9-ef75095c9165', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c2a3cd0>]}
[0m01:28:13.190560 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:28:13.191292 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:28:13.192788 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:28:13.193346 [info ] [MainThread]: 
[0m01:28:13.200936 [debug] [Thread-1  ]: Began running node model.jaffle_shop.orders
[0m01:28:13.201566 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.orders ................................... [RUN]
[0m01:28:13.202930 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.orders"
[0m01:28:13.203243 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.orders
[0m01:28:13.203537 [debug] [Thread-1  ]: Compiling model.jaffle_shop.orders
[0m01:28:13.234557 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.orders"
[0m01:28:13.235209 [debug] [Thread-1  ]: finished collecting timing info
[0m01:28:13.235406 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.orders
[0m01:28:13.260330 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.orders"
[0m01:28:13.261141 [debug] [Thread-1  ]: On model.jaffle_shop.orders: 
  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    dbt.config(
        submission_method='all_purpose_cluster',
        create_notebook=False,
        cluster_id='0124-220040-qfvx2ysq'
    )

    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    payment_methods = ['credit_card', 'coupon', 'bank_transfer', 'gift_card']

    agg_list = [F.sum(F.when(stg_payments_df.payment_method == payment_method, stg_payments_df.amount).otherwise(0)).alias(payment_method + '_amount') for payment_method in payment_methods]

    agg_list.append(F.sum(F.col('amount')).alias('total_amount'))

    order_payments_df = (
        stg_payments_df
        .group_by('order_id')
        .agg(*agg_list)
    )

    final_df = (
        stg_orders_df
        .join(order_payments, stg_orders_df.order_id == order_payments.order_id, 'left')
        .select(stg_orders_df.order_id.alias('order_id'),
                stg_orders_df.customer_id.alias('customer_id'),
                stg_orders_df.order_date.alias('order_date'),
                stg_orders_df.status.alias('status'),
                *[F.col(payment_method + '_amount') for payment_method in payment_methods],
                order_payments_df.total_amount.alias('amount')
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_orders": "bruno.stg_orders", "stg_payments": "bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'None'
    schema = 'bruno'
    identifier = 'orders'
    def __repr__(self):
        return 'bruno.orders'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist
import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write.mode("overwrite").format("delta").option("overwriteSchema", "true").saveAsTable("bruno.orders")
  
[0m01:28:14.543999 [debug] [Thread-1  ]: finished collecting timing info
[0m01:28:14.545668 [debug] [Thread-1  ]: Runtime Error in model orders (models/orders.py)
  Error creating an execution context.
   b'{"error":"ClusterNotReadyException: Cluster 0124-220040-qfvx2ysq not currently ready for driver client (currently Pending)"}\n'
[0m01:28:14.546282 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0f84a39d-49fa-49b7-80d9-ef75095c9165', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a973c40>]}
[0m01:28:14.546987 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.orders .......................... [[31mERROR[0m in 1.34s]
[0m01:28:14.548048 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.orders
[0m01:28:14.550355 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m01:28:14.550778 [debug] [MainThread]: On master: ROLLBACK
[0m01:28:14.551046 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:28:15.611176 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:28:15.612623 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:28:15.613171 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:28:15.613816 [debug] [MainThread]: On master: ROLLBACK
[0m01:28:15.614370 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:28:15.614931 [debug] [MainThread]: On master: Close
[0m01:28:16.676010 [info ] [MainThread]: 
[0m01:28:16.677702 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 11.64 seconds (11.64s).
[0m01:28:16.678384 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:28:16.678785 [debug] [MainThread]: Connection 'model.jaffle_shop.orders' was properly closed.
[0m01:28:16.703088 [info ] [MainThread]: 
[0m01:28:16.703618 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m01:28:16.704056 [info ] [MainThread]: 
[0m01:28:16.704412 [error] [MainThread]: [33mRuntime Error in model orders (models/orders.py)[0m
[0m01:28:16.704755 [error] [MainThread]:   Error creating an execution context.
[0m01:28:16.705082 [error] [MainThread]:    b'{"error":"ClusterNotReadyException: Cluster 0124-220040-qfvx2ysq not currently ready for driver client (currently Pending)"}\n'
[0m01:28:16.705382 [info ] [MainThread]: 
[0m01:28:16.705670 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:28:16.706088 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a96ef40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a631be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a941910>]}
[0m01:28:16.706412 [debug] [MainThread]: Flushing usage events


============================== 2023-01-25 01:34:11.477681 | c2639a80-10de-4a57-865b-b580abee6f63 ==============================
[0m01:34:11.477743 [info ] [MainThread]: Running with dbt=1.3.2
[0m01:34:11.478576 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['orders.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m01:34:11.478722 [debug] [MainThread]: Tracking: tracking
[0m01:34:11.503986 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a69a8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a69abb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a6ac640>]}
[0m01:34:11.564290 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:34:11.564481 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:34:11.570622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c2639a80-10de-4a57-865b-b580abee6f63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a8a10d0>]}
[0m01:34:11.579528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c2639a80-10de-4a57-865b-b580abee6f63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a7b2940>]}
[0m01:34:11.579785 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m01:34:11.579979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c2639a80-10de-4a57-865b-b580abee6f63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a7b2970>]}
[0m01:34:11.581352 [info ] [MainThread]: 
[0m01:34:11.581845 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m01:34:11.582678 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m01:34:11.628125 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m01:34:11.628321 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:34:11.628415 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:34:14.501254 [debug] [ThreadPool]: SQL status: OK in 2.87 seconds
[0m01:34:14.529506 [debug] [ThreadPool]: On list_schemas: Close
[0m01:34:15.723307 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_bruno"
[0m01:34:15.745912 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:34:15.746415 [debug] [ThreadPool]: Using databricks connection "list_None_bruno"
[0m01:34:15.746711 [debug] [ThreadPool]: On list_None_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_bruno"} */
show table extended in bruno like '*'
  
[0m01:34:15.746956 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:34:18.878581 [debug] [ThreadPool]: SQL status: OK in 3.13 seconds
[0m01:34:18.894723 [debug] [ThreadPool]: On list_None_bruno: ROLLBACK
[0m01:34:18.895662 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m01:34:18.895994 [debug] [ThreadPool]: On list_None_bruno: Close
[0m01:34:20.004030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c2639a80-10de-4a57-865b-b580abee6f63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a7b2370>]}
[0m01:34:20.005963 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:34:20.006534 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:34:20.008647 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:34:20.009784 [info ] [MainThread]: 
[0m01:34:20.021244 [debug] [Thread-1  ]: Began running node model.jaffle_shop.orders
[0m01:34:20.022419 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.orders ................................... [RUN]
[0m01:34:20.024470 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.orders"
[0m01:34:20.025030 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.orders
[0m01:34:20.025355 [debug] [Thread-1  ]: Compiling model.jaffle_shop.orders
[0m01:34:20.060577 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.orders"
[0m01:34:20.061251 [debug] [Thread-1  ]: finished collecting timing info
[0m01:34:20.061427 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.orders
[0m01:34:20.085932 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.orders"
[0m01:34:20.086782 [debug] [Thread-1  ]: On model.jaffle_shop.orders: 
  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    dbt.config(
        submission_method='all_purpose_cluster',
        create_notebook=False,
        cluster_id='0124-220040-qfvx2ysq'
    )

    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    payment_methods = ['credit_card', 'coupon', 'bank_transfer', 'gift_card']

    agg_list = [F.sum(F.when(stg_payments_df.payment_method == payment_method, stg_payments_df.amount).otherwise(0)).alias(payment_method + '_amount') for payment_method in payment_methods]

    agg_list.append(F.sum(F.col('amount')).alias('total_amount'))

    order_payments_df = (
        stg_payments_df
        .group_by('order_id')
        .agg(*agg_list)
    )

    final_df = (
        stg_orders_df
        .join(order_payments, stg_orders_df.order_id == order_payments.order_id, 'left')
        .select(stg_orders_df.order_id.alias('order_id'),
                stg_orders_df.customer_id.alias('customer_id'),
                stg_orders_df.order_date.alias('order_date'),
                stg_orders_df.status.alias('status'),
                *[F.col(payment_method + '_amount') for payment_method in payment_methods],
                order_payments_df.total_amount.alias('amount')
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_orders": "bruno.stg_orders", "stg_payments": "bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'None'
    schema = 'bruno'
    identifier = 'orders'
    def __repr__(self):
        return 'bruno.orders'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist
import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write.mode("overwrite").format("delta").option("overwriteSchema", "true").saveAsTable("bruno.orders")
  
[0m01:34:21.392773 [debug] [Thread-1  ]: finished collecting timing info
[0m01:34:21.395181 [debug] [Thread-1  ]: Runtime Error in model orders (models/orders.py)
  Error creating an execution context.
   b'{"error":"ClusterNotReadyException: Cluster 0124-220040-qfvx2ysq not currently ready for driver client (currently Pending)"}\n'
[0m01:34:21.396059 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c2639a80-10de-4a57-865b-b580abee6f63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a972ca0>]}
[0m01:34:21.397280 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.orders .......................... [[31mERROR[0m in 1.37s]
[0m01:34:21.398461 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.orders
[0m01:34:21.400763 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m01:34:21.401178 [debug] [MainThread]: On master: ROLLBACK
[0m01:34:21.401447 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:34:22.556367 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:34:22.557589 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:34:22.557929 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:34:22.558315 [debug] [MainThread]: On master: ROLLBACK
[0m01:34:22.558642 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:34:22.558962 [debug] [MainThread]: On master: Close
[0m01:34:23.706234 [info ] [MainThread]: 
[0m01:34:23.707636 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 12.12 seconds (12.12s).
[0m01:34:23.708333 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:34:23.708636 [debug] [MainThread]: Connection 'model.jaffle_shop.orders' was properly closed.
[0m01:34:23.731524 [info ] [MainThread]: 
[0m01:34:23.732040 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m01:34:23.732470 [info ] [MainThread]: 
[0m01:34:23.732825 [error] [MainThread]: [33mRuntime Error in model orders (models/orders.py)[0m
[0m01:34:23.733167 [error] [MainThread]:   Error creating an execution context.
[0m01:34:23.733493 [error] [MainThread]:    b'{"error":"ClusterNotReadyException: Cluster 0124-220040-qfvx2ysq not currently ready for driver client (currently Pending)"}\n'
[0m01:34:23.733799 [info ] [MainThread]: 
[0m01:34:23.734085 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:34:23.734506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a6c62b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a6acca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a93f910>]}
[0m01:34:23.734829 [debug] [MainThread]: Flushing usage events


============================== 2023-01-25 01:35:59.689551 | 6bde175c-5c1e-4854-9d1c-36356e6f877f ==============================
[0m01:35:59.689592 [info ] [MainThread]: Running with dbt=1.3.2
[0m01:35:59.690605 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['orders.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m01:35:59.690771 [debug] [MainThread]: Tracking: tracking
[0m01:35:59.719218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c98a910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c98ac10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c99c6a0>]}
[0m01:35:59.780059 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:35:59.780236 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:35:59.786134 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6bde175c-5c1e-4854-9d1c-36356e6f877f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12cb900d0>]}
[0m01:35:59.794640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6bde175c-5c1e-4854-9d1c-36356e6f877f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12caa59a0>]}
[0m01:35:59.794862 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m01:35:59.795056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6bde175c-5c1e-4854-9d1c-36356e6f877f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e572d90>]}
[0m01:35:59.796397 [info ] [MainThread]: 
[0m01:35:59.796882 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m01:35:59.797695 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m01:35:59.843552 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m01:35:59.843725 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:35:59.843822 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:36:02.634872 [debug] [ThreadPool]: SQL status: OK in 2.79 seconds
[0m01:36:02.660384 [debug] [ThreadPool]: On list_schemas: Close
[0m01:36:03.968116 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_bruno"
[0m01:36:03.983363 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:36:03.983738 [debug] [ThreadPool]: Using databricks connection "list_None_bruno"
[0m01:36:03.983988 [debug] [ThreadPool]: On list_None_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_bruno"} */
show table extended in bruno like '*'
  
[0m01:36:03.984212 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:36:07.193970 [debug] [ThreadPool]: SQL status: OK in 3.21 seconds
[0m01:36:07.203782 [debug] [ThreadPool]: On list_None_bruno: ROLLBACK
[0m01:36:07.204134 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m01:36:07.204357 [debug] [ThreadPool]: On list_None_bruno: Close
[0m01:36:08.470085 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6bde175c-5c1e-4854-9d1c-36356e6f877f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e5f3550>]}
[0m01:36:08.471768 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:36:08.472189 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:36:08.473790 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:36:08.474747 [info ] [MainThread]: 
[0m01:36:08.483811 [debug] [Thread-1  ]: Began running node model.jaffle_shop.orders
[0m01:36:08.484437 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.orders ................................... [RUN]
[0m01:36:08.485797 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.orders"
[0m01:36:08.486091 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.orders
[0m01:36:08.486374 [debug] [Thread-1  ]: Compiling model.jaffle_shop.orders
[0m01:36:08.517237 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.orders"
[0m01:36:08.518342 [debug] [Thread-1  ]: finished collecting timing info
[0m01:36:08.518540 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.orders
[0m01:36:08.543915 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.orders"
[0m01:36:08.544941 [debug] [Thread-1  ]: On model.jaffle_shop.orders: 
  
    
import snowflake.snowpark.functions as F

def model(dbt, session):

    dbt.config(
        submission_method='all_purpose_cluster',
        create_notebook=False,
        cluster_id='0124-220040-qfvx2ysq'
    )

    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    payment_methods = ['credit_card', 'coupon', 'bank_transfer', 'gift_card']

    agg_list = [F.sum(F.when(stg_payments_df.payment_method == payment_method, stg_payments_df.amount).otherwise(0)).alias(payment_method + '_amount') for payment_method in payment_methods]

    agg_list.append(F.sum(F.col('amount')).alias('total_amount'))

    order_payments_df = (
        stg_payments_df
        .group_by('order_id')
        .agg(*agg_list)
    )

    final_df = (
        stg_orders_df
        .join(order_payments, stg_orders_df.order_id == order_payments.order_id, 'left')
        .select(stg_orders_df.order_id.alias('order_id'),
                stg_orders_df.customer_id.alias('customer_id'),
                stg_orders_df.order_date.alias('order_date'),
                stg_orders_df.status.alias('status'),
                *[F.col(payment_method + '_amount') for payment_method in payment_methods],
                order_payments_df.total_amount.alias('amount')
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_orders": "bruno.stg_orders", "stg_payments": "bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'None'
    schema = 'bruno'
    identifier = 'orders'
    def __repr__(self):
        return 'bruno.orders'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist
import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write.mode("overwrite").format("delta").option("overwriteSchema", "true").saveAsTable("bruno.orders")
  
[0m01:36:23.526963 [debug] [Thread-1  ]: finished collecting timing info
[0m01:36:23.528903 [debug] [Thread-1  ]: Runtime Error in model orders (models/orders.py)
  Python model failed with traceback as:
  [0;31m---------------------------------------------------------------------------[0m
  [0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
  [0;32m<command--1>[0m in [0;36m<cell line: 1>[0;34m()[0m
  [0;32m----> 1[0;31m [0;32mimport[0m [0msnowflake[0m[0;34m.[0m[0msnowpark[0m[0;34m.[0m[0mfunctions[0m [0;32mas[0m [0mF[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m      2[0m [0;34m[0m[0m
  [1;32m      3[0m [0;32mdef[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0msession[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
  [1;32m      4[0m [0;34m[0m[0m
  [1;32m      5[0m     dbt.config(
  
  [0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py[0m in [0;36mimport_patch[0;34m(name, globals, locals, fromlist, level)[0m
  [1;32m    169[0m             [0;31m# Import the desired module. If you’re seeing this while debugging a failed import,[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  [1;32m    170[0m             [0;31m# look at preceding stack frames for relevant error information.[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  [0;32m--> 171[0;31m             [0moriginal_result[0m [0;34m=[0m [0mpython_builtin_import[0m[0;34m([0m[0mname[0m[0;34m,[0m [0mglobals[0m[0;34m,[0m [0mlocals[0m[0;34m,[0m [0mfromlist[0m[0;34m,[0m [0mlevel[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m    172[0m [0;34m[0m[0m
  [1;32m    173[0m             [0mis_root_import[0m [0;34m=[0m [0mthread_local[0m[0;34m.[0m[0m_nest_level[0m [0;34m==[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
  
  [0;31mModuleNotFoundError[0m: No module named 'snowflake'
[0m01:36:23.529525 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6bde175c-5c1e-4854-9d1c-36356e6f877f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12cc5cf40>]}
[0m01:36:23.530226 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.orders .......................... [[31mERROR[0m in 15.04s]
[0m01:36:23.531243 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.orders
[0m01:36:23.533632 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m01:36:23.534047 [debug] [MainThread]: On master: ROLLBACK
[0m01:36:23.534310 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:36:24.623323 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:36:24.623753 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:36:24.623952 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:36:24.624172 [debug] [MainThread]: On master: ROLLBACK
[0m01:36:24.624359 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:36:24.624528 [debug] [MainThread]: On master: Close
[0m01:36:25.879207 [info ] [MainThread]: 
[0m01:36:25.881032 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 26.08 seconds (26.08s).
[0m01:36:25.881562 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:36:25.881823 [debug] [MainThread]: Connection 'model.jaffle_shop.orders' was properly closed.
[0m01:36:25.905676 [info ] [MainThread]: 
[0m01:36:25.906178 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m01:36:25.906613 [info ] [MainThread]: 
[0m01:36:25.906932 [error] [MainThread]: [33mRuntime Error in model orders (models/orders.py)[0m
[0m01:36:25.907232 [error] [MainThread]:   Python model failed with traceback as:
[0m01:36:25.907512 [error] [MainThread]:   [0;31m---------------------------------------------------------------------------[0m
[0m01:36:25.907787 [error] [MainThread]:   [0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0m01:36:25.908061 [error] [MainThread]:   [0;32m<command--1>[0m in [0;36m<cell line: 1>[0;34m()[0m
[0m01:36:25.908334 [error] [MainThread]:   [0;32m----> 1[0;31m [0;32mimport[0m [0msnowflake[0m[0;34m.[0m[0msnowpark[0m[0;34m.[0m[0mfunctions[0m [0;32mas[0m [0mF[0m[0;34m[0m[0;34m[0m[0m
[0m01:36:25.908613 [error] [MainThread]:   [0m[1;32m      2[0m [0;34m[0m[0m
[0m01:36:25.908886 [error] [MainThread]:   [1;32m      3[0m [0;32mdef[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0msession[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m01:36:25.909161 [error] [MainThread]:   [1;32m      4[0m [0;34m[0m[0m
[0m01:36:25.909434 [error] [MainThread]:   [1;32m      5[0m     dbt.config(
[0m01:36:25.909709 [error] [MainThread]:   
[0m01:36:25.909981 [error] [MainThread]:   [0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py[0m in [0;36mimport_patch[0;34m(name, globals, locals, fromlist, level)[0m
[0m01:36:25.910254 [error] [MainThread]:   [1;32m    169[0m             [0;31m# Import the desired module. If you’re seeing this while debugging a failed import,[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m01:36:25.910540 [error] [MainThread]:   [1;32m    170[0m             [0;31m# look at preceding stack frames for relevant error information.[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m01:36:25.910816 [error] [MainThread]:   [0;32m--> 171[0;31m             [0moriginal_result[0m [0;34m=[0m [0mpython_builtin_import[0m[0;34m([0m[0mname[0m[0;34m,[0m [0mglobals[0m[0;34m,[0m [0mlocals[0m[0;34m,[0m [0mfromlist[0m[0;34m,[0m [0mlevel[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m01:36:25.911091 [error] [MainThread]:   [0m[1;32m    172[0m [0;34m[0m[0m
[0m01:36:25.911363 [error] [MainThread]:   [1;32m    173[0m             [0mis_root_import[0m [0;34m=[0m [0mthread_local[0m[0;34m.[0m[0m_nest_level[0m [0;34m==[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0m01:36:25.911637 [error] [MainThread]:   
[0m01:36:25.911920 [error] [MainThread]:   [0;31mModuleNotFoundError[0m: No module named 'snowflake'
[0m01:36:25.912244 [info ] [MainThread]: 
[0m01:36:25.912563 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:36:25.913010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12cc5cfa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12cc63d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12cce34c0>]}
[0m01:36:25.913331 [debug] [MainThread]: Flushing usage events


============================== 2023-01-25 01:36:59.359798 | 9eeb55d8-60d3-464d-93af-ceeeccd34dab ==============================
[0m01:36:59.359859 [info ] [MainThread]: Running with dbt=1.3.2
[0m01:36:59.360720 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['orders.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m01:36:59.360895 [debug] [MainThread]: Tracking: tracking
[0m01:36:59.387617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ac6b880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ac6bb80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ac7c610>]}
[0m01:36:59.446122 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m01:36:59.446506 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/orders.py
[0m01:36:59.566952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9eeb55d8-60d3-464d-93af-ceeeccd34dab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ac47dc0>]}
[0m01:36:59.574578 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9eeb55d8-60d3-464d-93af-ceeeccd34dab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ade2040>]}
[0m01:36:59.574797 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m01:36:59.574992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9eeb55d8-60d3-464d-93af-ceeeccd34dab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a7463a0>]}
[0m01:36:59.576091 [info ] [MainThread]: 
[0m01:36:59.576575 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m01:36:59.577378 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m01:36:59.586337 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m01:36:59.586533 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:36:59.586629 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:37:02.249587 [debug] [ThreadPool]: SQL status: OK in 2.66 seconds
[0m01:37:02.274625 [debug] [ThreadPool]: On list_schemas: Close
[0m01:37:03.443791 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_bruno"
[0m01:37:03.457647 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:37:03.457950 [debug] [ThreadPool]: Using databricks connection "list_None_bruno"
[0m01:37:03.458190 [debug] [ThreadPool]: On list_None_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_bruno"} */
show table extended in bruno like '*'
  
[0m01:37:03.458403 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:37:06.601969 [debug] [ThreadPool]: SQL status: OK in 3.14 seconds
[0m01:37:06.612443 [debug] [ThreadPool]: On list_None_bruno: ROLLBACK
[0m01:37:06.612929 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m01:37:06.613206 [debug] [ThreadPool]: On list_None_bruno: Close
[0m01:37:07.646031 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9eeb55d8-60d3-464d-93af-ceeeccd34dab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12adb8dc0>]}
[0m01:37:07.647517 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:37:07.647864 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:37:07.649106 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:37:07.649728 [info ] [MainThread]: 
[0m01:37:07.657904 [debug] [Thread-1  ]: Began running node model.jaffle_shop.orders
[0m01:37:07.658605 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.orders ................................... [RUN]
[0m01:37:07.660175 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.orders"
[0m01:37:07.660500 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.orders
[0m01:37:07.660801 [debug] [Thread-1  ]: Compiling model.jaffle_shop.orders
[0m01:37:07.692801 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.orders"
[0m01:37:07.693650 [debug] [Thread-1  ]: finished collecting timing info
[0m01:37:07.693849 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.orders
[0m01:37:07.719260 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.orders"
[0m01:37:07.719994 [debug] [Thread-1  ]: On model.jaffle_shop.orders: 
  
    
import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method='all_purpose_cluster',
        create_notebook=False,
        cluster_id='0124-220040-qfvx2ysq'
    )

    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    payment_methods = ['credit_card', 'coupon', 'bank_transfer', 'gift_card']

    agg_list = [F.sum(F.when(stg_payments_df.payment_method == payment_method, stg_payments_df.amount).otherwise(0)).alias(payment_method + '_amount') for payment_method in payment_methods]

    agg_list.append(F.sum(F.col('amount')).alias('total_amount'))

    order_payments_df = (
        stg_payments_df
        .groupby('order_id')
        .agg(*agg_list)
    )

    final_df = (
        stg_orders_df
        .join(order_payments, stg_orders_df.order_id == order_payments.order_id, 'left')
        .select(stg_orders_df.order_id.alias('order_id'),
                stg_orders_df.customer_id.alias('customer_id'),
                stg_orders_df.order_date.alias('order_date'),
                stg_orders_df.status.alias('status'),
                *[F.col(payment_method + '_amount') for payment_method in payment_methods],
                order_payments_df.total_amount.alias('amount')
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_orders": "bruno.stg_orders", "stg_payments": "bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'None'
    schema = 'bruno'
    identifier = 'orders'
    def __repr__(self):
        return 'bruno.orders'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist
import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write.mode("overwrite").format("delta").option("overwriteSchema", "true").saveAsTable("bruno.orders")
  
[0m01:37:22.722055 [debug] [Thread-1  ]: finished collecting timing info
[0m01:37:22.723614 [debug] [Thread-1  ]: Runtime Error in model orders (models/orders.py)
  Python model failed with traceback as:
  [0;31m---------------------------------------------------------------------------[0m
  [0;31mNameError[0m                                 Traceback (most recent call last)
  [0;32m<command--1>[0m in [0;36m<cell line: 95>[0;34m()[0m
  [1;32m     93[0m [0;31m# --- Autogenerated dbt materialization code. --- #[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     94[0m [0mdbt[0m [0;34m=[0m [0mdbtObj[0m[0;34m([0m[0mspark[0m[0;34m.[0m[0mtable[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0;32m---> 95[0;31m [0mdf[0m [0;34m=[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0mspark[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m     96[0m [0;34m[0m[0m
  [1;32m     97[0m [0;31m# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  
  [0;32m<command--1>[0m in [0;36mmodel[0;34m(dbt, session)[0m
  [1;32m     26[0m     final_df = (
  [1;32m     27[0m         [0mstg_orders_df[0m[0;34m[0m[0;34m[0m[0m
  [0;32m---> 28[0;31m         [0;34m.[0m[0mjoin[0m[0;34m([0m[0morder_payments[0m[0;34m,[0m [0mstg_orders_df[0m[0;34m.[0m[0morder_id[0m [0;34m==[0m [0morder_payments[0m[0;34m.[0m[0morder_id[0m[0;34m,[0m [0;34m'left'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m     29[0m         .select(stg_orders_df.order_id.alias('order_id'),
  [1;32m     30[0m                 [0mstg_orders_df[0m[0;34m.[0m[0mcustomer_id[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m'customer_id'[0m[0;34m)[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
  
  [0;31mNameError[0m: name 'order_payments' is not defined
[0m01:37:22.724184 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9eeb55d8-60d3-464d-93af-ceeeccd34dab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12aff21f0>]}
[0m01:37:22.724918 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.orders .......................... [[31mERROR[0m in 15.06s]
[0m01:37:22.725947 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.orders
[0m01:37:22.728150 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m01:37:22.728552 [debug] [MainThread]: On master: ROLLBACK
[0m01:37:22.728824 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:37:24.045100 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:37:24.046612 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:37:24.047163 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:37:24.047762 [debug] [MainThread]: On master: ROLLBACK
[0m01:37:24.048321 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:37:24.048872 [debug] [MainThread]: On master: Close
[0m01:37:25.275478 [info ] [MainThread]: 
[0m01:37:25.277414 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 25.70 seconds (25.70s).
[0m01:37:25.277976 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:37:25.278268 [debug] [MainThread]: Connection 'model.jaffle_shop.orders' was properly closed.
[0m01:37:25.307097 [info ] [MainThread]: 
[0m01:37:25.307565 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m01:37:25.307944 [info ] [MainThread]: 
[0m01:37:25.308266 [error] [MainThread]: [33mRuntime Error in model orders (models/orders.py)[0m
[0m01:37:25.308571 [error] [MainThread]:   Python model failed with traceback as:
[0m01:37:25.308862 [error] [MainThread]:   [0;31m---------------------------------------------------------------------------[0m
[0m01:37:25.309149 [error] [MainThread]:   [0;31mNameError[0m                                 Traceback (most recent call last)
[0m01:37:25.309559 [error] [MainThread]:   [0;32m<command--1>[0m in [0;36m<cell line: 95>[0;34m()[0m
[0m01:37:25.309944 [error] [MainThread]:   [1;32m     93[0m [0;31m# --- Autogenerated dbt materialization code. --- #[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m01:37:25.310249 [error] [MainThread]:   [1;32m     94[0m [0mdbt[0m [0;34m=[0m [0mdbtObj[0m[0;34m([0m[0mspark[0m[0;34m.[0m[0mtable[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m01:37:25.310540 [error] [MainThread]:   [0;32m---> 95[0;31m [0mdf[0m [0;34m=[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0mspark[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m01:37:25.310824 [error] [MainThread]:   [0m[1;32m     96[0m [0;34m[0m[0m
[0m01:37:25.311107 [error] [MainThread]:   [1;32m     97[0m [0;31m# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m01:37:25.311390 [error] [MainThread]:   
[0m01:37:25.311675 [error] [MainThread]:   [0;32m<command--1>[0m in [0;36mmodel[0;34m(dbt, session)[0m
[0m01:37:25.311953 [error] [MainThread]:   [1;32m     26[0m     final_df = (
[0m01:37:25.312229 [error] [MainThread]:   [1;32m     27[0m         [0mstg_orders_df[0m[0;34m[0m[0;34m[0m[0m
[0m01:37:25.312507 [error] [MainThread]:   [0;32m---> 28[0;31m         [0;34m.[0m[0mjoin[0m[0;34m([0m[0morder_payments[0m[0;34m,[0m [0mstg_orders_df[0m[0;34m.[0m[0morder_id[0m [0;34m==[0m [0morder_payments[0m[0;34m.[0m[0morder_id[0m[0;34m,[0m [0;34m'left'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m01:37:25.312788 [error] [MainThread]:   [0m[1;32m     29[0m         .select(stg_orders_df.order_id.alias('order_id'),
[0m01:37:25.313052 [error] [MainThread]:   [1;32m     30[0m                 [0mstg_orders_df[0m[0;34m.[0m[0mcustomer_id[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m'customer_id'[0m[0;34m)[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[0m01:37:25.313297 [error] [MainThread]:   
[0m01:37:25.313537 [error] [MainThread]:   [0;31mNameError[0m: name 'order_payments' is not defined
[0m01:37:25.313809 [info ] [MainThread]: 
[0m01:37:25.314080 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:37:25.314487 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12adb8e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12afae160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12afae310>]}
[0m01:37:25.314798 [debug] [MainThread]: Flushing usage events


============================== 2023-01-25 01:37:39.193747 | 62a8a8d6-15af-4b68-a3e5-f9b7ce2cf75c ==============================
[0m01:37:39.193868 [info ] [MainThread]: Running with dbt=1.3.2
[0m01:37:39.195047 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['orders.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m01:37:39.195336 [debug] [MainThread]: Tracking: tracking
[0m01:37:39.238246 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c6ab8e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c6abbe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c6be670>]}
[0m01:37:39.296378 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m01:37:39.296775 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/orders.py
[0m01:37:39.414574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '62a8a8d6-15af-4b68-a3e5-f9b7ce2cf75c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d51c160>]}
[0m01:37:39.422164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '62a8a8d6-15af-4b68-a3e5-f9b7ce2cf75c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c85ff10>]}
[0m01:37:39.422362 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m01:37:39.422547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '62a8a8d6-15af-4b68-a3e5-f9b7ce2cf75c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c6710d0>]}
[0m01:37:39.423677 [info ] [MainThread]: 
[0m01:37:39.424165 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m01:37:39.424987 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m01:37:39.434058 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m01:37:39.434246 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:37:39.434339 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:37:41.979574 [debug] [ThreadPool]: SQL status: OK in 2.55 seconds
[0m01:37:42.002788 [debug] [ThreadPool]: On list_schemas: Close
[0m01:37:42.959937 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_bruno"
[0m01:37:42.972201 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:37:42.972483 [debug] [ThreadPool]: Using databricks connection "list_None_bruno"
[0m01:37:42.972726 [debug] [ThreadPool]: On list_None_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_bruno"} */
show table extended in bruno like '*'
  
[0m01:37:42.972928 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:37:46.084835 [debug] [ThreadPool]: SQL status: OK in 3.11 seconds
[0m01:37:46.092760 [debug] [ThreadPool]: On list_None_bruno: ROLLBACK
[0m01:37:46.093056 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m01:37:46.093273 [debug] [ThreadPool]: On list_None_bruno: Close
[0m01:37:47.295390 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '62a8a8d6-15af-4b68-a3e5-f9b7ce2cf75c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cb6eb80>]}
[0m01:37:47.296985 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:37:47.297264 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:37:47.298407 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:37:47.299293 [info ] [MainThread]: 
[0m01:37:47.308586 [debug] [Thread-1  ]: Began running node model.jaffle_shop.orders
[0m01:37:47.309214 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.orders ................................... [RUN]
[0m01:37:47.310569 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.orders"
[0m01:37:47.310844 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.orders
[0m01:37:47.311092 [debug] [Thread-1  ]: Compiling model.jaffle_shop.orders
[0m01:37:47.342011 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.orders"
[0m01:37:47.342796 [debug] [Thread-1  ]: finished collecting timing info
[0m01:37:47.343016 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.orders
[0m01:37:47.368647 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.orders"
[0m01:37:47.369567 [debug] [Thread-1  ]: On model.jaffle_shop.orders: 
  
    
import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method='all_purpose_cluster',
        create_notebook=False,
        cluster_id='0124-220040-qfvx2ysq'
    )

    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    payment_methods = ['credit_card', 'coupon', 'bank_transfer', 'gift_card']

    agg_list = [F.sum(F.when(stg_payments_df.payment_method == payment_method, stg_payments_df.amount).otherwise(0)).alias(payment_method + '_amount') for payment_method in payment_methods]

    agg_list.append(F.sum(F.col('amount')).alias('total_amount'))

    order_payments_df = (
        stg_payments_df
        .groupby('order_id')
        .agg(*agg_list)
    )

    final_df = (
        stg_orders_df
        .join(order_payments, stg_orders_df.order_id == order_payments_df.order_id, 'left')
        .select(stg_orders_df.order_id.alias('order_id'),
                stg_orders_df.customer_id.alias('customer_id'),
                stg_orders_df.order_date.alias('order_date'),
                stg_orders_df.status.alias('status'),
                *[F.col(payment_method + '_amount') for payment_method in payment_methods],
                order_payments_df.total_amount.alias('amount')
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_orders": "bruno.stg_orders", "stg_payments": "bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'None'
    schema = 'bruno'
    identifier = 'orders'
    def __repr__(self):
        return 'bruno.orders'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist
import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write.mode("overwrite").format("delta").option("overwriteSchema", "true").saveAsTable("bruno.orders")
  
[0m01:38:02.453907 [debug] [Thread-1  ]: finished collecting timing info
[0m01:38:02.457010 [debug] [Thread-1  ]: Runtime Error in model orders (models/orders.py)
  Python model failed with traceback as:
  [0;31m---------------------------------------------------------------------------[0m
  [0;31mNameError[0m                                 Traceback (most recent call last)
  [0;32m<command--1>[0m in [0;36m<cell line: 95>[0;34m()[0m
  [1;32m     93[0m [0;31m# --- Autogenerated dbt materialization code. --- #[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     94[0m [0mdbt[0m [0;34m=[0m [0mdbtObj[0m[0;34m([0m[0mspark[0m[0;34m.[0m[0mtable[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0;32m---> 95[0;31m [0mdf[0m [0;34m=[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0mspark[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m     96[0m [0;34m[0m[0m
  [1;32m     97[0m [0;31m# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  
  [0;32m<command--1>[0m in [0;36mmodel[0;34m(dbt, session)[0m
  [1;32m     26[0m     final_df = (
  [1;32m     27[0m         [0mstg_orders_df[0m[0;34m[0m[0;34m[0m[0m
  [0;32m---> 28[0;31m         [0;34m.[0m[0mjoin[0m[0;34m([0m[0morder_payments[0m[0;34m,[0m [0mstg_orders_df[0m[0;34m.[0m[0morder_id[0m [0;34m==[0m [0morder_payments_df[0m[0;34m.[0m[0morder_id[0m[0;34m,[0m [0;34m'left'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m     29[0m         .select(stg_orders_df.order_id.alias('order_id'),
  [1;32m     30[0m                 [0mstg_orders_df[0m[0;34m.[0m[0mcustomer_id[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m'customer_id'[0m[0;34m)[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
  
  [0;31mNameError[0m: name 'order_payments' is not defined
[0m01:38:02.457649 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '62a8a8d6-15af-4b68-a3e5-f9b7ce2cf75c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ca30250>]}
[0m01:38:02.458391 [error] [Thread-1  ]: 1 of 1 ERROR creating python table model bruno.orders .......................... [[31mERROR[0m in 15.15s]
[0m01:38:02.459464 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.orders
[0m01:38:02.461781 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m01:38:02.462161 [debug] [MainThread]: On master: ROLLBACK
[0m01:38:02.462417 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:38:03.612315 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:38:03.612891 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:38:03.613230 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:38:03.613576 [debug] [MainThread]: On master: ROLLBACK
[0m01:38:03.613889 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:38:03.614196 [debug] [MainThread]: On master: Close
[0m01:38:04.694601 [info ] [MainThread]: 
[0m01:38:04.696093 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 25.27 seconds (25.27s).
[0m01:38:04.696741 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:38:04.697109 [debug] [MainThread]: Connection 'model.jaffle_shop.orders' was properly closed.
[0m01:38:04.728875 [info ] [MainThread]: 
[0m01:38:04.729304 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m01:38:04.729666 [info ] [MainThread]: 
[0m01:38:04.729984 [error] [MainThread]: [33mRuntime Error in model orders (models/orders.py)[0m
[0m01:38:04.730281 [error] [MainThread]:   Python model failed with traceback as:
[0m01:38:04.730564 [error] [MainThread]:   [0;31m---------------------------------------------------------------------------[0m
[0m01:38:04.730845 [error] [MainThread]:   [0;31mNameError[0m                                 Traceback (most recent call last)
[0m01:38:04.731172 [error] [MainThread]:   [0;32m<command--1>[0m in [0;36m<cell line: 95>[0;34m()[0m
[0m01:38:04.731457 [error] [MainThread]:   [1;32m     93[0m [0;31m# --- Autogenerated dbt materialization code. --- #[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m01:38:04.731739 [error] [MainThread]:   [1;32m     94[0m [0mdbt[0m [0;34m=[0m [0mdbtObj[0m[0;34m([0m[0mspark[0m[0;34m.[0m[0mtable[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m01:38:04.732017 [error] [MainThread]:   [0;32m---> 95[0;31m [0mdf[0m [0;34m=[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0mspark[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m01:38:04.732294 [error] [MainThread]:   [0m[1;32m     96[0m [0;34m[0m[0m
[0m01:38:04.732569 [error] [MainThread]:   [1;32m     97[0m [0;31m# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m01:38:04.732843 [error] [MainThread]:   
[0m01:38:04.733118 [error] [MainThread]:   [0;32m<command--1>[0m in [0;36mmodel[0;34m(dbt, session)[0m
[0m01:38:04.733391 [error] [MainThread]:   [1;32m     26[0m     final_df = (
[0m01:38:04.733666 [error] [MainThread]:   [1;32m     27[0m         [0mstg_orders_df[0m[0;34m[0m[0;34m[0m[0m
[0m01:38:04.733940 [error] [MainThread]:   [0;32m---> 28[0;31m         [0;34m.[0m[0mjoin[0m[0;34m([0m[0morder_payments[0m[0;34m,[0m [0mstg_orders_df[0m[0;34m.[0m[0morder_id[0m [0;34m==[0m [0morder_payments_df[0m[0;34m.[0m[0morder_id[0m[0;34m,[0m [0;34m'left'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m01:38:04.734193 [error] [MainThread]:   [0m[1;32m     29[0m         .select(stg_orders_df.order_id.alias('order_id'),
[0m01:38:04.734452 [error] [MainThread]:   [1;32m     30[0m                 [0mstg_orders_df[0m[0;34m.[0m[0mcustomer_id[0m[0;34m.[0m[0malias[0m[0;34m([0m[0;34m'customer_id'[0m[0;34m)[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[0m01:38:04.734691 [error] [MainThread]:   
[0m01:38:04.734927 [error] [MainThread]:   [0;31mNameError[0m: name 'order_payments' is not defined
[0m01:38:04.735183 [info ] [MainThread]: 
[0m01:38:04.735442 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:38:04.735824 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c8232e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c9ed700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c9ed490>]}
[0m01:38:04.736116 [debug] [MainThread]: Flushing usage events


============================== 2023-01-25 01:38:23.531703 | 158759b5-701e-4429-8edc-ec19b9d2a531 ==============================
[0m01:38:23.531749 [info ] [MainThread]: Running with dbt=1.3.2
[0m01:38:23.532775 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['orders.py'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m01:38:23.532964 [debug] [MainThread]: Tracking: tracking
[0m01:38:23.557797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c49a8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c49abb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c4ad640>]}
[0m01:38:23.618592 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m01:38:23.619040 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/orders.py
[0m01:38:23.740883 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '158759b5-701e-4429-8edc-ec19b9d2a531', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c45a4c0>]}
[0m01:38:23.748482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '158759b5-701e-4429-8edc-ec19b9d2a531', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c612400>]}
[0m01:38:23.748687 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m01:38:23.748877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '158759b5-701e-4429-8edc-ec19b9d2a531', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c64dee0>]}
[0m01:38:23.749982 [info ] [MainThread]: 
[0m01:38:23.750471 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m01:38:23.751262 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m01:38:23.760238 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m01:38:23.760442 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:38:23.760540 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:38:26.329067 [debug] [ThreadPool]: SQL status: OK in 2.57 seconds
[0m01:38:26.353443 [debug] [ThreadPool]: On list_schemas: Close
[0m01:38:27.508990 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_bruno"
[0m01:38:27.520444 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:38:27.520722 [debug] [ThreadPool]: Using databricks connection "list_None_bruno"
[0m01:38:27.520956 [debug] [ThreadPool]: On list_None_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "dbt_databricks_version": "1.3.2", "databricks_sql_connector_version": "2.3.0", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_bruno"} */
show table extended in bruno like '*'
  
[0m01:38:27.521164 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:38:30.858996 [debug] [ThreadPool]: SQL status: OK in 3.34 seconds
[0m01:38:30.867245 [debug] [ThreadPool]: On list_None_bruno: ROLLBACK
[0m01:38:30.867638 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m01:38:30.867900 [debug] [ThreadPool]: On list_None_bruno: Close
[0m01:38:31.884484 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '158759b5-701e-4429-8edc-ec19b9d2a531', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c669fa0>]}
[0m01:38:31.885094 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:38:31.885330 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:38:31.886146 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:38:31.886579 [info ] [MainThread]: 
[0m01:38:31.893136 [debug] [Thread-1  ]: Began running node model.jaffle_shop.orders
[0m01:38:31.893583 [info ] [Thread-1  ]: 1 of 1 START python table model bruno.orders ................................... [RUN]
[0m01:38:31.894515 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.orders"
[0m01:38:31.894729 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.orders
[0m01:38:31.894966 [debug] [Thread-1  ]: Compiling model.jaffle_shop.orders
[0m01:38:31.921920 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.orders"
[0m01:38:31.922766 [debug] [Thread-1  ]: finished collecting timing info
[0m01:38:31.922950 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.orders
[0m01:38:31.946019 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.orders"
[0m01:38:31.946782 [debug] [Thread-1  ]: On model.jaffle_shop.orders: 
  
    
import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method='all_purpose_cluster',
        create_notebook=False,
        cluster_id='0124-220040-qfvx2ysq'
    )

    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    payment_methods = ['credit_card', 'coupon', 'bank_transfer', 'gift_card']

    agg_list = [F.sum(F.when(stg_payments_df.payment_method == payment_method, stg_payments_df.amount).otherwise(0)).alias(payment_method + '_amount') for payment_method in payment_methods]

    agg_list.append(F.sum(F.col('amount')).alias('total_amount'))

    order_payments_df = (
        stg_payments_df
        .groupby('order_id')
        .agg(*agg_list)
    )

    final_df = (
        stg_orders_df
        .join(order_payments_df, stg_orders_df.order_id == order_payments_df.order_id, 'left')
        .select(stg_orders_df.order_id.alias('order_id'),
                stg_orders_df.customer_id.alias('customer_id'),
                stg_orders_df.order_date.alias('order_date'),
                stg_orders_df.status.alias('status'),
                *[F.col(payment_method + '_amount') for payment_method in payment_methods],
                order_payments_df.total_amount.alias('amount')
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_orders": "bruno.stg_orders", "stg_payments": "bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'None'
    schema = 'bruno'
    identifier = 'orders'
    def __repr__(self):
        return 'bruno.orders'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist
import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write.mode("overwrite").format("delta").option("overwriteSchema", "true").saveAsTable("bruno.orders")
  
[0m01:39:10.345953 [debug] [Thread-1  ]: Execution status: OK in 38.4 seconds
[0m01:39:10.381594 [debug] [Thread-1  ]: finished collecting timing info
[0m01:39:10.382434 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '158759b5-701e-4429-8edc-ec19b9d2a531', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c7ddc70>]}
[0m01:39:10.382894 [info ] [Thread-1  ]: 1 of 1 OK created python table model bruno.orders .............................. [[32mOK[0m in 38.49s]
[0m01:39:10.383528 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.orders
[0m01:39:10.384965 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m01:39:10.385183 [debug] [MainThread]: On master: ROLLBACK
[0m01:39:10.385331 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:39:11.531748 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:39:11.532309 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:39:11.532573 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:39:11.532856 [debug] [MainThread]: On master: ROLLBACK
[0m01:39:11.533116 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m01:39:11.533351 [debug] [MainThread]: On master: Close
[0m01:39:12.530049 [info ] [MainThread]: 
[0m01:39:12.531348 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 48.78 seconds (48.78s).
[0m01:39:12.531970 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:39:12.532271 [debug] [MainThread]: Connection 'model.jaffle_shop.orders' was properly closed.
[0m01:39:12.560857 [info ] [MainThread]: 
[0m01:39:12.561286 [info ] [MainThread]: [32mCompleted successfully[0m
[0m01:39:12.561675 [info ] [MainThread]: 
[0m01:39:12.561984 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m01:39:12.562398 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c669f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c5b2820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c849ac0>]}
[0m01:39:12.562718 [debug] [MainThread]: Flushing usage events


============================== 2023-01-25 03:25:09.981652 | 25089abc-e59d-418d-b214-47ad2d4b4486 ==============================
[0m03:25:09.981664 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:25:09.986180 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m03:25:09.986282 [debug] [MainThread]: Tracking: tracking
[0m03:25:10.015523 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cb6b970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c903340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c77c640>]}
[0m03:25:11.956431 [debug] [MainThread]: Executing "git --help"
[0m03:25:11.996603 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m03:25:11.997285 [debug] [MainThread]: STDERR: "b''"
[0m03:25:12.004730 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m03:25:12.005652 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:25:12.006143 [info ] [MainThread]: BigQuery adapter: Please log into GCP to continue
[0m03:25:12.006430 [debug] [MainThread]: Executing "gcloud --version"
[0m03:25:12.014354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10de9b5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10de00130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10de00430>]}
[0m03:25:12.015380 [debug] [MainThread]: Flushing usage events
[0m03:25:12.819890 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-01-25 03:34:33.144953 | f42ec6f1-4eb1-481c-8c53-e479147e1359 ==============================
[0m03:34:33.144967 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:34:33.145351 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m03:34:33.145455 [debug] [MainThread]: Tracking: tracking
[0m03:34:33.177288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c7797f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c786df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c7a7340>]}
[0m03:34:33.524637 [debug] [MainThread]: Executing "git --help"
[0m03:34:33.555553 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m03:34:33.556456 [debug] [MainThread]: STDERR: "b''"
[0m03:34:33.563792 [debug] [MainThread]: Acquiring new bigquery connection "debug"
[0m03:34:33.564740 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:34:33.566819 [debug] [MainThread]: On debug: select 1 as id
[0m03:34:35.854071 [debug] [MainThread]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:23408f12-1517-4a75-8ad5-f5106d00b70b:US&page=queryresults
[0m03:34:35.855721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dba9f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10db98580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dbc7af0>]}
[0m03:34:35.856558 [debug] [MainThread]: Flushing usage events
[0m03:34:36.826322 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-01-25 03:35:01.651190 | e801db3f-8e0b-4c0f-9b79-a6f831846b60 ==============================
[0m03:35:01.651243 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:35:01.651944 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'indirect_selection': 'eager', 'resource_types': [], 'which': 'build', 'rpc_method': 'build'}
[0m03:35:01.652094 [debug] [MainThread]: Tracking: tracking
[0m03:35:01.677184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d8be880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d8afca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d8afc10>]}
[0m03:35:01.718030 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m03:35:01.718362 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
[0m03:35:01.718535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'e801db3f-8e0b-4c0f-9b79-a6f831846b60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d8d9760>]}
[0m03:35:01.742848 [debug] [MainThread]: Parsing macros/etc.sql
[0m03:35:01.745110 [debug] [MainThread]: Parsing macros/catalog.sql
[0m03:35:01.751412 [debug] [MainThread]: Parsing macros/adapters.sql
[0m03:35:01.769224 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m03:35:01.771187 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m03:35:01.773322 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m03:35:01.779406 [debug] [MainThread]: Parsing macros/materializations/copy.sql
[0m03:35:01.781417 [debug] [MainThread]: Parsing macros/materializations/incremental.sql
[0m03:35:01.795310 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m03:35:01.796541 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m03:35:01.796815 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m03:35:01.797267 [debug] [MainThread]: Parsing macros/utils/timestamps.sql
[0m03:35:01.798028 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m03:35:01.798279 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m03:35:01.798680 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m03:35:01.799153 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m03:35:01.799893 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m03:35:01.800783 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m03:35:01.801143 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m03:35:01.801494 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m03:35:01.801872 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m03:35:01.802227 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m03:35:01.802527 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m03:35:01.803627 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m03:35:01.803998 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m03:35:01.804606 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m03:35:01.805023 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m03:35:01.806958 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m03:35:01.809831 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m03:35:01.811501 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m03:35:01.812764 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m03:35:01.825553 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m03:35:01.837124 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m03:35:01.847494 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m03:35:01.850944 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m03:35:01.852235 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m03:35:01.853513 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m03:35:01.859685 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m03:35:01.873070 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m03:35:01.874182 [debug] [MainThread]: Parsing macros/materializations/models/incremental/strategies.sql
[0m03:35:01.879233 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m03:35:01.887516 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m03:35:01.902529 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m03:35:01.906899 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m03:35:01.909447 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m03:35:01.913779 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m03:35:01.914695 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m03:35:01.917221 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m03:35:01.918837 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m03:35:01.924325 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m03:35:01.939984 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m03:35:01.941045 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m03:35:01.942906 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m03:35:01.944036 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m03:35:01.944681 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m03:35:01.945241 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m03:35:01.945721 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m03:35:01.946716 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m03:35:01.950716 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m03:35:01.957441 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m03:35:01.958019 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m03:35:01.958933 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m03:35:01.959610 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m03:35:01.960279 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m03:35:01.961180 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m03:35:01.961748 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m03:35:01.962473 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m03:35:01.963482 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m03:35:01.965253 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m03:35:01.966136 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m03:35:01.966925 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m03:35:01.967687 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m03:35:01.968416 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m03:35:01.969128 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m03:35:01.969924 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m03:35:01.970562 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m03:35:01.976982 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m03:35:01.977759 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m03:35:01.978422 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m03:35:01.979704 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m03:35:01.981323 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m03:35:01.982059 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m03:35:01.983159 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m03:35:01.983894 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m03:35:01.985426 [debug] [MainThread]: Parsing macros/adapters/timestamps.sql
[0m03:35:01.987799 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m03:35:01.989795 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m03:35:02.001780 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m03:35:02.003213 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m03:35:02.013341 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m03:35:02.016659 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m03:35:02.022263 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m03:35:02.029765 [debug] [MainThread]: Parsing macros/python_model/python.sql
[0m03:35:02.034534 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m03:35:02.275767 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_customers.sql
[0m03:35:02.285194 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_payments.sql
[0m03:35:02.287257 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_orders.sql
[0m03:35:02.423062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e801db3f-8e0b-4c0f-9b79-a6f831846b60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10db090d0>]}
[0m03:35:02.430880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e801db3f-8e0b-4c0f-9b79-a6f831846b60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d953280>]}
[0m03:35:02.431097 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 319 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m03:35:02.431272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e801db3f-8e0b-4c0f-9b79-a6f831846b60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10da97e20>]}
[0m03:35:02.432843 [info ] [MainThread]: 
[0m03:35:02.433376 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m03:35:02.434299 [debug] [ThreadPool]: Acquiring new bigquery connection "list_jaffle-shop-375704"
[0m03:35:02.434434 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:35:03.201032 [debug] [ThreadPool]: Acquiring new bigquery connection "create_jaffle-shop-375704_bruno"
[0m03:35:03.202088 [debug] [ThreadPool]: Acquiring new bigquery connection "create_jaffle-shop-375704_bruno"
[0m03:35:03.202508 [debug] [ThreadPool]: Creating schema "_ReferenceKey(database='jaffle-shop-375704', schema='bruno', identifier=None)"
[0m03:35:03.222409 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:35:03.224239 [debug] [ThreadPool]: On create_jaffle-shop-375704_bruno: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "create_jaffle-shop-375704_bruno"} */
create schema if not exists `jaffle-shop-375704`.`bruno`
  
[0m03:35:05.020038 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:fddba19c-8b55-4fe0-b08c-b60f609aacb6:US&page=queryresults
[0m03:35:05.025854 [debug] [ThreadPool]: Acquiring new bigquery connection "list_jaffle-shop-375704_bruno"
[0m03:35:05.026472 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:35:05.763242 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e801db3f-8e0b-4c0f-9b79-a6f831846b60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10da97e20>]}
[0m03:35:05.764620 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:35:05.765190 [info ] [MainThread]: 
[0m03:35:05.773150 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_customers
[0m03:35:05.773691 [info ] [Thread-1  ]: 1 of 28 START seed file bruno.raw_customers .................................... [RUN]
[0m03:35:05.774738 [debug] [Thread-1  ]: Acquiring new bigquery connection "seed.jaffle_shop.raw_customers"
[0m03:35:05.774983 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_customers
[0m03:35:05.775232 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:05.775451 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_customers
[0m03:35:05.822658 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:35:11.981569 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_customers"
[0m03:35:12.014343 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:12.014968 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e801db3f-8e0b-4c0f-9b79-a6f831846b60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dbebac0>]}
[0m03:35:12.015254 [info ] [Thread-1  ]: 1 of 28 OK loaded seed file bruno.raw_customers ................................ [[32mINSERT 100[0m in 6.24s]
[0m03:35:12.015764 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_customers
[0m03:35:12.015956 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_orders
[0m03:35:12.016378 [info ] [Thread-1  ]: 2 of 28 START seed file bruno.raw_orders ....................................... [RUN]
[0m03:35:12.017128 [debug] [Thread-1  ]: Acquiring new bigquery connection "seed.jaffle_shop.raw_orders"
[0m03:35:12.017281 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_orders
[0m03:35:12.017427 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:12.017564 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_orders
[0m03:35:12.039739 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:35:18.362042 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_orders"
[0m03:35:18.370688 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:18.371908 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e801db3f-8e0b-4c0f-9b79-a6f831846b60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dbbfe50>]}
[0m03:35:18.372427 [info ] [Thread-1  ]: 2 of 28 OK loaded seed file bruno.raw_orders ................................... [[32mINSERT 99[0m in 6.35s]
[0m03:35:18.373161 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_orders
[0m03:35:18.373508 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_payments
[0m03:35:18.374087 [info ] [Thread-1  ]: 3 of 28 START seed file bruno.raw_payments ..................................... [RUN]
[0m03:35:18.375486 [debug] [Thread-1  ]: Acquiring new bigquery connection "seed.jaffle_shop.raw_payments"
[0m03:35:18.375828 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_payments
[0m03:35:18.376110 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:18.376345 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_payments
[0m03:35:18.396549 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:35:24.944608 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_payments"
[0m03:35:24.952415 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:24.953604 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e801db3f-8e0b-4c0f-9b79-a6f831846b60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dbd8040>]}
[0m03:35:24.954146 [info ] [Thread-1  ]: 3 of 28 OK loaded seed file bruno.raw_payments ................................. [[32mINSERT 113[0m in 6.58s]
[0m03:35:24.954933 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_payments
[0m03:35:24.955311 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m03:35:24.955975 [info ] [Thread-1  ]: 4 of 28 START sql view model bruno.stg_customers ............................... [RUN]
[0m03:35:24.957435 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.jaffle_shop.stg_customers"
[0m03:35:24.957750 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m03:35:24.958036 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m03:35:24.963719 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m03:35:24.965144 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:24.965429 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m03:35:24.998632 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_customers"
[0m03:35:24.999217 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:35:25.000402 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */


  create or replace view `jaffle-shop-375704`.`bruno`.`stg_customers`
  OPTIONS()
  as with source as (
    select * from `jaffle-shop-375704`.`bruno`.`raw_customers`

),

renamed as (

    select
        id as customer_id,
        first_name,
        last_name

    from source

)

select * from renamed;


[0m03:35:26.381995 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:146b1675-722f-40c9-8e9e-b5e226d51165:US&page=queryresults
[0m03:35:26.384680 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:26.385628 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e801db3f-8e0b-4c0f-9b79-a6f831846b60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dbd8100>]}
[0m03:35:26.386189 [info ] [Thread-1  ]: 4 of 28 OK created sql view model bruno.stg_customers .......................... [[32mCREATE VIEW (0 processed)[0m in 1.43s]
[0m03:35:26.386748 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m03:35:26.387002 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m03:35:26.387279 [info ] [Thread-1  ]: 5 of 28 START sql view model bruno.stg_orders .................................. [RUN]
[0m03:35:26.388340 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.jaffle_shop.stg_orders"
[0m03:35:26.388760 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m03:35:26.389048 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m03:35:26.397015 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m03:35:26.397920 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:26.398114 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m03:35:26.404175 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_orders"
[0m03:35:26.404954 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:35:26.406429 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */


  create or replace view `jaffle-shop-375704`.`bruno`.`stg_orders`
  OPTIONS()
  as with source as (
    select * from `jaffle-shop-375704`.`bruno`.`raw_orders`

),

renamed as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from source

)

select * from renamed;


[0m03:35:27.987583 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:824cfbb2-042d-436f-ab45-7053b7113176:US&page=queryresults
[0m03:35:27.992011 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:27.993373 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e801db3f-8e0b-4c0f-9b79-a6f831846b60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dd1ff40>]}
[0m03:35:27.994082 [info ] [Thread-1  ]: 5 of 28 OK created sql view model bruno.stg_orders ............................. [[32mCREATE VIEW (0 processed)[0m in 1.61s]
[0m03:35:27.994908 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m03:35:27.995269 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m03:35:27.995877 [info ] [Thread-1  ]: 6 of 28 START sql view model bruno.stg_payments ................................ [RUN]
[0m03:35:27.997338 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.jaffle_shop.stg_payments"
[0m03:35:27.997663 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m03:35:27.997939 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m03:35:28.019623 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m03:35:28.020819 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:28.021071 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m03:35:28.025165 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_payments"
[0m03:35:28.025780 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:35:28.027345 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */


  create or replace view `jaffle-shop-375704`.`bruno`.`stg_payments`
  OPTIONS()
  as with source as (
    select * from `jaffle-shop-375704`.`bruno`.`raw_payments`

),

renamed as (

    select
        id as payment_id,
        order_id,
        payment_method,

        -- `amount` is currently stored in cents, so we convert it to dollars
        amount / 100 as amount

    from source

)

select * from renamed;


[0m03:35:29.630508 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:8e64f180-e3a5-47a3-a251-140f1c153f66:US&page=queryresults
[0m03:35:29.636404 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:29.637781 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e801db3f-8e0b-4c0f-9b79-a6f831846b60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dbbfee0>]}
[0m03:35:29.638519 [info ] [Thread-1  ]: 6 of 28 OK created sql view model bruno.stg_payments ........................... [[32mCREATE VIEW (0 processed)[0m in 1.64s]
[0m03:35:29.639354 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m03:35:29.639736 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:35:29.640216 [info ] [Thread-1  ]: 7 of 28 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m03:35:29.641620 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m03:35:29.641946 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:35:29.642263 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:35:29.660527 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m03:35:29.662151 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:29.662397 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:35:29.682590 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m03:35:29.683162 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:35:29.684508 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `jaffle-shop-375704`.`bruno`.`stg_customers`
where customer_id is null



      
    ) dbt_internal_test
[0m03:35:31.979496 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:3bbc0e1b-3e5e-4094-a0d3-91bdbdf03908:US&page=queryresults
[0m03:35:31.983604 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:31.985460 [info ] [Thread-1  ]: 7 of 28 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 2.34s]
[0m03:35:31.986373 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:35:31.986755 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m03:35:31.987206 [info ] [Thread-1  ]: 8 of 28 START test unique_stg_customers_customer_id ............................ [RUN]
[0m03:35:31.988484 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m03:35:31.988795 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m03:35:31.989073 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m03:35:32.009613 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m03:35:32.011726 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:32.011973 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m03:35:32.015726 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m03:35:32.016304 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:35:32.017959 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `jaffle-shop-375704`.`bruno`.`stg_customers`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m03:35:34.436927 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:d93c42cf-ee6a-422d-992b-e6a5006d9bf6:US&page=queryresults
[0m03:35:34.439760 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:34.442794 [info ] [Thread-1  ]: 8 of 28 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 2.45s]
[0m03:35:34.444334 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m03:35:34.444835 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m03:35:34.445294 [info ] [Thread-1  ]: 9 of 28 START test accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned  [RUN]
[0m03:35:34.446748 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m03:35:34.447077 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m03:35:34.447352 [debug] [Thread-1  ]: Compiling test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m03:35:34.461700 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m03:35:34.463074 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:34.463370 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m03:35:34.475527 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m03:35:34.476182 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:35:34.482582 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from `jaffle-shop-375704`.`bruno`.`stg_orders`
    group by status

)

select *
from all_values
where value_field not in (
    'placed','shipped','completed','return_pending','returned'
)



      
    ) dbt_internal_test
[0m03:35:36.906383 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:806c31db-f4b8-4fc3-b796-c2ce671ff35b:US&page=queryresults
[0m03:35:36.907393 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:36.908477 [info ] [Thread-1  ]: 9 of 28 PASS accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned  [[32mPASS[0m in 2.46s]
[0m03:35:36.909302 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m03:35:36.909680 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m03:35:36.910126 [info ] [Thread-1  ]: 10 of 28 START test not_null_stg_orders_order_id ............................... [RUN]
[0m03:35:36.911403 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m03:35:36.911716 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m03:35:36.911993 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m03:35:36.924964 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m03:35:36.926747 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:36.927019 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m03:35:36.933247 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m03:35:36.933911 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:35:36.935855 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from `jaffle-shop-375704`.`bruno`.`stg_orders`
where order_id is null



      
    ) dbt_internal_test
[0m03:35:39.251445 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:f0234983-82a8-4bb3-9b0b-78dd23f63b64:US&page=queryresults
[0m03:35:39.254030 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:39.255861 [info ] [Thread-1  ]: 10 of 28 PASS not_null_stg_orders_order_id ..................................... [[32mPASS[0m in 2.35s]
[0m03:35:39.256950 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m03:35:39.257681 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m03:35:39.258297 [info ] [Thread-1  ]: 11 of 28 START test unique_stg_orders_order_id ................................. [RUN]
[0m03:35:39.259858 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m03:35:39.260177 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m03:35:39.260470 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m03:35:39.273892 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m03:35:39.275074 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:39.275339 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m03:35:39.281881 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m03:35:39.282465 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:35:39.284348 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `jaffle-shop-375704`.`bruno`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m03:35:41.708120 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:54c055f4-cd60-4a7e-8727-326647103322:US&page=queryresults
[0m03:35:41.710431 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:41.712174 [info ] [Thread-1  ]: 11 of 28 PASS unique_stg_orders_order_id ....................................... [[32mPASS[0m in 2.45s]
[0m03:35:41.713699 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m03:35:41.714372 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m03:35:41.714902 [info ] [Thread-1  ]: 12 of 28 START test accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card  [RUN]
[0m03:35:41.716341 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m03:35:41.716662 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m03:35:41.716951 [debug] [Thread-1  ]: Compiling test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m03:35:41.737928 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m03:35:41.739527 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:41.739799 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m03:35:41.743311 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m03:35:41.743901 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:35:41.745615 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        payment_method as value_field,
        count(*) as n_records

    from `jaffle-shop-375704`.`bruno`.`stg_payments`
    group by payment_method

)

select *
from all_values
where value_field not in (
    'credit_card','coupon','bank_transfer','gift_card'
)



      
    ) dbt_internal_test
[0m03:35:43.948087 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:eaab3556-656f-495d-b2be-25093ab1985d:US&page=queryresults
[0m03:35:43.951043 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:43.952860 [info ] [Thread-1  ]: 12 of 28 PASS accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card  [[32mPASS[0m in 2.24s]
[0m03:35:43.953812 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m03:35:43.954150 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m03:35:43.954527 [info ] [Thread-1  ]: 13 of 28 START test not_null_stg_payments_payment_id ........................... [RUN]
[0m03:35:43.955561 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m03:35:43.955803 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m03:35:43.956021 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m03:35:43.972100 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m03:35:43.972854 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:43.973087 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m03:35:43.976107 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m03:35:43.976576 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:35:43.978071 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select payment_id
from `jaffle-shop-375704`.`bruno`.`stg_payments`
where payment_id is null



      
    ) dbt_internal_test
[0m03:35:46.517157 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:5d595e66-f0e5-484e-a6d5-24939cabb6bb:US&page=queryresults
[0m03:35:46.518019 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:46.518899 [info ] [Thread-1  ]: 13 of 28 PASS not_null_stg_payments_payment_id ................................. [[32mPASS[0m in 2.56s]
[0m03:35:46.519522 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m03:35:46.519815 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m03:35:46.520162 [info ] [Thread-1  ]: 14 of 28 START test unique_stg_payments_payment_id ............................. [RUN]
[0m03:35:46.521170 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m03:35:46.521439 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m03:35:46.521684 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m03:35:46.534939 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m03:35:46.536454 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:46.536662 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m03:35:46.539801 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m03:35:46.540478 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:35:46.542086 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_payments_payment_id.3744510712: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select payment_id as unique_field
  from `jaffle-shop-375704`.`bruno`.`stg_payments`
  where payment_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m03:35:48.774407 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:8497220c-a2f8-4a94-a422-7571863cc7d2:US&page=queryresults
[0m03:35:48.776892 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:48.778414 [info ] [Thread-1  ]: 14 of 28 PASS unique_stg_payments_payment_id ................................... [[32mPASS[0m in 2.26s]
[0m03:35:48.779478 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m03:35:48.780627 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m03:35:48.781283 [info ] [Thread-1  ]: 15 of 28 START python table model bruno.customers .............................. [RUN]
[0m03:35:48.782467 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.jaffle_shop.customers"
[0m03:35:48.782754 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m03:35:48.783016 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m03:35:48.822171 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m03:35:48.823886 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:48.824064 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m03:35:48.847517 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m03:35:48.848665 [debug] [Thread-1  ]: On model.jaffle_shop.customers: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","dbt_python_bigquery_bucket")

import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method='all_purpose_cluster',
        create_notebook=False,
        cluster_id='0124-220040-qfvx2ysq'
    )

    stg_customers_df = dbt.ref('stg_customers')
    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    customer_orders_df = (
        stg_orders_df
        .groupby('customer_id')
        .agg(
            F.min(F.col('order_date')).alias('first_order'),
            F.max(F.col('order_date')).alias('most_recent_order'),
            F.count(F.col('order_id')).alias('number_of_orders')
        )
    )

    customer_payments_df = (
        stg_payments_df
        .join(stg_orders_df, stg_payments_df.order_id == stg_orders_df.order_id, 'left')
        .groupby(stg_orders_df.customer_id)
        .agg(
            F.sum(F.col('amount')).alias('total_amount')
        )
    )

    final_df = (
        stg_customers_df.alias('customers') \
            .join(customer_orders_df.alias('customer_orders'), F.col('customers.customer_id') == F.col('customer_orders.customer_id'), 'left') \
            .join(customer_payments_df.alias('customer_payments'), F.col('customers.customer_id') == F.col('customer_payments.customer_id'), 'left') \
            .select(F.col('customers.customer_id').alias('customer_id'),
                    F.col('customers.first_name').alias('first_name'),
                    F.col('customers.last_name').alias('last_name'),
                    F.col('customer_orders.first_order').alias('first_order'),
                    F.col('customer_orders.most_recent_order').alias('most_recent_order'),
                    F.col('customer_orders.number_of_orders').alias('number_of_orders'),
                    F.col('customer_payments.total_amount').alias('customer_lifetime_value')
            )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle-shop-375704.bruno.stg_customers", "stg_orders": "jaffle-shop-375704.bruno.stg_orders", "stg_payments": "jaffle-shop-375704.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle-shop-375704'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle-shop-375704.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "direct").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("jaffle-shop-375704.bruno.customers")

  
[0m03:35:48.848850 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:48.849230 [error] [Thread-1  ]: [31mUnhandled error while executing model.jaffle_shop.customers[0m
Submission method all_purpose_cluster is not supported for current adapter
[0m03:35:48.849448 [debug] [Thread-1  ]: 
[0m03:35:48.849608 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e801db3f-8e0b-4c0f-9b79-a6f831846b60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dcd2c70>]}
[0m03:35:48.849861 [error] [Thread-1  ]: 15 of 28 ERROR creating python table model bruno.customers ..................... [[31mERROR[0m in 0.07s]
[0m03:35:48.850165 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m03:35:48.850306 [debug] [Thread-1  ]: Began running node model.jaffle_shop.orders
[0m03:35:48.850558 [info ] [Thread-1  ]: 16 of 28 START python table model bruno.orders ................................. [RUN]
[0m03:35:48.851139 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.jaffle_shop.orders"
[0m03:35:48.851260 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.orders
[0m03:35:48.851370 [debug] [Thread-1  ]: Compiling model.jaffle_shop.orders
[0m03:35:48.854782 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.orders"
[0m03:35:48.855770 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:48.855918 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.orders
[0m03:35:48.858383 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.orders"
[0m03:35:48.859042 [debug] [Thread-1  ]: On model.jaffle_shop.orders: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","dbt_python_bigquery_bucket")

import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method='all_purpose_cluster',
        create_notebook=False,
        cluster_id='0124-220040-qfvx2ysq'
    )

    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    payment_methods = ['credit_card', 'coupon', 'bank_transfer', 'gift_card']

    agg_list = [F.sum(F.when(stg_payments_df.payment_method == payment_method, stg_payments_df.amount).otherwise(0)).alias(payment_method + '_amount') for payment_method in payment_methods]

    agg_list.append(F.sum(F.col('amount')).alias('total_amount'))

    order_payments_df = (
        stg_payments_df
        .groupby('order_id')
        .agg(*agg_list)
    )

    final_df = (
        stg_orders_df
        .join(order_payments_df, stg_orders_df.order_id == order_payments_df.order_id, 'left')
        .select(stg_orders_df.order_id.alias('order_id'),
                stg_orders_df.customer_id.alias('customer_id'),
                stg_orders_df.order_date.alias('order_date'),
                stg_orders_df.status.alias('status'),
                *[F.col(payment_method + '_amount') for payment_method in payment_methods],
                order_payments_df.total_amount.alias('amount')
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_orders": "jaffle-shop-375704.bruno.stg_orders", "stg_payments": "jaffle-shop-375704.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle-shop-375704'
    schema = 'bruno'
    identifier = 'orders'
    def __repr__(self):
        return 'jaffle-shop-375704.bruno.orders'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "direct").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("jaffle-shop-375704.bruno.orders")

  
[0m03:35:48.859183 [debug] [Thread-1  ]: finished collecting timing info
[0m03:35:48.859538 [error] [Thread-1  ]: [31mUnhandled error while executing model.jaffle_shop.orders[0m
Submission method all_purpose_cluster is not supported for current adapter
[0m03:35:48.859741 [debug] [Thread-1  ]: 
[0m03:35:48.859888 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e801db3f-8e0b-4c0f-9b79-a6f831846b60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dd2a490>]}
[0m03:35:48.860129 [error] [Thread-1  ]: 16 of 28 ERROR creating python table model bruno.orders ........................ [[31mERROR[0m in 0.01s]
[0m03:35:48.860412 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.orders
[0m03:35:48.860547 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m03:35:48.860768 [info ] [Thread-1  ]: 17 of 28 SKIP test not_null_customers_customer_id .............................. [[33mSKIP[0m]
[0m03:35:48.862387 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m03:35:48.862541 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m03:35:48.862697 [info ] [Thread-1  ]: 18 of 28 SKIP test unique_customers_customer_id ................................ [[33mSKIP[0m]
[0m03:35:48.862936 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m03:35:48.863054 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m03:35:48.863179 [info ] [Thread-1  ]: 19 of 28 SKIP test accepted_values_orders_status__placed__shipped__completed__return_pending__returned  [[33mSKIP[0m]
[0m03:35:48.863394 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m03:35:48.863509 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m03:35:48.863634 [info ] [Thread-1  ]: 20 of 28 SKIP test not_null_orders_amount ...................................... [[33mSKIP[0m]
[0m03:35:48.863853 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m03:35:48.863962 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m03:35:48.864080 [info ] [Thread-1  ]: 21 of 28 SKIP test not_null_orders_bank_transfer_amount ........................ [[33mSKIP[0m]
[0m03:35:48.864290 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m03:35:48.864390 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m03:35:48.864502 [info ] [Thread-1  ]: 22 of 28 SKIP test not_null_orders_coupon_amount ............................... [[33mSKIP[0m]
[0m03:35:48.864694 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m03:35:48.864795 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m03:35:48.864901 [info ] [Thread-1  ]: 23 of 28 SKIP test not_null_orders_credit_card_amount .......................... [[33mSKIP[0m]
[0m03:35:48.865096 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m03:35:48.865197 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m03:35:48.865303 [info ] [Thread-1  ]: 24 of 28 SKIP test not_null_orders_customer_id ................................. [[33mSKIP[0m]
[0m03:35:48.865494 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m03:35:48.865593 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m03:35:48.865699 [info ] [Thread-1  ]: 25 of 28 SKIP test not_null_orders_gift_card_amount ............................ [[33mSKIP[0m]
[0m03:35:48.865889 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m03:35:48.865992 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m03:35:48.866098 [info ] [Thread-1  ]: 26 of 28 SKIP test not_null_orders_order_id .................................... [[33mSKIP[0m]
[0m03:35:48.866289 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m03:35:48.866388 [debug] [Thread-1  ]: Began running node test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m03:35:48.866495 [info ] [Thread-1  ]: 27 of 28 SKIP test relationships_orders_customer_id__customer_id__ref_customers_  [[33mSKIP[0m]
[0m03:35:48.866685 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m03:35:48.866785 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m03:35:48.866887 [info ] [Thread-1  ]: 28 of 28 SKIP test unique_orders_order_id ...................................... [[33mSKIP[0m]
[0m03:35:48.867078 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m03:35:48.867971 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m03:35:48.868401 [info ] [MainThread]: 
[0m03:35:48.868572 [info ] [MainThread]: Finished running 3 seeds, 3 view models, 20 tests, 2 table models in 0 hours 0 minutes and 46.44 seconds (46.44s).
[0m03:35:48.868728 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:35:48.868805 [debug] [MainThread]: Connection 'model.jaffle_shop.orders' was properly closed.
[0m03:35:48.868880 [debug] [MainThread]: Connection 'list_jaffle-shop-375704_bruno' was properly closed.
[0m03:35:48.875822 [info ] [MainThread]: 
[0m03:35:48.876037 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m03:35:48.876215 [info ] [MainThread]: 
[0m03:35:48.876367 [error] [MainThread]: [33mSubmission method all_purpose_cluster is not supported for current adapter[0m
[0m03:35:48.876515 [info ] [MainThread]: 
[0m03:35:48.876656 [error] [MainThread]: [33mSubmission method all_purpose_cluster is not supported for current adapter[0m
[0m03:35:48.876828 [info ] [MainThread]: 
[0m03:35:48.876974 [info ] [MainThread]: Done. PASS=14 WARN=0 ERROR=2 SKIP=12 TOTAL=28
[0m03:35:48.877196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dbb1b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dbb1970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dba9550>]}
[0m03:35:48.877372 [debug] [MainThread]: Flushing usage events


============================== 2023-01-25 03:37:13.612465 | 1d35ac0f-4031-470b-ba77-96c30098fc66 ==============================
[0m03:37:13.612519 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:37:13.613394 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'indirect_selection': 'eager', 'resource_types': [], 'which': 'build', 'rpc_method': 'build'}
[0m03:37:13.613533 [debug] [MainThread]: Tracking: tracking
[0m03:37:13.637204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110372a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110363dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110363e80>]}
[0m03:37:13.689695 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m03:37:13.690082 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/orders.py
[0m03:37:13.690264 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m03:37:13.782167 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1d35ac0f-4031-470b-ba77-96c30098fc66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1105f60d0>]}
[0m03:37:13.789658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1d35ac0f-4031-470b-ba77-96c30098fc66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1105a9c10>]}
[0m03:37:13.789866 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 319 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m03:37:13.790054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1d35ac0f-4031-470b-ba77-96c30098fc66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11038c070>]}
[0m03:37:13.791715 [info ] [MainThread]: 
[0m03:37:13.792232 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m03:37:13.793903 [debug] [ThreadPool]: Acquiring new bigquery connection "list_jaffle-shop-375704"
[0m03:37:13.794153 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:37:14.475303 [debug] [ThreadPool]: Acquiring new bigquery connection "list_jaffle-shop-375704_bruno"
[0m03:37:14.476173 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:37:15.203885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1d35ac0f-4031-470b-ba77-96c30098fc66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11056d970>]}
[0m03:37:15.205249 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:37:15.205791 [info ] [MainThread]: 
[0m03:37:15.213467 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_customers
[0m03:37:15.213951 [info ] [Thread-1  ]: 1 of 28 START seed file bruno.raw_customers .................................... [RUN]
[0m03:37:15.214988 [debug] [Thread-1  ]: Acquiring new bigquery connection "seed.jaffle_shop.raw_customers"
[0m03:37:15.215242 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_customers
[0m03:37:15.215501 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:15.215716 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_customers
[0m03:37:15.258437 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:21.978908 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_customers"
[0m03:37:22.007293 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:22.007850 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d35ac0f-4031-470b-ba77-96c30098fc66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cef9df0>]}
[0m03:37:22.008113 [info ] [Thread-1  ]: 1 of 28 OK loaded seed file bruno.raw_customers ................................ [[32mINSERT 100[0m in 6.79s]
[0m03:37:22.008553 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_customers
[0m03:37:22.008722 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_orders
[0m03:37:22.009009 [info ] [Thread-1  ]: 2 of 28 START seed file bruno.raw_orders ....................................... [RUN]
[0m03:37:22.009767 [debug] [Thread-1  ]: Acquiring new bigquery connection "seed.jaffle_shop.raw_orders"
[0m03:37:22.009938 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_orders
[0m03:37:22.010079 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:22.010212 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_orders
[0m03:37:22.060546 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:28.431316 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_orders"
[0m03:37:28.438348 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:28.439697 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d35ac0f-4031-470b-ba77-96c30098fc66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1105fe280>]}
[0m03:37:28.440261 [info ] [Thread-1  ]: 2 of 28 OK loaded seed file bruno.raw_orders ................................... [[32mINSERT 99[0m in 6.43s]
[0m03:37:28.441025 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_orders
[0m03:37:28.441379 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_payments
[0m03:37:28.441960 [info ] [Thread-1  ]: 3 of 28 START seed file bruno.raw_payments ..................................... [RUN]
[0m03:37:28.443310 [debug] [Thread-1  ]: Acquiring new bigquery connection "seed.jaffle_shop.raw_payments"
[0m03:37:28.443630 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_payments
[0m03:37:28.443898 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:28.444151 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_payments
[0m03:37:28.457279 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:35.758015 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_payments"
[0m03:37:35.764874 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:35.766135 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d35ac0f-4031-470b-ba77-96c30098fc66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1105a6eb0>]}
[0m03:37:35.766684 [info ] [Thread-1  ]: 3 of 28 OK loaded seed file bruno.raw_payments ................................. [[32mINSERT 113[0m in 7.32s]
[0m03:37:35.767463 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_payments
[0m03:37:35.767825 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m03:37:35.768458 [info ] [Thread-1  ]: 4 of 28 START sql view model bruno.stg_customers ............................... [RUN]
[0m03:37:35.769978 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.jaffle_shop.stg_customers"
[0m03:37:35.770365 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m03:37:35.770664 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m03:37:35.780362 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m03:37:35.781437 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:35.781770 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m03:37:35.821986 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_customers"
[0m03:37:35.822496 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:35.823667 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */


  create or replace view `jaffle-shop-375704`.`bruno`.`stg_customers`
  OPTIONS()
  as with source as (
    select * from `jaffle-shop-375704`.`bruno`.`raw_customers`

),

renamed as (

    select
        id as customer_id,
        first_name,
        last_name

    from source

)

select * from renamed;


[0m03:37:37.294249 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:357a5222-533b-4b3c-82f5-66eefc98995e:US&page=queryresults
[0m03:37:37.304620 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:37.305900 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d35ac0f-4031-470b-ba77-96c30098fc66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11054d9a0>]}
[0m03:37:37.307166 [info ] [Thread-1  ]: 4 of 28 OK created sql view model bruno.stg_customers .......................... [[32mCREATE VIEW (0 processed)[0m in 1.54s]
[0m03:37:37.308079 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m03:37:37.308463 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m03:37:37.309013 [info ] [Thread-1  ]: 5 of 28 START sql view model bruno.stg_orders .................................. [RUN]
[0m03:37:37.310361 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.jaffle_shop.stg_orders"
[0m03:37:37.310653 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m03:37:37.310894 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m03:37:37.320462 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m03:37:37.321505 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:37.321794 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m03:37:37.333835 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_orders"
[0m03:37:37.334397 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:37.335912 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */


  create or replace view `jaffle-shop-375704`.`bruno`.`stg_orders`
  OPTIONS()
  as with source as (
    select * from `jaffle-shop-375704`.`bruno`.`raw_orders`

),

renamed as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from source

)

select * from renamed;


[0m03:37:38.687000 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:2f677139-41d2-44e9-a02f-b1eb48b32358:US&page=queryresults
[0m03:37:38.693353 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:38.694839 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d35ac0f-4031-470b-ba77-96c30098fc66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11051b9d0>]}
[0m03:37:38.695476 [info ] [Thread-1  ]: 5 of 28 OK created sql view model bruno.stg_orders ............................. [[32mCREATE VIEW (0 processed)[0m in 1.39s]
[0m03:37:38.696268 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m03:37:38.696618 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m03:37:38.697246 [info ] [Thread-1  ]: 6 of 28 START sql view model bruno.stg_payments ................................ [RUN]
[0m03:37:38.702803 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.jaffle_shop.stg_payments"
[0m03:37:38.703110 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m03:37:38.703352 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m03:37:38.708619 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m03:37:38.709423 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:38.709681 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m03:37:38.714938 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_payments"
[0m03:37:38.715615 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:38.717332 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */


  create or replace view `jaffle-shop-375704`.`bruno`.`stg_payments`
  OPTIONS()
  as with source as (
    select * from `jaffle-shop-375704`.`bruno`.`raw_payments`

),

renamed as (

    select
        id as payment_id,
        order_id,
        payment_method,

        -- `amount` is currently stored in cents, so we convert it to dollars
        amount / 100 as amount

    from source

)

select * from renamed;


[0m03:37:40.196132 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:c5bb00ce-1e56-4cc8-b6fe-ec1b47a8858e:US&page=queryresults
[0m03:37:40.204618 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:40.206757 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d35ac0f-4031-470b-ba77-96c30098fc66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110537250>]}
[0m03:37:40.207818 [info ] [Thread-1  ]: 6 of 28 OK created sql view model bruno.stg_payments ........................... [[32mCREATE VIEW (0 processed)[0m in 1.51s]
[0m03:37:40.208916 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m03:37:40.209419 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:37:40.210432 [info ] [Thread-1  ]: 7 of 28 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m03:37:40.211909 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m03:37:40.212391 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:37:40.212686 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:37:40.234690 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m03:37:40.235500 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:40.235734 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:37:40.256347 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m03:37:40.257219 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:40.258544 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `jaffle-shop-375704`.`bruno`.`stg_customers`
where customer_id is null



      
    ) dbt_internal_test
[0m03:37:42.456887 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:3174d78b-be7a-4fc9-ade8-63e9152e8458:US&page=queryresults
[0m03:37:42.463130 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:42.466159 [info ] [Thread-1  ]: 7 of 28 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 2.25s]
[0m03:37:42.467493 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:37:42.468794 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m03:37:42.469649 [info ] [Thread-1  ]: 8 of 28 START test unique_stg_customers_customer_id ............................ [RUN]
[0m03:37:42.471441 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m03:37:42.471951 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m03:37:42.472354 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m03:37:42.493247 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m03:37:42.494127 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:42.494396 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m03:37:42.498722 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m03:37:42.499379 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:42.501376 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `jaffle-shop-375704`.`bruno`.`stg_customers`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m03:37:44.626226 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:63b64e40-4ce0-450a-9d9e-e55803467fa7:US&page=queryresults
[0m03:37:44.627579 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:44.628827 [info ] [Thread-1  ]: 8 of 28 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 2.16s]
[0m03:37:44.629719 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m03:37:44.630127 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m03:37:44.630591 [info ] [Thread-1  ]: 9 of 28 START test accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned  [RUN]
[0m03:37:44.631957 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m03:37:44.632462 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m03:37:44.632845 [debug] [Thread-1  ]: Compiling test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m03:37:44.685775 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m03:37:44.686518 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:44.686717 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m03:37:44.689858 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m03:37:44.690442 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:44.691812 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from `jaffle-shop-375704`.`bruno`.`stg_orders`
    group by status

)

select *
from all_values
where value_field not in (
    'placed','shipped','completed','return_pending','returned'
)



      
    ) dbt_internal_test
[0m03:37:47.026187 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:d86f2b5c-efc3-4051-9553-493c0a7a779e:US&page=queryresults
[0m03:37:47.028825 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:47.032048 [info ] [Thread-1  ]: 9 of 28 PASS accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned  [[32mPASS[0m in 2.40s]
[0m03:37:47.033523 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m03:37:47.034109 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m03:37:47.034992 [info ] [Thread-1  ]: 10 of 28 START test not_null_stg_orders_order_id ............................... [RUN]
[0m03:37:47.037267 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m03:37:47.039056 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m03:37:47.039606 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m03:37:47.063182 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m03:37:47.064007 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:47.064283 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m03:37:47.068257 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m03:37:47.068827 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:47.070764 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from `jaffle-shop-375704`.`bruno`.`stg_orders`
where order_id is null



      
    ) dbt_internal_test
[0m03:37:49.201996 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:a7d14c84-0958-48d3-848d-c1e8d552eb31:US&page=queryresults
[0m03:37:49.205634 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:49.207961 [info ] [Thread-1  ]: 10 of 28 PASS not_null_stg_orders_order_id ..................................... [[32mPASS[0m in 2.17s]
[0m03:37:49.209345 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m03:37:49.209972 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m03:37:49.210746 [info ] [Thread-1  ]: 11 of 28 START test unique_stg_orders_order_id ................................. [RUN]
[0m03:37:49.212754 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m03:37:49.213374 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m03:37:49.213827 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m03:37:49.237867 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m03:37:49.239032 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:49.239350 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m03:37:49.243636 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m03:37:49.244454 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:49.246516 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `jaffle-shop-375704`.`bruno`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m03:37:51.760775 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:7d057ad7-f947-45c3-a125-334e2e962756:US&page=queryresults
[0m03:37:51.762025 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:51.763317 [info ] [Thread-1  ]: 11 of 28 PASS unique_stg_orders_order_id ....................................... [[32mPASS[0m in 2.55s]
[0m03:37:51.764184 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m03:37:51.764729 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m03:37:51.765231 [info ] [Thread-1  ]: 12 of 28 START test accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card  [RUN]
[0m03:37:51.766586 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m03:37:51.767148 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m03:37:51.767502 [debug] [Thread-1  ]: Compiling test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m03:37:51.794681 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m03:37:51.795468 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:51.795744 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m03:37:51.799858 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m03:37:51.800539 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:51.802651 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        payment_method as value_field,
        count(*) as n_records

    from `jaffle-shop-375704`.`bruno`.`stg_payments`
    group by payment_method

)

select *
from all_values
where value_field not in (
    'credit_card','coupon','bank_transfer','gift_card'
)



      
    ) dbt_internal_test
[0m03:37:54.192335 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:18054fb8-7822-4319-9b9e-73a9f7a52aed:US&page=queryresults
[0m03:37:54.193713 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:54.195058 [info ] [Thread-1  ]: 12 of 28 PASS accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card  [[32mPASS[0m in 2.43s]
[0m03:37:54.195977 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m03:37:54.196437 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m03:37:54.196740 [info ] [Thread-1  ]: 13 of 28 START test not_null_stg_payments_payment_id ........................... [RUN]
[0m03:37:54.198096 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m03:37:54.198474 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m03:37:54.198748 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m03:37:54.210232 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m03:37:54.211305 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:54.211666 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m03:37:54.217640 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m03:37:54.218885 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:54.221588 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select payment_id
from `jaffle-shop-375704`.`bruno`.`stg_payments`
where payment_id is null



      
    ) dbt_internal_test
[0m03:37:56.574804 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:cdf42327-bb3c-453e-942a-ed86a83c6720:US&page=queryresults
[0m03:37:56.576166 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:56.577796 [info ] [Thread-1  ]: 13 of 28 PASS not_null_stg_payments_payment_id ................................. [[32mPASS[0m in 2.38s]
[0m03:37:56.578754 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m03:37:56.579270 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m03:37:56.579942 [info ] [Thread-1  ]: 14 of 28 START test unique_stg_payments_payment_id ............................. [RUN]
[0m03:37:56.581693 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m03:37:56.582216 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m03:37:56.582579 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m03:37:56.597750 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m03:37:56.599063 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:56.599617 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m03:37:56.605928 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m03:37:56.606787 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m03:37:56.609043 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_payments_payment_id.3744510712: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select payment_id as unique_field
  from `jaffle-shop-375704`.`bruno`.`stg_payments`
  where payment_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m03:37:58.935685 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:85c011a2-2c72-48d9-9e34-941c402d035a:US&page=queryresults
[0m03:37:58.937731 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:58.939877 [info ] [Thread-1  ]: 14 of 28 PASS unique_stg_payments_payment_id ................................... [[32mPASS[0m in 2.36s]
[0m03:37:58.941493 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m03:37:58.943393 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m03:37:58.944393 [info ] [Thread-1  ]: 15 of 28 START python table model bruno.customers .............................. [RUN]
[0m03:37:58.946333 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.jaffle_shop.customers"
[0m03:37:58.946829 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m03:37:58.947180 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m03:37:58.984481 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m03:37:58.985136 [debug] [Thread-1  ]: finished collecting timing info
[0m03:37:58.985345 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m03:37:59.011281 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m03:37:59.012464 [debug] [Thread-1  ]: On model.jaffle_shop.customers: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","dbt_python_bigquery_bucket")

import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method="serverless",
    )

    stg_customers_df = dbt.ref('stg_customers')
    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    customer_orders_df = (
        stg_orders_df
        .groupby('customer_id')
        .agg(
            F.min(F.col('order_date')).alias('first_order'),
            F.max(F.col('order_date')).alias('most_recent_order'),
            F.count(F.col('order_id')).alias('number_of_orders')
        )
    )

    customer_payments_df = (
        stg_payments_df
        .join(stg_orders_df, stg_payments_df.order_id == stg_orders_df.order_id, 'left')
        .groupby(stg_orders_df.customer_id)
        .agg(
            F.sum(F.col('amount')).alias('total_amount')
        )
    )

    final_df = (
        stg_customers_df.alias('customers') \
            .join(customer_orders_df.alias('customer_orders'), F.col('customers.customer_id') == F.col('customer_orders.customer_id'), 'left') \
            .join(customer_payments_df.alias('customer_payments'), F.col('customers.customer_id') == F.col('customer_payments.customer_id'), 'left') \
            .select(F.col('customers.customer_id').alias('customer_id'),
                    F.col('customers.first_name').alias('first_name'),
                    F.col('customers.last_name').alias('last_name'),
                    F.col('customer_orders.first_order').alias('first_order'),
                    F.col('customer_orders.most_recent_order').alias('most_recent_order'),
                    F.col('customer_orders.number_of_orders').alias('number_of_orders'),
                    F.col('customer_payments.total_amount').alias('customer_lifetime_value')
            )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle-shop-375704.bruno.stg_customers", "stg_orders": "jaffle-shop-375704.bruno.stg_orders", "stg_payments": "jaffle-shop-375704.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle-shop-375704'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle-shop-375704.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "direct").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("jaffle-shop-375704.bruno.customers")

  
[0m03:38:06.841462 [debug] [Thread-1  ]: finished collecting timing info
[0m03:38:06.843207 [error] [Thread-1  ]: [31mUnhandled error while executing model.jaffle_shop.customers[0m
400 Subnetwork 'default' does not support Private Google Access which is required for Dataproc clusters when 'internal_ip_only' is set to 'true'. Enable Private Google Access on subnetwork 'default' or set 'internal_ip_only' to 'false'.
[0m03:38:06.843975 [debug] [Thread-1  ]: 
[0m03:38:06.845899 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d35ac0f-4031-470b-ba77-96c30098fc66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110765be0>]}
[0m03:38:06.846649 [error] [Thread-1  ]: 15 of 28 ERROR creating python table model bruno.customers ..................... [[31mERROR[0m in 7.90s]
[0m03:38:06.847495 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m03:38:06.847848 [debug] [Thread-1  ]: Began running node model.jaffle_shop.orders
[0m03:38:06.848514 [info ] [Thread-1  ]: 16 of 28 START python table model bruno.orders ................................. [RUN]
[0m03:38:06.850014 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.jaffle_shop.orders"
[0m03:38:06.850359 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.orders
[0m03:38:06.850645 [debug] [Thread-1  ]: Compiling model.jaffle_shop.orders
[0m03:38:06.870483 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.orders"
[0m03:38:06.871152 [debug] [Thread-1  ]: finished collecting timing info
[0m03:38:06.871373 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.orders
[0m03:38:06.881762 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.orders"
[0m03:38:06.882392 [debug] [Thread-1  ]: On model.jaffle_shop.orders: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","dbt_python_bigquery_bucket")

import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method="serverless",
    )

    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    payment_methods = ['credit_card', 'coupon', 'bank_transfer', 'gift_card']

    agg_list = [F.sum(F.when(stg_payments_df.payment_method == payment_method, stg_payments_df.amount).otherwise(0)).alias(payment_method + '_amount') for payment_method in payment_methods]

    agg_list.append(F.sum(F.col('amount')).alias('total_amount'))

    order_payments_df = (
        stg_payments_df
        .groupby('order_id')
        .agg(*agg_list)
    )

    final_df = (
        stg_orders_df
        .join(order_payments_df, stg_orders_df.order_id == order_payments_df.order_id, 'left')
        .select(stg_orders_df.order_id.alias('order_id'),
                stg_orders_df.customer_id.alias('customer_id'),
                stg_orders_df.order_date.alias('order_date'),
                stg_orders_df.status.alias('status'),
                *[F.col(payment_method + '_amount') for payment_method in payment_methods],
                order_payments_df.total_amount.alias('amount')
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_orders": "jaffle-shop-375704.bruno.stg_orders", "stg_payments": "jaffle-shop-375704.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle-shop-375704'
    schema = 'bruno'
    identifier = 'orders'
    def __repr__(self):
        return 'jaffle-shop-375704.bruno.orders'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "direct").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("jaffle-shop-375704.bruno.orders")

  
[0m03:38:14.700541 [debug] [Thread-1  ]: finished collecting timing info
[0m03:38:14.703498 [error] [Thread-1  ]: [31mUnhandled error while executing model.jaffle_shop.orders[0m
400 Subnetwork 'default' does not support Private Google Access which is required for Dataproc clusters when 'internal_ip_only' is set to 'true'. Enable Private Google Access on subnetwork 'default' or set 'internal_ip_only' to 'false'.
[0m03:38:14.704640 [debug] [Thread-1  ]: 
[0m03:38:14.706185 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d35ac0f-4031-470b-ba77-96c30098fc66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1107eb4f0>]}
[0m03:38:14.707246 [error] [Thread-1  ]: 16 of 28 ERROR creating python table model bruno.orders ........................ [[31mERROR[0m in 7.86s]
[0m03:38:14.708619 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.orders
[0m03:38:14.709020 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m03:38:14.709675 [info ] [Thread-1  ]: 17 of 28 SKIP test not_null_customers_customer_id .............................. [[33mSKIP[0m]
[0m03:38:14.710940 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m03:38:14.711326 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m03:38:14.711778 [info ] [Thread-1  ]: 18 of 28 SKIP test unique_customers_customer_id ................................ [[33mSKIP[0m]
[0m03:38:14.712555 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m03:38:14.712897 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m03:38:14.713268 [info ] [Thread-1  ]: 19 of 28 SKIP test accepted_values_orders_status__placed__shipped__completed__return_pending__returned  [[33mSKIP[0m]
[0m03:38:14.714004 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m03:38:14.714327 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m03:38:14.714745 [info ] [Thread-1  ]: 20 of 28 SKIP test not_null_orders_amount ...................................... [[33mSKIP[0m]
[0m03:38:14.715525 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m03:38:14.715867 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m03:38:14.716256 [info ] [Thread-1  ]: 21 of 28 SKIP test not_null_orders_bank_transfer_amount ........................ [[33mSKIP[0m]
[0m03:38:14.716886 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m03:38:14.717211 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m03:38:14.717477 [info ] [Thread-1  ]: 22 of 28 SKIP test not_null_orders_coupon_amount ............................... [[33mSKIP[0m]
[0m03:38:14.718259 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m03:38:14.718593 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m03:38:14.718942 [info ] [Thread-1  ]: 23 of 28 SKIP test not_null_orders_credit_card_amount .......................... [[33mSKIP[0m]
[0m03:38:14.719578 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m03:38:14.719884 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m03:38:14.720241 [info ] [Thread-1  ]: 24 of 28 SKIP test not_null_orders_customer_id ................................. [[33mSKIP[0m]
[0m03:38:14.720814 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m03:38:14.721083 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m03:38:14.721370 [info ] [Thread-1  ]: 25 of 28 SKIP test not_null_orders_gift_card_amount ............................ [[33mSKIP[0m]
[0m03:38:14.721903 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m03:38:14.722156 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m03:38:14.722424 [info ] [Thread-1  ]: 26 of 28 SKIP test not_null_orders_order_id .................................... [[33mSKIP[0m]
[0m03:38:14.722938 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m03:38:14.723191 [debug] [Thread-1  ]: Began running node test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m03:38:14.723497 [info ] [Thread-1  ]: 27 of 28 SKIP test relationships_orders_customer_id__customer_id__ref_customers_  [[33mSKIP[0m]
[0m03:38:14.724042 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m03:38:14.724305 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m03:38:14.724584 [info ] [Thread-1  ]: 28 of 28 SKIP test unique_orders_order_id ...................................... [[33mSKIP[0m]
[0m03:38:14.725108 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m03:38:14.726881 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m03:38:14.727869 [info ] [MainThread]: 
[0m03:38:14.728281 [info ] [MainThread]: Finished running 3 seeds, 3 view models, 20 tests, 2 table models in 0 hours 1 minutes and 0.94 seconds (60.94s).
[0m03:38:14.728654 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:38:14.728846 [debug] [MainThread]: Connection 'model.jaffle_shop.orders' was properly closed.
[0m03:38:14.748514 [info ] [MainThread]: 
[0m03:38:14.748879 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m03:38:14.749195 [info ] [MainThread]: 
[0m03:38:14.749463 [error] [MainThread]: [33m400 Subnetwork 'default' does not support Private Google Access which is required for Dataproc clusters when 'internal_ip_only' is set to 'true'. Enable Private Google Access on subnetwork 'default' or set 'internal_ip_only' to 'false'.[0m
[0m03:38:14.749722 [info ] [MainThread]: 
[0m03:38:14.749966 [error] [MainThread]: [33m400 Subnetwork 'default' does not support Private Google Access which is required for Dataproc clusters when 'internal_ip_only' is set to 'true'. Enable Private Google Access on subnetwork 'default' or set 'internal_ip_only' to 'false'.[0m
[0m03:38:14.750261 [info ] [MainThread]: 
[0m03:38:14.750514 [info ] [MainThread]: Done. PASS=14 WARN=0 ERROR=2 SKIP=12 TOTAL=28
[0m03:38:14.750875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1105f62b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ef4c280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110363160>]}
[0m03:38:14.751150 [debug] [MainThread]: Flushing usage events


============================== 2023-01-25 03:42:59.529023 | eba2d00a-2a0b-4343-80e3-4113a20d76c7 ==============================
[0m03:42:59.529071 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:42:59.529857 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'indirect_selection': 'eager', 'resource_types': [], 'select': ['orders.py', 'customers.py'], 'which': 'build', 'rpc_method': 'build'}
[0m03:42:59.530038 [debug] [MainThread]: Tracking: tracking
[0m03:42:59.553497 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d48ba60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d47be80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d47bdf0>]}
[0m03:42:59.606598 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m03:42:59.606778 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m03:42:59.612579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'eba2d00a-2a0b-4343-80e3-4113a20d76c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d65b0d0>]}
[0m03:42:59.620920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'eba2d00a-2a0b-4343-80e3-4113a20d76c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d590c10>]}
[0m03:42:59.621139 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 319 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m03:42:59.621334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eba2d00a-2a0b-4343-80e3-4113a20d76c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d4a3580>]}
[0m03:42:59.622910 [info ] [MainThread]: 
[0m03:42:59.623405 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m03:42:59.624261 [debug] [ThreadPool]: Acquiring new bigquery connection "list_jaffle-shop-375704"
[0m03:42:59.624426 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:43:00.727552 [debug] [ThreadPool]: Acquiring new bigquery connection "list_jaffle-shop-375704_bruno"
[0m03:43:00.729129 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:43:01.443206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eba2d00a-2a0b-4343-80e3-4113a20d76c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d48bf10>]}
[0m03:43:01.446803 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:43:01.447914 [info ] [MainThread]: 
[0m03:43:01.456855 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m03:43:01.457535 [info ] [Thread-1  ]: 1 of 14 START python table model bruno.customers ............................... [RUN]
[0m03:43:01.458936 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.jaffle_shop.customers"
[0m03:43:01.459210 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m03:43:01.459465 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m03:43:01.490876 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m03:43:01.491960 [debug] [Thread-1  ]: finished collecting timing info
[0m03:43:01.492150 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m03:43:01.532221 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m03:43:01.533515 [debug] [Thread-1  ]: On model.jaffle_shop.customers: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","dbt_python_bigquery_bucket")

import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method="serverless",
    )

    stg_customers_df = dbt.ref('stg_customers')
    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    customer_orders_df = (
        stg_orders_df
        .groupby('customer_id')
        .agg(
            F.min(F.col('order_date')).alias('first_order'),
            F.max(F.col('order_date')).alias('most_recent_order'),
            F.count(F.col('order_id')).alias('number_of_orders')
        )
    )

    customer_payments_df = (
        stg_payments_df
        .join(stg_orders_df, stg_payments_df.order_id == stg_orders_df.order_id, 'left')
        .groupby(stg_orders_df.customer_id)
        .agg(
            F.sum(F.col('amount')).alias('total_amount')
        )
    )

    final_df = (
        stg_customers_df.alias('customers') \
            .join(customer_orders_df.alias('customer_orders'), F.col('customers.customer_id') == F.col('customer_orders.customer_id'), 'left') \
            .join(customer_payments_df.alias('customer_payments'), F.col('customers.customer_id') == F.col('customer_payments.customer_id'), 'left') \
            .select(F.col('customers.customer_id').alias('customer_id'),
                    F.col('customers.first_name').alias('first_name'),
                    F.col('customers.last_name').alias('last_name'),
                    F.col('customer_orders.first_order').alias('first_order'),
                    F.col('customer_orders.most_recent_order').alias('most_recent_order'),
                    F.col('customer_orders.number_of_orders').alias('number_of_orders'),
                    F.col('customer_payments.total_amount').alias('customer_lifetime_value')
            )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle-shop-375704.bruno.stg_customers", "stg_orders": "jaffle-shop-375704.bruno.stg_orders", "stg_payments": "jaffle-shop-375704.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle-shop-375704'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle-shop-375704.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "direct").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("jaffle-shop-375704.bruno.customers")

  
[0m03:43:09.429599 [debug] [Thread-1  ]: finished collecting timing info
[0m03:43:09.431316 [error] [Thread-1  ]: [31mUnhandled error while executing model.jaffle_shop.customers[0m
400 User not authorized to act as service account '774124611193-compute@developer.gserviceaccount.com'. To act as a service account, user must have one of [Owner, Editor, Service Account Actor] roles. See https://cloud.google.com/iam/docs/understanding-service-accounts for additional details.
[0m03:43:09.431956 [debug] [Thread-1  ]: 
[0m03:43:09.434193 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eba2d00a-2a0b-4343-80e3-4113a20d76c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d750f70>]}
[0m03:43:09.434961 [error] [Thread-1  ]: 1 of 14 ERROR creating python table model bruno.customers ...................... [[31mERROR[0m in 7.98s]
[0m03:43:09.435884 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m03:43:09.436240 [debug] [Thread-1  ]: Began running node model.jaffle_shop.orders
[0m03:43:09.436903 [info ] [Thread-1  ]: 2 of 14 START python table model bruno.orders .................................. [RUN]
[0m03:43:09.438343 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.jaffle_shop.orders"
[0m03:43:09.438715 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.orders
[0m03:43:09.439008 [debug] [Thread-1  ]: Compiling model.jaffle_shop.orders
[0m03:43:09.445971 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.orders"
[0m03:43:09.447047 [debug] [Thread-1  ]: finished collecting timing info
[0m03:43:09.447341 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.orders
[0m03:43:09.452005 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.orders"
[0m03:43:09.452941 [debug] [Thread-1  ]: On model.jaffle_shop.orders: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","dbt_python_bigquery_bucket")

import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method="serverless",
    )

    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    payment_methods = ['credit_card', 'coupon', 'bank_transfer', 'gift_card']

    agg_list = [F.sum(F.when(stg_payments_df.payment_method == payment_method, stg_payments_df.amount).otherwise(0)).alias(payment_method + '_amount') for payment_method in payment_methods]

    agg_list.append(F.sum(F.col('amount')).alias('total_amount'))

    order_payments_df = (
        stg_payments_df
        .groupby('order_id')
        .agg(*agg_list)
    )

    final_df = (
        stg_orders_df
        .join(order_payments_df, stg_orders_df.order_id == order_payments_df.order_id, 'left')
        .select(stg_orders_df.order_id.alias('order_id'),
                stg_orders_df.customer_id.alias('customer_id'),
                stg_orders_df.order_date.alias('order_date'),
                stg_orders_df.status.alias('status'),
                *[F.col(payment_method + '_amount') for payment_method in payment_methods],
                order_payments_df.total_amount.alias('amount')
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_orders": "jaffle-shop-375704.bruno.stg_orders", "stg_payments": "jaffle-shop-375704.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle-shop-375704'
    schema = 'bruno'
    identifier = 'orders'
    def __repr__(self):
        return 'jaffle-shop-375704.bruno.orders'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "direct").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("jaffle-shop-375704.bruno.orders")

  
[0m03:43:17.412008 [debug] [Thread-1  ]: finished collecting timing info
[0m03:43:17.416348 [error] [Thread-1  ]: [31mUnhandled error while executing model.jaffle_shop.orders[0m
400 User not authorized to act as service account '774124611193-compute@developer.gserviceaccount.com'. To act as a service account, user must have one of [Owner, Editor, Service Account Actor] roles. See https://cloud.google.com/iam/docs/understanding-service-accounts for additional details.
[0m03:43:17.417450 [debug] [Thread-1  ]: 
[0m03:43:17.419787 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eba2d00a-2a0b-4343-80e3-4113a20d76c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d6f1610>]}
[0m03:43:17.420802 [error] [Thread-1  ]: 2 of 14 ERROR creating python table model bruno.orders ......................... [[31mERROR[0m in 7.98s]
[0m03:43:17.422382 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.orders
[0m03:43:17.423066 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m03:43:17.423789 [info ] [Thread-1  ]: 3 of 14 SKIP test not_null_customers_customer_id ............................... [[33mSKIP[0m]
[0m03:43:17.425113 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m03:43:17.425488 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m03:43:17.425849 [info ] [Thread-1  ]: 4 of 14 SKIP test unique_customers_customer_id ................................. [[33mSKIP[0m]
[0m03:43:17.426560 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m03:43:17.426900 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m03:43:17.427178 [info ] [Thread-1  ]: 5 of 14 SKIP test accepted_values_orders_status__placed__shipped__completed__return_pending__returned  [[33mSKIP[0m]
[0m03:43:17.428046 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m03:43:17.428424 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m03:43:17.428818 [info ] [Thread-1  ]: 6 of 14 SKIP test not_null_orders_amount ....................................... [[33mSKIP[0m]
[0m03:43:17.429493 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m03:43:17.429831 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m03:43:17.430184 [info ] [Thread-1  ]: 7 of 14 SKIP test not_null_orders_bank_transfer_amount ......................... [[33mSKIP[0m]
[0m03:43:17.430842 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m03:43:17.431169 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m03:43:17.431538 [info ] [Thread-1  ]: 8 of 14 SKIP test not_null_orders_coupon_amount ................................ [[33mSKIP[0m]
[0m03:43:17.432172 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m03:43:17.432477 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m03:43:17.432841 [info ] [Thread-1  ]: 9 of 14 SKIP test not_null_orders_credit_card_amount ........................... [[33mSKIP[0m]
[0m03:43:17.433459 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m03:43:17.433723 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m03:43:17.433947 [info ] [Thread-1  ]: 10 of 14 SKIP test not_null_orders_customer_id ................................. [[33mSKIP[0m]
[0m03:43:17.434610 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m03:43:17.434919 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m03:43:17.435217 [info ] [Thread-1  ]: 11 of 14 SKIP test not_null_orders_gift_card_amount ............................ [[33mSKIP[0m]
[0m03:43:17.435742 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m03:43:17.436019 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m03:43:17.436324 [info ] [Thread-1  ]: 12 of 14 SKIP test not_null_orders_order_id .................................... [[33mSKIP[0m]
[0m03:43:17.436925 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m03:43:17.437200 [debug] [Thread-1  ]: Began running node test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m03:43:17.437492 [info ] [Thread-1  ]: 13 of 14 SKIP test relationships_orders_customer_id__customer_id__ref_customers_  [[33mSKIP[0m]
[0m03:43:17.437980 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m03:43:17.438231 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m03:43:17.438529 [info ] [Thread-1  ]: 14 of 14 SKIP test unique_orders_order_id ...................................... [[33mSKIP[0m]
[0m03:43:17.439062 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m03:43:17.441038 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m03:43:17.441955 [info ] [MainThread]: 
[0m03:43:17.442376 [info ] [MainThread]: Finished running 2 table models, 12 tests in 0 hours 0 minutes and 17.82 seconds (17.82s).
[0m03:43:17.442763 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:43:17.442959 [debug] [MainThread]: Connection 'model.jaffle_shop.orders' was properly closed.
[0m03:43:17.461267 [info ] [MainThread]: 
[0m03:43:17.461646 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m03:43:17.461974 [info ] [MainThread]: 
[0m03:43:17.462244 [error] [MainThread]: [33m400 User not authorized to act as service account '774124611193-compute@developer.gserviceaccount.com'. To act as a service account, user must have one of [Owner, Editor, Service Account Actor] roles. See https://cloud.google.com/iam/docs/understanding-service-accounts for additional details.[0m
[0m03:43:17.462510 [info ] [MainThread]: 
[0m03:43:17.462759 [error] [MainThread]: [33m400 User not authorized to act as service account '774124611193-compute@developer.gserviceaccount.com'. To act as a service account, user must have one of [Owner, Editor, Service Account Actor] roles. See https://cloud.google.com/iam/docs/understanding-service-accounts for additional details.[0m
[0m03:43:17.463031 [info ] [MainThread]: 
[0m03:43:17.463293 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=12 TOTAL=14
[0m03:43:17.463660 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d79be80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d766f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d84bb80>]}
[0m03:43:17.463940 [debug] [MainThread]: Flushing usage events


============================== 2023-01-25 03:45:49.657509 | 431141fd-3ffd-4a0c-b41a-3d950a6a9c97 ==============================
[0m03:45:49.657558 [info ] [MainThread]: Running with dbt=1.3.2
[0m03:45:49.658288 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'indirect_selection': 'eager', 'resource_types': [], 'select': ['orders.py', 'customers.py'], 'which': 'build', 'rpc_method': 'build'}
[0m03:45:49.658432 [debug] [MainThread]: Tracking: tracking
[0m03:45:49.682590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d483430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d474850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d4747c0>]}
[0m03:45:49.736056 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m03:45:49.736221 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m03:45:49.741890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '431141fd-3ffd-4a0c-b41a-3d950a6a9c97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d6500d0>]}
[0m03:45:49.749712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '431141fd-3ffd-4a0c-b41a-3d950a6a9c97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d57d460>]}
[0m03:45:49.749924 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 319 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m03:45:49.750120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '431141fd-3ffd-4a0c-b41a-3d950a6a9c97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d57d6a0>]}
[0m03:45:49.751727 [info ] [MainThread]: 
[0m03:45:49.752205 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m03:45:49.753069 [debug] [ThreadPool]: Acquiring new bigquery connection "list_jaffle-shop-375704"
[0m03:45:49.753232 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:45:50.449112 [debug] [ThreadPool]: Acquiring new bigquery connection "list_jaffle-shop-375704_bruno"
[0m03:45:50.449756 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:45:51.231327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '431141fd-3ffd-4a0c-b41a-3d950a6a9c97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d57d580>]}
[0m03:45:51.233285 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:45:51.233888 [info ] [MainThread]: 
[0m03:45:51.242589 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m03:45:51.243049 [info ] [Thread-1  ]: 1 of 14 START python table model bruno.customers ............................... [RUN]
[0m03:45:51.244070 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.jaffle_shop.customers"
[0m03:45:51.244321 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m03:45:51.244544 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m03:45:51.274920 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m03:45:51.276179 [debug] [Thread-1  ]: finished collecting timing info
[0m03:45:51.276368 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m03:45:51.315258 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m03:45:51.316769 [debug] [Thread-1  ]: On model.jaffle_shop.customers: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","dbt_python_bigquery_bucket")

import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method="serverless",
    )

    stg_customers_df = dbt.ref('stg_customers')
    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    customer_orders_df = (
        stg_orders_df
        .groupby('customer_id')
        .agg(
            F.min(F.col('order_date')).alias('first_order'),
            F.max(F.col('order_date')).alias('most_recent_order'),
            F.count(F.col('order_id')).alias('number_of_orders')
        )
    )

    customer_payments_df = (
        stg_payments_df
        .join(stg_orders_df, stg_payments_df.order_id == stg_orders_df.order_id, 'left')
        .groupby(stg_orders_df.customer_id)
        .agg(
            F.sum(F.col('amount')).alias('total_amount')
        )
    )

    final_df = (
        stg_customers_df.alias('customers') \
            .join(customer_orders_df.alias('customer_orders'), F.col('customers.customer_id') == F.col('customer_orders.customer_id'), 'left') \
            .join(customer_payments_df.alias('customer_payments'), F.col('customers.customer_id') == F.col('customer_payments.customer_id'), 'left') \
            .select(F.col('customers.customer_id').alias('customer_id'),
                    F.col('customers.first_name').alias('first_name'),
                    F.col('customers.last_name').alias('last_name'),
                    F.col('customer_orders.first_order').alias('first_order'),
                    F.col('customer_orders.most_recent_order').alias('most_recent_order'),
                    F.col('customer_orders.number_of_orders').alias('number_of_orders'),
                    F.col('customer_payments.total_amount').alias('customer_lifetime_value')
            )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle-shop-375704.bruno.stg_customers", "stg_orders": "jaffle-shop-375704.bruno.stg_orders", "stg_payments": "jaffle-shop-375704.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle-shop-375704'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle-shop-375704.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "direct").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("jaffle-shop-375704.bruno.customers")

  
[0m03:46:01.413429 [debug] [Thread-1  ]: finished collecting timing info
[0m03:46:01.417676 [error] [Thread-1  ]: [31mUnhandled error while executing model.jaffle_shop.customers[0m
400 Multiple validation errors:
 - Insufficient 'CPUS' quota. Requested 12.0, available 8.0.
 - This request exceeds CPU quota. Some things to try: request fewer workers (a minimum of 2 is required), use smaller master and/or worker machine types (such as n1-standard-2).
[0m03:46:01.419141 [debug] [Thread-1  ]: 
[0m03:46:01.420041 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '431141fd-3ffd-4a0c-b41a-3d950a6a9c97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d779100>]}
[0m03:46:01.420820 [error] [Thread-1  ]: 1 of 14 ERROR creating python table model bruno.customers ...................... [[31mERROR[0m in 10.18s]
[0m03:46:01.421739 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m03:46:01.422119 [debug] [Thread-1  ]: Began running node model.jaffle_shop.orders
[0m03:46:01.422873 [info ] [Thread-1  ]: 2 of 14 START python table model bruno.orders .................................. [RUN]
[0m03:46:01.424575 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.jaffle_shop.orders"
[0m03:46:01.424915 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.orders
[0m03:46:01.425203 [debug] [Thread-1  ]: Compiling model.jaffle_shop.orders
[0m03:46:01.434892 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.orders"
[0m03:46:01.436017 [debug] [Thread-1  ]: finished collecting timing info
[0m03:46:01.436296 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.orders
[0m03:46:01.440665 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.orders"
[0m03:46:01.441578 [debug] [Thread-1  ]: On model.jaffle_shop.orders: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","dbt_python_bigquery_bucket")

import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method="serverless",
    )

    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    payment_methods = ['credit_card', 'coupon', 'bank_transfer', 'gift_card']

    agg_list = [F.sum(F.when(stg_payments_df.payment_method == payment_method, stg_payments_df.amount).otherwise(0)).alias(payment_method + '_amount') for payment_method in payment_methods]

    agg_list.append(F.sum(F.col('amount')).alias('total_amount'))

    order_payments_df = (
        stg_payments_df
        .groupby('order_id')
        .agg(*agg_list)
    )

    final_df = (
        stg_orders_df
        .join(order_payments_df, stg_orders_df.order_id == order_payments_df.order_id, 'left')
        .select(stg_orders_df.order_id.alias('order_id'),
                stg_orders_df.customer_id.alias('customer_id'),
                stg_orders_df.order_date.alias('order_date'),
                stg_orders_df.status.alias('status'),
                *[F.col(payment_method + '_amount') for payment_method in payment_methods],
                order_payments_df.total_amount.alias('amount')
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_orders": "jaffle-shop-375704.bruno.stg_orders", "stg_payments": "jaffle-shop-375704.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle-shop-375704'
    schema = 'bruno'
    identifier = 'orders'
    def __repr__(self):
        return 'jaffle-shop-375704.bruno.orders'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "direct").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("jaffle-shop-375704.bruno.orders")

  
[0m03:46:09.863692 [debug] [Thread-1  ]: finished collecting timing info
[0m03:46:09.867599 [error] [Thread-1  ]: [31mUnhandled error while executing model.jaffle_shop.orders[0m
400 Multiple validation errors:
 - Insufficient 'CPUS' quota. Requested 12.0, available 8.0.
 - This request exceeds CPU quota. Some things to try: request fewer workers (a minimum of 2 is required), use smaller master and/or worker machine types (such as n1-standard-2).
[0m03:46:09.868244 [debug] [Thread-1  ]: 
[0m03:46:09.868762 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '431141fd-3ffd-4a0c-b41a-3d950a6a9c97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d826130>]}
[0m03:46:09.869763 [error] [Thread-1  ]: 2 of 14 ERROR creating python table model bruno.orders ......................... [[31mERROR[0m in 8.44s]
[0m03:46:09.871087 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.orders
[0m03:46:09.871550 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m03:46:09.872206 [info ] [Thread-1  ]: 3 of 14 SKIP test not_null_customers_customer_id ............................... [[33mSKIP[0m]
[0m03:46:09.875286 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m03:46:09.879949 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m03:46:09.880563 [info ] [Thread-1  ]: 4 of 14 SKIP test unique_customers_customer_id ................................. [[33mSKIP[0m]
[0m03:46:09.881488 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m03:46:09.882373 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m03:46:09.882827 [info ] [Thread-1  ]: 5 of 14 SKIP test accepted_values_orders_status__placed__shipped__completed__return_pending__returned  [[33mSKIP[0m]
[0m03:46:09.883424 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m03:46:09.883705 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m03:46:09.884036 [info ] [Thread-1  ]: 6 of 14 SKIP test not_null_orders_amount ....................................... [[33mSKIP[0m]
[0m03:46:09.884533 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m03:46:09.884792 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m03:46:09.885063 [info ] [Thread-1  ]: 7 of 14 SKIP test not_null_orders_bank_transfer_amount ......................... [[33mSKIP[0m]
[0m03:46:09.885534 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m03:46:09.885785 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m03:46:09.886078 [info ] [Thread-1  ]: 8 of 14 SKIP test not_null_orders_coupon_amount ................................ [[33mSKIP[0m]
[0m03:46:09.886542 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m03:46:09.886780 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m03:46:09.887040 [info ] [Thread-1  ]: 9 of 14 SKIP test not_null_orders_credit_card_amount ........................... [[33mSKIP[0m]
[0m03:46:09.887495 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m03:46:09.887731 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m03:46:09.888003 [info ] [Thread-1  ]: 10 of 14 SKIP test not_null_orders_customer_id ................................. [[33mSKIP[0m]
[0m03:46:09.888603 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m03:46:09.888879 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m03:46:09.889168 [info ] [Thread-1  ]: 11 of 14 SKIP test not_null_orders_gift_card_amount ............................ [[33mSKIP[0m]
[0m03:46:09.889662 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m03:46:09.889908 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m03:46:09.890173 [info ] [Thread-1  ]: 12 of 14 SKIP test not_null_orders_order_id .................................... [[33mSKIP[0m]
[0m03:46:09.890683 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m03:46:09.890937 [debug] [Thread-1  ]: Began running node test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m03:46:09.891237 [info ] [Thread-1  ]: 13 of 14 SKIP test relationships_orders_customer_id__customer_id__ref_customers_  [[33mSKIP[0m]
[0m03:46:09.891886 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m03:46:09.892181 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m03:46:09.892472 [info ] [Thread-1  ]: 14 of 14 SKIP test unique_orders_order_id ...................................... [[33mSKIP[0m]
[0m03:46:09.892995 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m03:46:09.894312 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m03:46:09.895076 [info ] [MainThread]: 
[0m03:46:09.895427 [info ] [MainThread]: Finished running 2 table models, 12 tests in 0 hours 0 minutes and 20.14 seconds (20.14s).
[0m03:46:09.895759 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:46:09.895929 [debug] [MainThread]: Connection 'model.jaffle_shop.orders' was properly closed.
[0m03:46:09.910120 [info ] [MainThread]: 
[0m03:46:09.910514 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m03:46:09.910829 [info ] [MainThread]: 
[0m03:46:09.911102 [error] [MainThread]: [33m400 Multiple validation errors:[0m
[0m03:46:09.911485 [error] [MainThread]:  - Insufficient 'CPUS' quota. Requested 12.0, available 8.0.
[0m03:46:09.911757 [error] [MainThread]:  - This request exceeds CPU quota. Some things to try: request fewer workers (a minimum of 2 is required), use smaller master and/or worker machine types (such as n1-standard-2).
[0m03:46:09.912028 [info ] [MainThread]: 
[0m03:46:09.912306 [error] [MainThread]: [33m400 Multiple validation errors:[0m
[0m03:46:09.912568 [error] [MainThread]:  - Insufficient 'CPUS' quota. Requested 12.0, available 8.0.
[0m03:46:09.912814 [error] [MainThread]:  - This request exceeds CPU quota. Some things to try: request fewer workers (a minimum of 2 is required), use smaller master and/or worker machine types (such as n1-standard-2).
[0m03:46:09.913061 [info ] [MainThread]: 
[0m03:46:09.913306 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=12 TOTAL=14
[0m03:46:09.913683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d57d3a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d57d400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d73d550>]}
[0m03:46:09.914003 [debug] [MainThread]: Flushing usage events


============================== 2023-01-25 04:23:32.444547 | 014dad15-5452-4d65-84e7-8d3556c154bf ==============================
[0m04:23:32.444595 [info ] [MainThread]: Running with dbt=1.3.2
[0m04:23:32.445310 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'indirect_selection': 'eager', 'resource_types': [], 'select': ['orders.py', 'customers.py'], 'which': 'build', 'rpc_method': 'build'}
[0m04:23:32.445453 [debug] [MainThread]: Tracking: tracking
[0m04:23:32.474496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fc76a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fc67e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fc67dc0>]}
[0m04:23:32.527825 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m04:23:32.528196 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/customers.py
[0m04:23:32.528391 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/orders.py
[0m04:23:32.618400 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '014dad15-5452-4d65-84e7-8d3556c154bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ff030d0>]}
[0m04:23:32.626741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '014dad15-5452-4d65-84e7-8d3556c154bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fe4f5e0>]}
[0m04:23:32.626962 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 319 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m04:23:32.627163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '014dad15-5452-4d65-84e7-8d3556c154bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fc8ff40>]}
[0m04:23:32.628725 [info ] [MainThread]: 
[0m04:23:32.629221 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m04:23:32.630068 [debug] [ThreadPool]: Acquiring new bigquery connection "list_jaffle-shop-375704"
[0m04:23:32.630229 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:23:33.656112 [debug] [ThreadPool]: Acquiring new bigquery connection "list_jaffle-shop-375704_bruno"
[0m04:23:33.656690 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m04:23:34.366789 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '014dad15-5452-4d65-84e7-8d3556c154bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fe4f910>]}
[0m04:23:34.368270 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m04:23:34.369925 [info ] [MainThread]: 
[0m04:23:34.377228 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m04:23:34.377764 [info ] [Thread-1  ]: 1 of 14 START python table model bruno.customers ............................... [RUN]
[0m04:23:34.378837 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.jaffle_shop.customers"
[0m04:23:34.379114 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m04:23:34.379376 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m04:23:34.409872 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m04:23:34.411107 [debug] [Thread-1  ]: finished collecting timing info
[0m04:23:34.411292 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m04:23:34.443464 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m04:23:34.444387 [debug] [Thread-1  ]: On model.jaffle_shop.customers: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","dbt_python_bigquery_bucket")

import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method="cluster",
        dataproc_cluster_name="dbt-python"
    )

    stg_customers_df = dbt.ref('stg_customers')
    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    customer_orders_df = (
        stg_orders_df
        .groupby('customer_id')
        .agg(
            F.min(F.col('order_date')).alias('first_order'),
            F.max(F.col('order_date')).alias('most_recent_order'),
            F.count(F.col('order_id')).alias('number_of_orders')
        )
    )

    customer_payments_df = (
        stg_payments_df
        .join(stg_orders_df, stg_payments_df.order_id == stg_orders_df.order_id, 'left')
        .groupby(stg_orders_df.customer_id)
        .agg(
            F.sum(F.col('amount')).alias('total_amount')
        )
    )

    final_df = (
        stg_customers_df.alias('customers') \
            .join(customer_orders_df.alias('customer_orders'), F.col('customers.customer_id') == F.col('customer_orders.customer_id'), 'left') \
            .join(customer_payments_df.alias('customer_payments'), F.col('customers.customer_id') == F.col('customer_payments.customer_id'), 'left') \
            .select(F.col('customers.customer_id').alias('customer_id'),
                    F.col('customers.first_name').alias('first_name'),
                    F.col('customers.last_name').alias('last_name'),
                    F.col('customer_orders.first_order').alias('first_order'),
                    F.col('customer_orders.most_recent_order').alias('most_recent_order'),
                    F.col('customer_orders.number_of_orders').alias('number_of_orders'),
                    F.col('customer_payments.total_amount').alias('customer_lifetime_value')
            )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle-shop-375704.bruno.stg_customers", "stg_orders": "jaffle-shop-375704.bruno.stg_orders", "stg_payments": "jaffle-shop-375704.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle-shop-375704'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle-shop-375704.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "direct").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("jaffle-shop-375704.bruno.customers")

  
[0m04:25:13.862343 [debug] [Thread-1  ]: Execution status: OK in 99.42 seconds
[0m04:25:13.962029 [debug] [Thread-1  ]: finished collecting timing info
[0m04:25:13.962485 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '014dad15-5452-4d65-84e7-8d3556c154bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11003b190>]}
[0m04:25:13.962716 [info ] [Thread-1  ]: 1 of 14 OK created python table model bruno.customers .......................... [[32mOK[0m in 99.58s]
[0m04:25:13.963034 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m04:25:13.963168 [debug] [Thread-1  ]: Began running node model.jaffle_shop.orders
[0m04:25:13.963307 [info ] [Thread-1  ]: 2 of 14 START python table model bruno.orders .................................. [RUN]
[0m04:25:13.963876 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.jaffle_shop.orders"
[0m04:25:13.963992 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.orders
[0m04:25:13.964090 [debug] [Thread-1  ]: Compiling model.jaffle_shop.orders
[0m04:25:13.966763 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.orders"
[0m04:25:13.967768 [debug] [Thread-1  ]: finished collecting timing info
[0m04:25:13.967883 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.orders
[0m04:25:13.970078 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.orders"
[0m04:25:13.971144 [debug] [Thread-1  ]: On model.jaffle_shop.orders: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","dbt_python_bigquery_bucket")

import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method="cluster",
        dataproc_cluster_name="dbt-python"
    )

    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    payment_methods = ['credit_card', 'coupon', 'bank_transfer', 'gift_card']

    agg_list = [F.sum(F.when(stg_payments_df.payment_method == payment_method, stg_payments_df.amount).otherwise(0)).alias(payment_method + '_amount') for payment_method in payment_methods]

    agg_list.append(F.sum(F.col('amount')).alias('total_amount'))

    order_payments_df = (
        stg_payments_df
        .groupby('order_id')
        .agg(*agg_list)
    )

    final_df = (
        stg_orders_df
        .join(order_payments_df, stg_orders_df.order_id == order_payments_df.order_id, 'left')
        .select(stg_orders_df.order_id.alias('order_id'),
                stg_orders_df.customer_id.alias('customer_id'),
                stg_orders_df.order_date.alias('order_date'),
                stg_orders_df.status.alias('status'),
                *[F.col(payment_method + '_amount') for payment_method in payment_methods],
                order_payments_df.total_amount.alias('amount')
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_orders": "jaffle-shop-375704.bruno.stg_orders", "stg_payments": "jaffle-shop-375704.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle-shop-375704'
    schema = 'bruno'
    identifier = 'orders'
    def __repr__(self):
        return 'jaffle-shop-375704.bruno.orders'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "direct").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("jaffle-shop-375704.bruno.orders")

  
[0m04:26:33.413965 [debug] [Thread-1  ]: Execution status: OK in 79.44 seconds
[0m04:26:33.433321 [debug] [Thread-1  ]: finished collecting timing info
[0m04:26:33.435611 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '014dad15-5452-4d65-84e7-8d3556c154bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fe3b970>]}
[0m04:26:33.437096 [info ] [Thread-1  ]: 2 of 14 OK created python table model bruno.orders ............................. [[32mOK[0m in 79.47s]
[0m04:26:33.439289 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.orders
[0m04:26:33.440130 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m04:26:33.441427 [info ] [Thread-1  ]: 3 of 14 START test not_null_customers_customer_id .............................. [RUN]
[0m04:26:33.443922 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m04:26:33.444362 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m04:26:33.444874 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m04:26:33.465586 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m04:26:33.467802 [debug] [Thread-1  ]: finished collecting timing info
[0m04:26:33.468024 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m04:26:33.486707 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m04:26:33.487847 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:26:33.491537 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `jaffle-shop-375704`.`bruno`.`customers`
where customer_id is null



      
    ) dbt_internal_test
[0m04:26:46.145916 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:33b55462-fe46-4f94-a9d3-8f61a0c6d296:US&page=queryresults
[0m04:26:46.152590 [debug] [Thread-1  ]: finished collecting timing info
[0m04:26:46.153998 [info ] [Thread-1  ]: 3 of 14 PASS not_null_customers_customer_id .................................... [[32mPASS[0m in 12.71s]
[0m04:26:46.155252 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m04:26:46.156009 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m04:26:46.156806 [info ] [Thread-1  ]: 4 of 14 START test unique_customers_customer_id ................................ [RUN]
[0m04:26:46.159532 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m04:26:46.160378 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m04:26:46.160745 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m04:26:46.176048 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m04:26:46.177753 [debug] [Thread-1  ]: finished collecting timing info
[0m04:26:46.177985 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m04:26:46.189165 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m04:26:46.189966 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:26:46.191615 [debug] [Thread-1  ]: On test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `jaffle-shop-375704`.`bruno`.`customers`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m04:26:48.523054 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:4b5cdb68-c709-44f9-82ef-f05ad3e8ec3e:US&page=queryresults
[0m04:26:48.526051 [debug] [Thread-1  ]: finished collecting timing info
[0m04:26:48.527861 [info ] [Thread-1  ]: 4 of 14 PASS unique_customers_customer_id ...................................... [[32mPASS[0m in 2.37s]
[0m04:26:48.528912 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m04:26:48.529414 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m04:26:48.530265 [info ] [Thread-1  ]: 5 of 14 START test accepted_values_orders_status__placed__shipped__completed__return_pending__returned  [RUN]
[0m04:26:48.532248 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3"
[0m04:26:48.532984 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m04:26:48.533809 [debug] [Thread-1  ]: Compiling test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m04:26:48.564944 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3"
[0m04:26:48.567516 [debug] [Thread-1  ]: finished collecting timing info
[0m04:26:48.567808 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m04:26:48.571342 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3"
[0m04:26:48.572004 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:26:48.573696 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from `jaffle-shop-375704`.`bruno`.`orders`
    group by status

)

select *
from all_values
where value_field not in (
    'placed','shipped','completed','return_pending','returned'
)



      
    ) dbt_internal_test
[0m04:26:50.774479 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:0b7f780c-e897-4350-97ee-44e758d70868:US&page=queryresults
[0m04:26:50.775967 [debug] [Thread-1  ]: finished collecting timing info
[0m04:26:50.777379 [info ] [Thread-1  ]: 5 of 14 PASS accepted_values_orders_status__placed__shipped__completed__return_pending__returned  [[32mPASS[0m in 2.25s]
[0m04:26:50.778204 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m04:26:50.778596 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m04:26:50.779103 [info ] [Thread-1  ]: 6 of 14 START test not_null_orders_amount ...................................... [RUN]
[0m04:26:50.781132 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_orders_amount.106140f9fd"
[0m04:26:50.781762 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m04:26:50.782228 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m04:26:50.802818 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_orders_amount.106140f9fd"
[0m04:26:50.803791 [debug] [Thread-1  ]: finished collecting timing info
[0m04:26:50.804071 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m04:26:50.808365 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_orders_amount.106140f9fd"
[0m04:26:50.809341 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:26:50.811660 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_amount.106140f9fd: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_orders_amount.106140f9fd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select amount
from `jaffle-shop-375704`.`bruno`.`orders`
where amount is null



      
    ) dbt_internal_test
[0m04:26:53.331972 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:4c0ee12b-b5fa-4182-aa1d-f5b7a973d359:US&page=queryresults
[0m04:26:53.334974 [debug] [Thread-1  ]: finished collecting timing info
[0m04:26:53.337619 [info ] [Thread-1  ]: 6 of 14 PASS not_null_orders_amount ............................................ [[32mPASS[0m in 2.56s]
[0m04:26:53.338943 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m04:26:53.339827 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m04:26:53.340565 [info ] [Thread-1  ]: 7 of 14 START test not_null_orders_bank_transfer_amount ........................ [RUN]
[0m04:26:53.343110 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49"
[0m04:26:53.343698 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m04:26:53.344010 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m04:26:53.365882 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49"
[0m04:26:53.367105 [debug] [Thread-1  ]: finished collecting timing info
[0m04:26:53.367390 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m04:26:53.371524 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49"
[0m04:26:53.372205 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:26:53.374387 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select bank_transfer_amount
from `jaffle-shop-375704`.`bruno`.`orders`
where bank_transfer_amount is null



      
    ) dbt_internal_test
[0m04:26:55.433917 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:38f30e9e-cbfb-4d79-a288-a59a05edb30a:US&page=queryresults
[0m04:26:55.435070 [debug] [Thread-1  ]: finished collecting timing info
[0m04:26:55.436394 [info ] [Thread-1  ]: 7 of 14 PASS not_null_orders_bank_transfer_amount .............................. [[32mPASS[0m in 2.09s]
[0m04:26:55.437480 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m04:26:55.437970 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m04:26:55.438475 [info ] [Thread-1  ]: 8 of 14 START test not_null_orders_coupon_amount ............................... [RUN]
[0m04:26:55.440060 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625"
[0m04:26:55.440691 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m04:26:55.441307 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m04:26:55.454963 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625"
[0m04:26:55.460649 [debug] [Thread-1  ]: finished collecting timing info
[0m04:26:55.461006 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m04:26:55.471410 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625"
[0m04:26:55.472787 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:26:55.475596 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select coupon_amount
from `jaffle-shop-375704`.`bruno`.`orders`
where coupon_amount is null



      
    ) dbt_internal_test
[0m04:26:57.679117 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:c9eefa1e-9cf2-4a66-be1d-926334c4621d:US&page=queryresults
[0m04:26:57.684422 [debug] [Thread-1  ]: finished collecting timing info
[0m04:26:57.687603 [info ] [Thread-1  ]: 8 of 14 PASS not_null_orders_coupon_amount ..................................... [[32mPASS[0m in 2.25s]
[0m04:26:57.691433 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m04:26:57.693306 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m04:26:57.694752 [info ] [Thread-1  ]: 9 of 14 START test not_null_orders_credit_card_amount .......................... [RUN]
[0m04:26:57.697202 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59"
[0m04:26:57.698151 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m04:26:57.699075 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m04:26:57.718434 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59"
[0m04:26:57.719512 [debug] [Thread-1  ]: finished collecting timing info
[0m04:26:57.719800 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m04:26:57.724470 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59"
[0m04:26:57.725620 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:26:57.729342 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select credit_card_amount
from `jaffle-shop-375704`.`bruno`.`orders`
where credit_card_amount is null



      
    ) dbt_internal_test
[0m04:26:59.990916 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:bb627e55-a627-4f81-b3de-356e17503767:US&page=queryresults
[0m04:26:59.992755 [debug] [Thread-1  ]: finished collecting timing info
[0m04:26:59.994299 [info ] [Thread-1  ]: 9 of 14 PASS not_null_orders_credit_card_amount ................................ [[32mPASS[0m in 2.30s]
[0m04:26:59.995126 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m04:26:59.995535 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m04:26:59.995859 [info ] [Thread-1  ]: 10 of 14 START test not_null_orders_customer_id ................................ [RUN]
[0m04:26:59.997311 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_orders_customer_id.c5f02694af"
[0m04:26:59.997869 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m04:26:59.998235 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m04:27:00.015074 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_orders_customer_id.c5f02694af"
[0m04:27:00.016135 [debug] [Thread-1  ]: finished collecting timing info
[0m04:27:00.016476 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m04:27:00.021459 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_orders_customer_id.c5f02694af"
[0m04:27:00.022497 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:27:00.025040 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_customer_id.c5f02694af: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_orders_customer_id.c5f02694af"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `jaffle-shop-375704`.`bruno`.`orders`
where customer_id is null



      
    ) dbt_internal_test
[0m04:27:01.964835 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:01354062-1cf3-4ebd-a251-106594aad37c:US&page=queryresults
[0m04:27:01.967070 [debug] [Thread-1  ]: finished collecting timing info
[0m04:27:01.968762 [info ] [Thread-1  ]: 10 of 14 PASS not_null_orders_customer_id ...................................... [[32mPASS[0m in 1.97s]
[0m04:27:01.969654 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m04:27:01.970132 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m04:27:01.970751 [info ] [Thread-1  ]: 11 of 14 START test not_null_orders_gift_card_amount ........................... [RUN]
[0m04:27:01.972662 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a"
[0m04:27:01.973223 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m04:27:01.973522 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m04:27:01.988277 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a"
[0m04:27:01.989344 [debug] [Thread-1  ]: finished collecting timing info
[0m04:27:01.989700 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m04:27:01.991779 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a"
[0m04:27:01.992255 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:27:01.993210 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select gift_card_amount
from `jaffle-shop-375704`.`bruno`.`orders`
where gift_card_amount is null



      
    ) dbt_internal_test
[0m04:27:03.889596 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:0a96ea69-3cac-48fe-9002-81ee3be32a89:US&page=queryresults
[0m04:27:03.891858 [debug] [Thread-1  ]: finished collecting timing info
[0m04:27:03.894244 [info ] [Thread-1  ]: 11 of 14 PASS not_null_orders_gift_card_amount ................................. [[32mPASS[0m in 1.92s]
[0m04:27:03.895655 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m04:27:03.896105 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m04:27:03.896617 [info ] [Thread-1  ]: 12 of 14 START test not_null_orders_order_id ................................... [RUN]
[0m04:27:03.898134 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_orders_order_id.cf6c17daed"
[0m04:27:03.898781 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m04:27:03.899193 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m04:27:03.923541 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_orders_order_id.cf6c17daed"
[0m04:27:03.924060 [debug] [Thread-1  ]: finished collecting timing info
[0m04:27:03.924199 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m04:27:03.930440 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_orders_order_id.cf6c17daed"
[0m04:27:03.930927 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:27:03.932061 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_order_id.cf6c17daed: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_orders_order_id.cf6c17daed"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from `jaffle-shop-375704`.`bruno`.`orders`
where order_id is null



      
    ) dbt_internal_test
[0m04:27:06.006319 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:341e293a-2b2c-4d0a-a4a0-001da677fec7:US&page=queryresults
[0m04:27:06.008288 [debug] [Thread-1  ]: finished collecting timing info
[0m04:27:06.010826 [info ] [Thread-1  ]: 12 of 14 PASS not_null_orders_order_id ......................................... [[32mPASS[0m in 2.11s]
[0m04:27:06.012027 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m04:27:06.012541 [debug] [Thread-1  ]: Began running node test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m04:27:06.013177 [info ] [Thread-1  ]: 13 of 14 START test relationships_orders_customer_id__customer_id__ref_customers_  [RUN]
[0m04:27:06.014892 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2"
[0m04:27:06.015478 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m04:27:06.015900 [debug] [Thread-1  ]: Compiling test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m04:27:06.041308 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2"
[0m04:27:06.043065 [debug] [Thread-1  ]: finished collecting timing info
[0m04:27:06.043428 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m04:27:06.047776 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2"
[0m04:27:06.048705 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:27:06.050767 [debug] [Thread-1  ]: On test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select customer_id as from_field
    from `jaffle-shop-375704`.`bruno`.`orders`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `jaffle-shop-375704`.`bruno`.`customers`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m04:27:08.294014 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:6aef46ac-d28c-4454-9990-9ee1a0e58937:US&page=queryresults
[0m04:27:08.295161 [debug] [Thread-1  ]: finished collecting timing info
[0m04:27:08.296423 [info ] [Thread-1  ]: 13 of 14 PASS relationships_orders_customer_id__customer_id__ref_customers_ .... [[32mPASS[0m in 2.28s]
[0m04:27:08.297308 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m04:27:08.297717 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m04:27:08.298280 [info ] [Thread-1  ]: 14 of 14 START test unique_orders_order_id ..................................... [RUN]
[0m04:27:08.299640 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.unique_orders_order_id.fed79b3a6e"
[0m04:27:08.300096 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m04:27:08.300396 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m04:27:08.323677 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_orders_order_id.fed79b3a6e"
[0m04:27:08.325505 [debug] [Thread-1  ]: finished collecting timing info
[0m04:27:08.325835 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m04:27:08.330331 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_orders_order_id.fed79b3a6e"
[0m04:27:08.331342 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:27:08.333531 [debug] [Thread-1  ]: On test.jaffle_shop.unique_orders_order_id.fed79b3a6e: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_orders_order_id.fed79b3a6e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `jaffle-shop-375704`.`bruno`.`orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m04:27:11.049730 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:ce189490-813c-412b-809a-9e9af165752d:US&page=queryresults
[0m04:27:11.052479 [debug] [Thread-1  ]: finished collecting timing info
[0m04:27:11.054572 [info ] [Thread-1  ]: 14 of 14 PASS unique_orders_order_id ........................................... [[32mPASS[0m in 2.76s]
[0m04:27:11.056388 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m04:27:11.060793 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m04:27:11.062368 [info ] [MainThread]: 
[0m04:27:11.062971 [info ] [MainThread]: Finished running 2 table models, 12 tests in 0 hours 3 minutes and 38.43 seconds (218.43s).
[0m04:27:11.063479 [debug] [MainThread]: Connection 'master' was properly closed.
[0m04:27:11.063762 [debug] [MainThread]: Connection 'test.jaffle_shop.unique_orders_order_id.fed79b3a6e' was properly closed.
[0m04:27:11.093991 [info ] [MainThread]: 
[0m04:27:11.094540 [info ] [MainThread]: [32mCompleted successfully[0m
[0m04:27:11.095023 [info ] [MainThread]: 
[0m04:27:11.095401 [info ] [MainThread]: Done. PASS=14 WARN=0 ERROR=0 SKIP=0 TOTAL=14
[0m04:27:11.096092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fc76e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fe4f9a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ff03eb0>]}
[0m04:27:11.096544 [debug] [MainThread]: Flushing usage events


============================== 2023-01-25 04:27:55.860499 | 20ca0db1-5cb8-4318-ba1e-8497217425b2 ==============================
[0m04:27:55.860583 [info ] [MainThread]: Running with dbt=1.3.2
[0m04:27:55.861508 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bruno/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'indirect_selection': 'eager', 'resource_types': [], 'which': 'build', 'rpc_method': 'build'}
[0m04:27:55.861670 [debug] [MainThread]: Tracking: tracking
[0m04:27:55.885593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e1f7b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e1e8e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e1e8e80>]}
[0m04:27:55.938636 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m04:27:55.938803 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m04:27:55.944803 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '20ca0db1-5cb8-4318-ba1e-8497217425b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e3ccd60>]}
[0m04:27:55.953107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '20ca0db1-5cb8-4318-ba1e-8497217425b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e2f5cd0>]}
[0m04:27:55.953325 [info ] [MainThread]: Found 5 models, 20 tests, 0 snapshots, 0 analyses, 319 macros, 0 operations, 3 seed files, 0 sources, 0 exposures, 0 metrics
[0m04:27:55.953517 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '20ca0db1-5cb8-4318-ba1e-8497217425b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e218f70>]}
[0m04:27:55.955261 [info ] [MainThread]: 
[0m04:27:55.955747 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m04:27:55.956764 [debug] [ThreadPool]: Acquiring new bigquery connection "list_jaffle-shop-375704"
[0m04:27:55.956987 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:27:56.720517 [debug] [ThreadPool]: Acquiring new bigquery connection "list_jaffle-shop-375704_bruno"
[0m04:27:56.720760 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m04:27:57.439110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '20ca0db1-5cb8-4318-ba1e-8497217425b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e211190>]}
[0m04:27:57.440686 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m04:27:57.441236 [info ] [MainThread]: 
[0m04:27:57.448882 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_customers
[0m04:27:57.449440 [info ] [Thread-1  ]: 1 of 28 START seed file bruno.raw_customers .................................... [RUN]
[0m04:27:57.450545 [debug] [Thread-1  ]: Acquiring new bigquery connection "seed.jaffle_shop.raw_customers"
[0m04:27:57.450795 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_customers
[0m04:27:57.451074 [debug] [Thread-1  ]: finished collecting timing info
[0m04:27:57.451298 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_customers
[0m04:27:57.493806 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:28:03.823103 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_customers"
[0m04:28:03.851076 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:03.851633 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '20ca0db1-5cb8-4318-ba1e-8497217425b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e45ce20>]}
[0m04:28:03.851898 [info ] [Thread-1  ]: 1 of 28 OK loaded seed file bruno.raw_customers ................................ [[32mINSERT 100[0m in 6.40s]
[0m04:28:03.852340 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_customers
[0m04:28:03.852514 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_orders
[0m04:28:03.852839 [info ] [Thread-1  ]: 2 of 28 START seed file bruno.raw_orders ....................................... [RUN]
[0m04:28:03.853587 [debug] [Thread-1  ]: Acquiring new bigquery connection "seed.jaffle_shop.raw_orders"
[0m04:28:03.853747 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_orders
[0m04:28:03.853887 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:03.854019 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_orders
[0m04:28:03.862007 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:28:10.958984 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_orders"
[0m04:28:10.966588 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:10.967787 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '20ca0db1-5cb8-4318-ba1e-8497217425b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e4c96d0>]}
[0m04:28:10.968335 [info ] [Thread-1  ]: 2 of 28 OK loaded seed file bruno.raw_orders ................................... [[32mINSERT 99[0m in 7.11s]
[0m04:28:10.969092 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_orders
[0m04:28:10.969433 [debug] [Thread-1  ]: Began running node seed.jaffle_shop.raw_payments
[0m04:28:10.970076 [info ] [Thread-1  ]: 3 of 28 START seed file bruno.raw_payments ..................................... [RUN]
[0m04:28:10.971441 [debug] [Thread-1  ]: Acquiring new bigquery connection "seed.jaffle_shop.raw_payments"
[0m04:28:10.971726 [debug] [Thread-1  ]: Began compiling node seed.jaffle_shop.raw_payments
[0m04:28:10.971991 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:10.972240 [debug] [Thread-1  ]: Began executing node seed.jaffle_shop.raw_payments
[0m04:28:10.991999 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:28:17.921960 [debug] [Thread-1  ]: Writing runtime SQL for node "seed.jaffle_shop.raw_payments"
[0m04:28:17.929144 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:17.930309 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '20ca0db1-5cb8-4318-ba1e-8497217425b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e4c97c0>]}
[0m04:28:17.930766 [info ] [Thread-1  ]: 3 of 28 OK loaded seed file bruno.raw_payments ................................. [[32mINSERT 113[0m in 6.96s]
[0m04:28:17.931417 [debug] [Thread-1  ]: Finished running node seed.jaffle_shop.raw_payments
[0m04:28:17.931707 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m04:28:17.932070 [info ] [Thread-1  ]: 4 of 28 START sql view model bruno.stg_customers ............................... [RUN]
[0m04:28:17.933767 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.jaffle_shop.stg_customers"
[0m04:28:17.934217 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m04:28:17.934548 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m04:28:17.940058 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m04:28:17.941229 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:17.941490 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m04:28:17.977613 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_customers"
[0m04:28:17.978088 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:28:17.979244 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */


  create or replace view `jaffle-shop-375704`.`bruno`.`stg_customers`
  OPTIONS()
  as with source as (
    select * from `jaffle-shop-375704`.`bruno`.`raw_customers`

),

renamed as (

    select
        id as customer_id,
        first_name,
        last_name

    from source

)

select * from renamed;


[0m04:28:19.358103 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:73a4c1c6-52e4-40c1-a9d8-59e794f39934:US&page=queryresults
[0m04:28:19.370470 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:19.371823 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '20ca0db1-5cb8-4318-ba1e-8497217425b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e58ca60>]}
[0m04:28:19.372557 [info ] [Thread-1  ]: 4 of 28 OK created sql view model bruno.stg_customers .......................... [[32mCREATE VIEW (0 processed)[0m in 1.44s]
[0m04:28:19.373390 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m04:28:19.373836 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m04:28:19.374532 [info ] [Thread-1  ]: 5 of 28 START sql view model bruno.stg_orders .................................. [RUN]
[0m04:28:19.375995 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.jaffle_shop.stg_orders"
[0m04:28:19.376331 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m04:28:19.376595 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m04:28:19.386016 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m04:28:19.386939 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:19.387178 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m04:28:19.402269 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_orders"
[0m04:28:19.403008 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:28:19.404518 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */


  create or replace view `jaffle-shop-375704`.`bruno`.`stg_orders`
  OPTIONS()
  as with source as (
    select * from `jaffle-shop-375704`.`bruno`.`raw_orders`

),

renamed as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from source

)

select * from renamed;


[0m04:28:20.876500 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:03801a96-b959-4650-b925-ede7bd42a5ed:US&page=queryresults
[0m04:28:20.882354 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:20.883806 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '20ca0db1-5cb8-4318-ba1e-8497217425b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e4c9850>]}
[0m04:28:20.884505 [info ] [Thread-1  ]: 5 of 28 OK created sql view model bruno.stg_orders ............................. [[32mCREATE VIEW (0 processed)[0m in 1.51s]
[0m04:28:20.885303 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m04:28:20.885660 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m04:28:20.886278 [info ] [Thread-1  ]: 6 of 28 START sql view model bruno.stg_payments ................................ [RUN]
[0m04:28:20.887771 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.jaffle_shop.stg_payments"
[0m04:28:20.888067 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m04:28:20.888334 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m04:28:20.894153 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m04:28:20.895067 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:20.895340 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m04:28:20.900706 [debug] [Thread-1  ]: Writing runtime sql for node "model.jaffle_shop.stg_payments"
[0m04:28:20.901384 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:28:20.903311 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */


  create or replace view `jaffle-shop-375704`.`bruno`.`stg_payments`
  OPTIONS()
  as with source as (
    select * from `jaffle-shop-375704`.`bruno`.`raw_payments`

),

renamed as (

    select
        id as payment_id,
        order_id,
        payment_method,

        -- `amount` is currently stored in cents, so we convert it to dollars
        amount / 100 as amount

    from source

)

select * from renamed;


[0m04:28:22.531627 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:aacb4ab6-e5aa-4585-8d3a-9c8578e095a8:US&page=queryresults
[0m04:28:22.536772 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:22.538139 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '20ca0db1-5cb8-4318-ba1e-8497217425b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e4c97f0>]}
[0m04:28:22.538841 [info ] [Thread-1  ]: 6 of 28 OK created sql view model bruno.stg_payments ........................... [[32mCREATE VIEW (0 processed)[0m in 1.65s]
[0m04:28:22.539675 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m04:28:22.540041 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m04:28:22.540581 [info ] [Thread-1  ]: 7 of 28 START test not_null_stg_customers_customer_id .......................... [RUN]
[0m04:28:22.542018 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m04:28:22.542359 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m04:28:22.542728 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m04:28:22.566156 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m04:28:22.566813 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:22.567012 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m04:28:22.585202 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m04:28:22.585782 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:28:22.586956 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `jaffle-shop-375704`.`bruno`.`stg_customers`
where customer_id is null



      
    ) dbt_internal_test
[0m04:28:24.880085 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:45d6fcc1-b942-442f-91d0-b73f1bc4315d:US&page=queryresults
[0m04:28:24.882071 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:24.883020 [info ] [Thread-1  ]: 7 of 28 PASS not_null_stg_customers_customer_id ................................ [[32mPASS[0m in 2.34s]
[0m04:28:24.883668 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m04:28:24.883972 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m04:28:24.884356 [info ] [Thread-1  ]: 8 of 28 START test unique_stg_customers_customer_id ............................ [RUN]
[0m04:28:24.885410 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m04:28:24.885659 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m04:28:24.885851 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m04:28:24.901847 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m04:28:24.902487 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:24.902682 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m04:28:24.911982 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m04:28:24.912487 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:28:24.913803 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `jaffle-shop-375704`.`bruno`.`stg_customers`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m04:28:27.442876 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:6fafad31-c7aa-4374-91e6-9260caec7163:US&page=queryresults
[0m04:28:27.445341 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:27.448009 [info ] [Thread-1  ]: 8 of 28 PASS unique_stg_customers_customer_id .................................. [[32mPASS[0m in 2.56s]
[0m04:28:27.449846 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m04:28:27.450539 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m04:28:27.451146 [info ] [Thread-1  ]: 9 of 28 START test accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned  [RUN]
[0m04:28:27.452558 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m04:28:27.452939 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m04:28:27.453228 [debug] [Thread-1  ]: Compiling test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m04:28:27.478269 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m04:28:27.478964 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:27.479185 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m04:28:27.482478 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"
[0m04:28:27.483209 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:28:27.484734 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from `jaffle-shop-375704`.`bruno`.`stg_orders`
    group by status

)

select *
from all_values
where value_field not in (
    'placed','shipped','completed','return_pending','returned'
)



      
    ) dbt_internal_test
[0m04:28:30.001178 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:1d848282-7cdd-4e23-83bb-3b12a9ef3de3:US&page=queryresults
[0m04:28:30.002532 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:30.003968 [info ] [Thread-1  ]: 9 of 28 PASS accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned  [[32mPASS[0m in 2.55s]
[0m04:28:30.004948 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_stg_orders_status__placed__shipped__completed__return_pending__returned.080fb20aad
[0m04:28:30.005308 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m04:28:30.005735 [info ] [Thread-1  ]: 10 of 28 START test not_null_stg_orders_order_id ............................... [RUN]
[0m04:28:30.006984 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m04:28:30.007286 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m04:28:30.007562 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m04:28:30.023797 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m04:28:30.024610 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:30.024832 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m04:28:30.028192 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m04:28:30.028704 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:28:30.030379 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from `jaffle-shop-375704`.`bruno`.`stg_orders`
where order_id is null



      
    ) dbt_internal_test
[0m04:28:32.154312 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:a3fb46d2-dcfe-4152-98bc-64c49c7532a2:US&page=queryresults
[0m04:28:32.156670 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:32.158077 [info ] [Thread-1  ]: 10 of 28 PASS not_null_stg_orders_order_id ..................................... [[32mPASS[0m in 2.15s]
[0m04:28:32.158937 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m04:28:32.159300 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m04:28:32.159762 [info ] [Thread-1  ]: 11 of 28 START test unique_stg_orders_order_id ................................. [RUN]
[0m04:28:32.161047 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m04:28:32.161355 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m04:28:32.161626 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m04:28:32.170738 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m04:28:32.171646 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:32.171896 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m04:28:32.175961 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m04:28:32.176640 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:28:32.178484 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `jaffle-shop-375704`.`bruno`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m04:28:34.329541 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:ced218fb-d8cc-4f77-932d-744c6905fb2a:US&page=queryresults
[0m04:28:34.330504 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:34.331598 [info ] [Thread-1  ]: 11 of 28 PASS unique_stg_orders_order_id ....................................... [[32mPASS[0m in 2.17s]
[0m04:28:34.332367 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m04:28:34.332739 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m04:28:34.333198 [info ] [Thread-1  ]: 12 of 28 START test accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card  [RUN]
[0m04:28:34.334476 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m04:28:34.334770 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m04:28:34.335035 [debug] [Thread-1  ]: Compiling test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m04:28:34.347999 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m04:28:34.349636 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:34.349936 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m04:28:34.367012 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"
[0m04:28:34.367921 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:28:34.369419 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        payment_method as value_field,
        count(*) as n_records

    from `jaffle-shop-375704`.`bruno`.`stg_payments`
    group by payment_method

)

select *
from all_values
where value_field not in (
    'credit_card','coupon','bank_transfer','gift_card'
)



      
    ) dbt_internal_test
[0m04:28:36.657230 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:345d0a52-4a4c-4703-94d3-9c77d066e6ed:US&page=queryresults
[0m04:28:36.658367 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:36.659533 [info ] [Thread-1  ]: 12 of 28 PASS accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card  [[32mPASS[0m in 2.33s]
[0m04:28:36.660482 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_stg_payments_payment_method__credit_card__coupon__bank_transfer__gift_card.3c3820f278
[0m04:28:36.660990 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m04:28:36.661454 [info ] [Thread-1  ]: 13 of 28 START test not_null_stg_payments_payment_id ........................... [RUN]
[0m04:28:36.662743 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m04:28:36.663176 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m04:28:36.663461 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m04:28:36.678526 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m04:28:36.680375 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:36.680670 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m04:28:36.687831 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"
[0m04:28:36.688842 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:28:36.690593 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select payment_id
from `jaffle-shop-375704`.`bruno`.`stg_payments`
where payment_id is null



      
    ) dbt_internal_test
[0m04:28:39.013956 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:ce6424f3-2831-4128-b216-77693e569a6e:US&page=queryresults
[0m04:28:39.016077 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:39.017918 [info ] [Thread-1  ]: 13 of 28 PASS not_null_stg_payments_payment_id ................................. [[32mPASS[0m in 2.36s]
[0m04:28:39.018863 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_payments_payment_id.c19cc50075
[0m04:28:39.019254 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m04:28:39.019755 [info ] [Thread-1  ]: 14 of 28 START test unique_stg_payments_payment_id ............................. [RUN]
[0m04:28:39.021742 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m04:28:39.022278 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m04:28:39.022569 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m04:28:39.034683 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m04:28:39.035570 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:39.035789 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m04:28:39.042771 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"
[0m04:28:39.043550 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:28:39.045368 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_payments_payment_id.3744510712: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_payments_payment_id.3744510712"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select payment_id as unique_field
  from `jaffle-shop-375704`.`bruno`.`stg_payments`
  where payment_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m04:28:41.473916 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:9c29905f-f4d0-444e-ad3a-a7e77051d21e:US&page=queryresults
[0m04:28:41.476036 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:41.477756 [info ] [Thread-1  ]: 14 of 28 PASS unique_stg_payments_payment_id ................................... [[32mPASS[0m in 2.46s]
[0m04:28:41.478690 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_payments_payment_id.3744510712
[0m04:28:41.480679 [debug] [Thread-1  ]: Began running node model.jaffle_shop.customers
[0m04:28:41.481401 [info ] [Thread-1  ]: 15 of 28 START python table model bruno.customers .............................. [RUN]
[0m04:28:41.482914 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.jaffle_shop.customers"
[0m04:28:41.483265 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.customers
[0m04:28:41.483554 [debug] [Thread-1  ]: Compiling model.jaffle_shop.customers
[0m04:28:41.522045 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.customers"
[0m04:28:41.522686 [debug] [Thread-1  ]: finished collecting timing info
[0m04:28:41.522862 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.customers
[0m04:28:41.536327 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:28:42.315522 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.customers"
[0m04:28:42.318354 [debug] [Thread-1  ]: On model.jaffle_shop.customers: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","dbt_python_bigquery_bucket")

import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method="cluster",
        dataproc_cluster_name="dbt-python"
    )

    stg_customers_df = dbt.ref('stg_customers')
    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    customer_orders_df = (
        stg_orders_df
        .groupby('customer_id')
        .agg(
            F.min(F.col('order_date')).alias('first_order'),
            F.max(F.col('order_date')).alias('most_recent_order'),
            F.count(F.col('order_id')).alias('number_of_orders')
        )
    )

    customer_payments_df = (
        stg_payments_df
        .join(stg_orders_df, stg_payments_df.order_id == stg_orders_df.order_id, 'left')
        .groupby(stg_orders_df.customer_id)
        .agg(
            F.sum(F.col('amount')).alias('total_amount')
        )
    )

    final_df = (
        stg_customers_df.alias('customers') \
            .join(customer_orders_df.alias('customer_orders'), F.col('customers.customer_id') == F.col('customer_orders.customer_id'), 'left') \
            .join(customer_payments_df.alias('customer_payments'), F.col('customers.customer_id') == F.col('customer_payments.customer_id'), 'left') \
            .select(F.col('customers.customer_id').alias('customer_id'),
                    F.col('customers.first_name').alias('first_name'),
                    F.col('customers.last_name').alias('last_name'),
                    F.col('customer_orders.first_order').alias('first_order'),
                    F.col('customer_orders.most_recent_order').alias('most_recent_order'),
                    F.col('customer_orders.number_of_orders').alias('number_of_orders'),
                    F.col('customer_payments.total_amount').alias('customer_lifetime_value')
            )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_customers": "jaffle-shop-375704.bruno.stg_customers", "stg_orders": "jaffle-shop-375704.bruno.stg_orders", "stg_payments": "jaffle-shop-375704.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle-shop-375704'
    schema = 'bruno'
    identifier = 'customers'
    def __repr__(self):
        return 'jaffle-shop-375704.bruno.customers'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "direct").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("jaffle-shop-375704.bruno.customers")

  
[0m04:30:12.040228 [debug] [Thread-1  ]: Execution status: OK in 89.72 seconds
[0m04:30:12.047231 [debug] [Thread-1  ]: finished collecting timing info
[0m04:30:12.048615 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '20ca0db1-5cb8-4318-ba1e-8497217425b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e5fd0a0>]}
[0m04:30:12.049321 [info ] [Thread-1  ]: 15 of 28 OK created python table model bruno.customers ......................... [[32mOK[0m in 90.57s]
[0m04:30:12.050186 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.customers
[0m04:30:12.050542 [debug] [Thread-1  ]: Began running node model.jaffle_shop.orders
[0m04:30:12.051166 [info ] [Thread-1  ]: 16 of 28 START python table model bruno.orders ................................. [RUN]
[0m04:30:12.052596 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.jaffle_shop.orders"
[0m04:30:12.052894 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.orders
[0m04:30:12.053156 [debug] [Thread-1  ]: Compiling model.jaffle_shop.orders
[0m04:30:12.060578 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.orders"
[0m04:30:12.061799 [debug] [Thread-1  ]: finished collecting timing info
[0m04:30:12.062076 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.orders
[0m04:30:12.065738 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:30:12.940093 [debug] [Thread-1  ]: Writing runtime python for node "model.jaffle_shop.orders"
[0m04:30:12.943574 [debug] [Thread-1  ]: On model.jaffle_shop.orders: 
  
    
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('smallTest').getOrCreate()

spark.conf.set("viewsEnabled","true")
spark.conf.set("temporaryGcsBucket","dbt_python_bigquery_bucket")

import pyspark.sql.functions as F

def model(dbt, session):

    dbt.config(
        submission_method="cluster",
        dataproc_cluster_name="dbt-python"
    )

    stg_orders_df = dbt.ref('stg_orders')
    stg_payments_df = dbt.ref('stg_payments')

    payment_methods = ['credit_card', 'coupon', 'bank_transfer', 'gift_card']

    agg_list = [F.sum(F.when(stg_payments_df.payment_method == payment_method, stg_payments_df.amount).otherwise(0)).alias(payment_method + '_amount') for payment_method in payment_methods]

    agg_list.append(F.sum(F.col('amount')).alias('total_amount'))

    order_payments_df = (
        stg_payments_df
        .groupby('order_id')
        .agg(*agg_list)
    )

    final_df = (
        stg_orders_df
        .join(order_payments_df, stg_orders_df.order_id == order_payments_df.order_id, 'left')
        .select(stg_orders_df.order_id.alias('order_id'),
                stg_orders_df.customer_id.alias('customer_id'),
                stg_orders_df.order_date.alias('order_date'),
                stg_orders_df.status.alias('status'),
                *[F.col(payment_method + '_amount') for payment_method in payment_methods],
                order_payments_df.total_amount.alias('amount')
        )
    )

    return final_df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"stg_orders": "jaffle-shop-375704.bruno.stg_orders", "stg_payments": "jaffle-shop-375704.bruno.stg_payments"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'jaffle-shop-375704'
    schema = 'bruno'
    identifier = 'orders'
    def __repr__(self):
        return 'jaffle-shop-375704.bruno.orders'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------



dbt = dbtObj(spark.read.format("bigquery").load)
df = model(dbt, spark)

# COMMAND ----------
# this is materialization code dbt generated, please do not modify

import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write \
  .mode("overwrite") \
  .format("bigquery") \
  .option("writeMethod", "direct").option("writeDisposition", 'WRITE_TRUNCATE') \
  .save("jaffle-shop-375704.bruno.orders")

  
[0m04:31:21.852782 [debug] [Thread-1  ]: Execution status: OK in 68.91 seconds
[0m04:31:21.860445 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:21.861859 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '20ca0db1-5cb8-4318-ba1e-8497217425b2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e588f70>]}
[0m04:31:21.862572 [info ] [Thread-1  ]: 16 of 28 OK created python table model bruno.orders ............................ [[32mOK[0m in 69.81s]
[0m04:31:21.863492 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.orders
[0m04:31:21.863867 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m04:31:21.864401 [info ] [Thread-1  ]: 17 of 28 START test not_null_customers_customer_id ............................. [RUN]
[0m04:31:21.866107 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m04:31:21.866441 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m04:31:21.866772 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m04:31:21.875738 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m04:31:21.877660 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:21.877923 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m04:31:21.881844 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"
[0m04:31:21.882408 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:31:21.884135 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `jaffle-shop-375704`.`bruno`.`customers`
where customer_id is null



      
    ) dbt_internal_test
[0m04:31:24.174382 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:f0adb61f-843b-47be-8701-119c74102aca:US&page=queryresults
[0m04:31:24.175046 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:24.175799 [info ] [Thread-1  ]: 17 of 28 PASS not_null_customers_customer_id ................................... [[32mPASS[0m in 2.31s]
[0m04:31:24.176327 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_customers_customer_id.5c9bf9911d
[0m04:31:24.176572 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m04:31:24.176868 [info ] [Thread-1  ]: 18 of 28 START test unique_customers_customer_id ............................... [RUN]
[0m04:31:24.177751 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m04:31:24.177967 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m04:31:24.178160 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m04:31:24.188242 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m04:31:24.188862 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:24.189053 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m04:31:24.194307 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"
[0m04:31:24.194763 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:31:24.196090 [debug] [Thread-1  ]: On test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `jaffle-shop-375704`.`bruno`.`customers`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m04:31:26.442638 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:fc776947-c85c-4903-8fc9-56bfadc5a1cc:US&page=queryresults
[0m04:31:26.443962 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:26.445222 [info ] [Thread-1  ]: 18 of 28 PASS unique_customers_customer_id ..................................... [[32mPASS[0m in 2.27s]
[0m04:31:26.446032 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_customers_customer_id.c5af1ff4b1
[0m04:31:26.446406 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m04:31:26.446711 [info ] [Thread-1  ]: 19 of 28 START test accepted_values_orders_status__placed__shipped__completed__return_pending__returned  [RUN]
[0m04:31:26.448230 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3"
[0m04:31:26.448924 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m04:31:26.449405 [debug] [Thread-1  ]: Compiling test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m04:31:26.489460 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3"
[0m04:31:26.490258 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:26.490451 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m04:31:26.493331 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3"
[0m04:31:26.493769 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:31:26.495115 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from `jaffle-shop-375704`.`bruno`.`orders`
    group by status

)

select *
from all_values
where value_field not in (
    'placed','shipped','completed','return_pending','returned'
)



      
    ) dbt_internal_test
[0m04:31:28.799220 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:3c6c1cdf-b3f9-41ba-b303-8e9d1cebef8f:US&page=queryresults
[0m04:31:28.802103 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:28.804704 [info ] [Thread-1  ]: 19 of 28 PASS accepted_values_orders_status__placed__shipped__completed__return_pending__returned  [[32mPASS[0m in 2.36s]
[0m04:31:28.805824 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_orders_status__placed__shipped__completed__return_pending__returned.be6b5b5ec3
[0m04:31:28.806223 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m04:31:28.806671 [info ] [Thread-1  ]: 20 of 28 START test not_null_orders_amount ..................................... [RUN]
[0m04:31:28.808006 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_orders_amount.106140f9fd"
[0m04:31:28.808322 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m04:31:28.808596 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m04:31:28.824567 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_orders_amount.106140f9fd"
[0m04:31:28.825452 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:28.825699 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m04:31:28.828963 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_orders_amount.106140f9fd"
[0m04:31:28.829504 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:31:28.831039 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_amount.106140f9fd: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_orders_amount.106140f9fd"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select amount
from `jaffle-shop-375704`.`bruno`.`orders`
where amount is null



      
    ) dbt_internal_test
[0m04:31:31.311665 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:02fb8549-f024-4e56-8954-275121793e50:US&page=queryresults
[0m04:31:31.313751 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:31.315187 [info ] [Thread-1  ]: 20 of 28 PASS not_null_orders_amount ........................................... [[32mPASS[0m in 2.51s]
[0m04:31:31.316122 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_amount.106140f9fd
[0m04:31:31.316763 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m04:31:31.317706 [info ] [Thread-1  ]: 21 of 28 START test not_null_orders_bank_transfer_amount ....................... [RUN]
[0m04:31:31.319356 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49"
[0m04:31:31.319673 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m04:31:31.319960 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m04:31:31.337452 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49"
[0m04:31:31.338314 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:31.338576 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m04:31:31.342270 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49"
[0m04:31:31.343082 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:31:31.344959 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select bank_transfer_amount
from `jaffle-shop-375704`.`bruno`.`orders`
where bank_transfer_amount is null



      
    ) dbt_internal_test
[0m04:31:33.407328 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:c9dc9c2b-c123-4895-aabf-ed551f604a7b:US&page=queryresults
[0m04:31:33.410742 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:33.412966 [info ] [Thread-1  ]: 21 of 28 PASS not_null_orders_bank_transfer_amount ............................. [[32mPASS[0m in 2.09s]
[0m04:31:33.414175 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_bank_transfer_amount.7743500c49
[0m04:31:33.414642 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m04:31:33.415192 [info ] [Thread-1  ]: 22 of 28 START test not_null_orders_coupon_amount .............................. [RUN]
[0m04:31:33.417128 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625"
[0m04:31:33.417811 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m04:31:33.418126 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m04:31:33.436121 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625"
[0m04:31:33.437184 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:33.437499 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m04:31:33.441375 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625"
[0m04:31:33.442015 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:31:33.443688 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select coupon_amount
from `jaffle-shop-375704`.`bruno`.`orders`
where coupon_amount is null



      
    ) dbt_internal_test
[0m04:31:35.769163 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:e29f7847-1f0d-4d88-91ac-efaafe5d59de:US&page=queryresults
[0m04:31:35.771909 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:35.774402 [info ] [Thread-1  ]: 22 of 28 PASS not_null_orders_coupon_amount .................................... [[32mPASS[0m in 2.36s]
[0m04:31:35.775497 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_coupon_amount.ab90c90625
[0m04:31:35.775899 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m04:31:35.776334 [info ] [Thread-1  ]: 23 of 28 START test not_null_orders_credit_card_amount ......................... [RUN]
[0m04:31:35.777619 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59"
[0m04:31:35.777923 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m04:31:35.778200 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m04:31:35.787568 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59"
[0m04:31:35.788512 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:35.788804 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m04:31:35.792988 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59"
[0m04:31:35.793717 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:31:35.795795 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select credit_card_amount
from `jaffle-shop-375704`.`bruno`.`orders`
where credit_card_amount is null



      
    ) dbt_internal_test
[0m04:31:38.119027 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:cb763934-619c-4c7b-9b73-9acbce6804b4:US&page=queryresults
[0m04:31:38.121858 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:38.123588 [info ] [Thread-1  ]: 23 of 28 PASS not_null_orders_credit_card_amount ............................... [[32mPASS[0m in 2.35s]
[0m04:31:38.124481 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_credit_card_amount.d3ca593b59
[0m04:31:38.124873 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m04:31:38.125340 [info ] [Thread-1  ]: 24 of 28 START test not_null_orders_customer_id ................................ [RUN]
[0m04:31:38.126631 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_orders_customer_id.c5f02694af"
[0m04:31:38.126942 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m04:31:38.127217 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m04:31:38.136829 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_orders_customer_id.c5f02694af"
[0m04:31:38.137700 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:38.138038 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m04:31:38.142480 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_orders_customer_id.c5f02694af"
[0m04:31:38.143289 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:31:38.145355 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_customer_id.c5f02694af: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_orders_customer_id.c5f02694af"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `jaffle-shop-375704`.`bruno`.`orders`
where customer_id is null



      
    ) dbt_internal_test
[0m04:31:40.472755 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:1aa33276-c668-4513-b43f-8cf474c4a060:US&page=queryresults
[0m04:31:40.474857 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:40.476362 [info ] [Thread-1  ]: 24 of 28 PASS not_null_orders_customer_id ...................................... [[32mPASS[0m in 2.35s]
[0m04:31:40.477415 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_customer_id.c5f02694af
[0m04:31:40.478418 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m04:31:40.479053 [info ] [Thread-1  ]: 25 of 28 START test not_null_orders_gift_card_amount ........................... [RUN]
[0m04:31:40.480589 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a"
[0m04:31:40.480922 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m04:31:40.481194 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m04:31:40.491299 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a"
[0m04:31:40.492171 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:40.492425 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m04:31:40.496539 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a"
[0m04:31:40.497385 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:31:40.499676 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select gift_card_amount
from `jaffle-shop-375704`.`bruno`.`orders`
where gift_card_amount is null



      
    ) dbt_internal_test
[0m04:31:42.726968 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:4e2d9ab6-b7b7-4ff5-af65-95a1f30990d8:US&page=queryresults
[0m04:31:42.729346 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:42.732113 [info ] [Thread-1  ]: 25 of 28 PASS not_null_orders_gift_card_amount ................................. [[32mPASS[0m in 2.25s]
[0m04:31:42.733903 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_gift_card_amount.413a0d2d7a
[0m04:31:42.734573 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m04:31:42.735070 [info ] [Thread-1  ]: 26 of 28 START test not_null_orders_order_id ................................... [RUN]
[0m04:31:42.736512 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.not_null_orders_order_id.cf6c17daed"
[0m04:31:42.736854 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m04:31:42.737136 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m04:31:42.749721 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_orders_order_id.cf6c17daed"
[0m04:31:42.750413 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:42.750647 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m04:31:42.765693 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_orders_order_id.cf6c17daed"
[0m04:31:42.766565 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:31:42.768258 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_orders_order_id.cf6c17daed: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_orders_order_id.cf6c17daed"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from `jaffle-shop-375704`.`bruno`.`orders`
where order_id is null



      
    ) dbt_internal_test
[0m04:31:44.856815 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:de51ef01-9c9a-4eef-81f3-5e2bbb47c2d5:US&page=queryresults
[0m04:31:44.859285 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:44.860472 [info ] [Thread-1  ]: 26 of 28 PASS not_null_orders_order_id ......................................... [[32mPASS[0m in 2.12s]
[0m04:31:44.861260 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_orders_order_id.cf6c17daed
[0m04:31:44.861632 [debug] [Thread-1  ]: Began running node test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m04:31:44.862072 [info ] [Thread-1  ]: 27 of 28 START test relationships_orders_customer_id__customer_id__ref_customers_  [RUN]
[0m04:31:44.863402 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2"
[0m04:31:44.864138 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m04:31:44.864562 [debug] [Thread-1  ]: Compiling test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m04:31:44.895185 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2"
[0m04:31:44.896165 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:44.896405 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m04:31:44.899479 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2"
[0m04:31:44.899934 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:31:44.901410 [debug] [Thread-1  ]: On test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select customer_id as from_field
    from `jaffle-shop-375704`.`bruno`.`orders`
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from `jaffle-shop-375704`.`bruno`.`customers`
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m04:31:47.335360 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:54699030-4c06-437f-8bdf-fcb9a0555eb2:US&page=queryresults
[0m04:31:47.337784 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:47.339560 [info ] [Thread-1  ]: 27 of 28 PASS relationships_orders_customer_id__customer_id__ref_customers_ .... [[32mPASS[0m in 2.48s]
[0m04:31:47.340496 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.relationships_orders_customer_id__customer_id__ref_customers_.c6ec7f58f2
[0m04:31:47.341127 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m04:31:47.341932 [info ] [Thread-1  ]: 28 of 28 START test unique_orders_order_id ..................................... [RUN]
[0m04:31:47.343521 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.jaffle_shop.unique_orders_order_id.fed79b3a6e"
[0m04:31:47.343844 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m04:31:47.344128 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m04:31:47.361461 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_orders_order_id.fed79b3a6e"
[0m04:31:47.362415 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:47.362681 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m04:31:47.366730 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_orders_order_id.fed79b3a6e"
[0m04:31:47.367593 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m04:31:47.369431 [debug] [Thread-1  ]: On test.jaffle_shop.unique_orders_order_id.fed79b3a6e: /* {"app": "dbt", "dbt_version": "1.3.2", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_orders_order_id.fed79b3a6e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `jaffle-shop-375704`.`bruno`.`orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m04:31:49.587350 [debug] [Thread-1  ]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=jaffle-shop-375704&j=bq:123a06c9-b1f4-4942-af68-2278f67fcc4e:US&page=queryresults
[0m04:31:49.590058 [debug] [Thread-1  ]: finished collecting timing info
[0m04:31:49.592419 [info ] [Thread-1  ]: 28 of 28 PASS unique_orders_order_id ........................................... [[32mPASS[0m in 2.25s]
[0m04:31:49.593844 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_orders_order_id.fed79b3a6e
[0m04:31:49.597186 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m04:31:49.598825 [info ] [MainThread]: 
[0m04:31:49.599358 [info ] [MainThread]: Finished running 3 seeds, 3 view models, 20 tests, 2 table models in 0 hours 3 minutes and 53.64 seconds (233.64s).
[0m04:31:49.599830 [debug] [MainThread]: Connection 'master' was properly closed.
[0m04:31:49.600070 [debug] [MainThread]: Connection 'test.jaffle_shop.unique_orders_order_id.fed79b3a6e' was properly closed.
[0m04:31:49.625009 [info ] [MainThread]: 
[0m04:31:49.625466 [info ] [MainThread]: [32mCompleted successfully[0m
[0m04:31:49.625902 [info ] [MainThread]: 
[0m04:31:49.626212 [info ] [MainThread]: Done. PASS=28 WARN=0 ERROR=0 SKIP=0 TOTAL=28
[0m04:31:49.626659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e20e520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e2f5670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e19fa60>]}
[0m04:31:49.626988 [debug] [MainThread]: Flushing usage events
